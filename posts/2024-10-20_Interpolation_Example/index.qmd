---
title: "Random Exercises: Interpolation with Lagrange Polynomials"
author: "Nico"
date: "2024-10-20"
categories: [math]
---

## Intro

As I want to make sure I keep posting here, I keep looking for things to write about in my own personal â€œlibraryâ€ (book shelves).

One topic I want to make sure I donâ€™t forget about is â€œNumerical Methodsâ€. (Another one is graph theory, another one is anomaly detectionâ€¦ There are a few topics like that :D).

Anyhow. Today: **Interpolation**.

More specifically, Lagrange polynomials. Which is just one of many possible approaches (and has its own drawbacks).

## Concept of Interpolation

So first, what is â€œInterpolationâ€. Most people interested in Statistics and Machine Learning (mostly wrt time series and predicting the future of stock markets, sayâ€¦) will have heard about Extrapolation. And thatâ€™s mostly it: Take a distribution or timeline and â€œpredictâ€ what the future of it should look like. (To keep things simple, that is).

Interpolation is about finding a distribution *in between known points*. So not about the â€œfutureâ€ (in the above comparison) but instead about the â€œpresentâ€, if you will. Itâ€™s about â€œfilling the gapsâ€. Or trying to ğŸ™‚

## Lagrange Polynomials

â€œFor a given set of N+1 data points, we want to find the coefficients of an Nth-degree polynomial function to match them \[â€¦\]â€

![](lagrange_poly.png){alt=""}

Lagrange polynomials are somewhat intuitive because each term x_m can rather easily be shown to correspond exactly to y_m (by definition of L\_{N, m} above).

Alright, and the math above leads me to the following R code (my own, but not saying â€œperfectâ€, of course :D):

![](interpolation_lagrange_r_v001-1024x809.png){alt=""}

(The [source code can be found here](https://github.com/kaizen-R/R/blob/master/Sample/math/interpolation_lagrange_poly_v001.R))

And to validate that, I take the same example points as that of the reference book, and we can plot the results:

![](lagrange_poly_result_plot-1024x605.png){alt=""}

So we see a POSSIBLE approach here, which â€œlooksâ€ sensible (but MIGHT VERY WELL be wrong).

## Conclusions

Today was about one of many approaches, a short **introduction to the topic of Interpolation** (as â€œopposedâ€, though not really, to extrapolation).

There are several alternatives, namely *Newton Polynomials*, for which improvements can be obtained by better choosing sample points (*Chebyshev nodes*). Or the well appreciated C*ubic Splines*.

Newton Polynomials for instance work with recursion, and so might come in handy if one expects to ADD new reference data points in the future, because with Lagrange Polynomials, all calculations need to be redone from scratch.

All of which might make for a nice few **future entries** of this blog ğŸ™‚

## Resources

â€œApplied Numerical Methods Using MatLab, 2nd Editionâ€, Ed. Wiley, by W. Y. Yang, W. Cao, J. Kim & al.
