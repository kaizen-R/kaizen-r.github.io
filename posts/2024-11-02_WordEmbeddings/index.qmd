---
title: "ML Concepts: Word Embeddings"
author: "Nico"
date: "2024-11-02"
categories: [ML, NLP]
---

## Continuing on text for Cybersecurity ML

I discussed an unsupervised clustering algorithm, DBScan. We used multi-dimensional points (coordinates) in space. But what if our data is text? Thatâ€™s common in â€œgeneralâ€, but also in Cybersecurity. So the question becomes: Can we treat â€œtextâ€ as points on which to apply such (or similar) algorithms?

Enters the concept of **embedding**.

## Conceptual understanding

Once again, who am I to â€œteachâ€ you what an embedding is, hu? Itâ€™s probably better to go to [the definition](https://en.wikipedia.org/wiki/Word_embedding), which hopefully, thanks to the context provided in the last entry, can help intuit where weâ€™re going with all this:

*â€œInÂ natural language processing, aÂ **word embedding**Â is a representation of a word. The embedding is used in text analysis. Typically, the representation is aÂ real-valuedÂ vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning.â€*

So two things: Today is actually about *Natural Language Processing*, or NLP in short. Not a new topic in this Blog, but hey.

Second, weâ€™re looking for a representation of a word as a â€œreal-valued vectorâ€. So think of it like so: A â€œvectorâ€ can represent many things, but today weâ€™re going to consider it a set of coordinates.

So in 3 dimensions (3D), a word embedding would represent *a word as 3 numbers*, representing *each a coordinate of the (x, y, z) space*. Sounds familiar? Check again the entry on Clustering if not, please, it makes more sense to consider both entries togetherâ€¦

Youâ€™re back?

OK. For very large text, maybe the information of a word with only 3 dimensions is not enough to â€œencodeâ€ its relationship to other words. So you would go into higher dimensions, say 15, 30 or 1000 dimensions (again, why not? You and I canâ€™t visualize 30 dimensions in our heads, but itâ€™s easy for a computerâ€¦ Fun, ainâ€™t it?)

And so with a set of vectors, each representing a word, we get in fact a set of points in an N-dimensional space. And thenâ€¦

**Why not** use algorithms on these â€œembeddingsâ€, say the DBScan algorithm?

## In practice: Embedding from text(s)

Weâ€™re not going to discuss the current algorithms for embeddings in detail. They useâ€¦ Neural Networks, of all things. The general accepted version of Word2Vec for instance in essence takes pairs of words (out of â€œtraining textâ€) and transforms the â€œwordsâ€ into vectors of 300 dimensions (if I remember correctly). Better even, you can get the embeddings, so you need not train anything yourself (thanks Google AI!).

But for today, we might just want to go ahead and actually train our own embeddings. Letâ€™s go for it!

By the way: [Here the code for today.](https://github.com/kaizen-R/R/blob/master/Sample/NLP/word_embedding_cyber_wiki_v001.R)

Now one important concept for Machine Learning: **â€œGarbage IN? Garbage OUT!â€** So IF I use cra\*py (pardon my french) text as input, I shouldnâ€™t expect much of a result as an output.Â 

Letâ€™s say I consider the Wikipedia to hold â€œgoodâ€ text about some IT and Cybersecurity concepts. If so, I could use for instance the R Package wikipediR. (At least thatâ€™s the one I used for today)

``` R
library(WikipediR) # Get Wiki data
## Simple wrapper
my_page_content <- function(keywords) {
  page_content(language = "en",
    project = "wikipedia",
    page_name = keywords,
    as_wikitext = FALSE,
    clean_response = TRUE) |>
  clean_text_set()
}
## Explicitly for explanation:
firewall_wiki <- my_page_content("firewall (computing)")
firewall_wiki <- firewall_wiki[1:82]
switch_wiki <- my_page_content("network switch")
switch_wiki <- switch_wiki[2:96]
```

And it goes on, with a few other keywords of interest (say â€œrouterâ€, â€œhackerâ€â€¦). You have the details of this example in the code.

Why I filter and keep only 82 paragraphs of the Firewall entry? Just a matter of cleaner stuff, the way I parse the Wiki entries (which is detailed in â€œclean_text_set()â€ function), some lines contain a bit of only pairs of words, others a few references with proper Names, etc.Â  Consider this a manual process in this case because Iâ€™m lazy. In the real world, I would look for the key words that mark the section of the Wiki Entry that do not interest me, and keep the ones above that only. And clean those. Like it or not, in this case, I needed to extract meaningful sentences of some HTML pages. It requires a bit of work (my â€œcleant_text_set()â€ function, created for todayâ€™s exercise specifically, hopefully can show how one has to work, itâ€™s not always as simple as running a function callâ€¦). All in all, after pre-processing, Iâ€™ll end up with 692 sentences. **In traditional ML, a good part of the work is about getting the right data in the right format.** And thatâ€™s all I say about that today. Moving on.

The next step will be to use our data. Here Iâ€™m not going to implement anything myself, itâ€™s beyond my point. Suffice to say Iâ€™m going to use the â€œContinuous Bag of Wordsâ€, that looks at words AROUND one word to assign positions in space. In concept and simplifying a lot, weâ€™re seeing if â€œblockâ€ and â€œfirewallâ€ appear in the training text near one another more often than â€œrestaurantâ€ and â€œfirewallâ€. (Iâ€™d guess thatâ€™s about right :D)

Now we can use R to train our own Word2Vec Neural Network, with Bag Of Words, on our sample text (692 small blocks of text) and ask it to come up with vectors of 30 dimensions as embeddings for the main keywords found in the text.

``` R
## Lets' move on to something more... Substantial:
full_text <- c(
  switch_wiki,
  router_wiki,
  firewall_wiki,
  hacker_wiki,
  computer_wiki,
  cpu_wiki,
  virus_wiki
)

model <- word2vec(full_text, type="cbow", dim=30, iter = 50)
embeddings <- as.matrix(model)
embeddings
```

And yes, it requires a few things (the â€œword2vecâ€ R package, for one), a bit of understanding (or trial and error, but understanding is better!). But if all goes as planned youâ€™ll get something like this:

\
![](https://www.kaizen-r.com/wp-content/uploads/2024/10/embeddings_screenshot_1-1024x119.png){alt=""}

![](https://www.kaizen-r.com/wp-content/uploads/2024/10/embeddings_dims.png){alt=""}

So 970 words have been transformed into their corresponding 30-dimensional vectors! Good!

30 dimensions is going to be hard to â€œlook atâ€, but letâ€™s do it anyway. What we really want is **to understand the similarity of things here**. So for instance, FROM OUR SELECTED WIKI text (very small sample, if you ask me, and yetâ€¦):

![](https://www.kaizen-r.com/wp-content/uploads/2024/10/lookslike_5.png){alt=""}

Now I wouldnâ€™t necessarily agree that â€œpixâ€ is the best nearest word for â€œfirewallâ€ butâ€¦ Thatâ€™s what our sample text says, it would seem. Anyhow, it doesnâ€™t sound completely crazy either. (e.g. â€œRestaurantâ€, had it been in the sample text, hopefully wouldnâ€™t appear in the top 5 â€œnearestâ€ terms for Firewallâ€¦)

30 dimensions is going to be hard to â€œlook atâ€, but letâ€™s do it anyway. What we really want is **to understand the similarity of things here**. So for instance, FROM OUR SELECTED WIKI text (very small sample, if you ask me, and yetâ€¦):

Now I wouldnâ€™t necessarily agree that â€œpixâ€ is the best nearest word for â€œfirewallâ€ butâ€¦ Thatâ€™s what our sample text says, it would seem. Anyhow, it doesnâ€™t sound completely crazy either. (e.g. â€œRestaurantâ€, had it been in the sample text, hopefully wouldnâ€™t appear in the top 5 â€œnearestâ€ terms for Firewallâ€¦)

## Going 2D and hint for the future

Iâ€™m going to finish this with one visualization, and then hopefully everything will come together. Now Iâ€™ll say this first: I know you can bring 30 dimensions into 2, yes. I was going to try â€œmulti-dimensional scalingâ€ (as PCA is probably too lossy for such a reduction), or look into some algorithm for that. [But then I came across examples here](https://www.rdocumentation.org/packages/word2vec/versions/0.4.0), and heck, dimensionality reduction was beyond the point for today, and so *I skipped doing it myself.* (To this day, I havenâ€™t looked at how the umap() function works. I know, shame on me.)

But here is the **key of all the conversation for today**:

![Projecting Embeddings onto 2D plot](https://www.kaizen-r.com/wp-content/uploads/2024/10/embeddings_2D-1024x495.png){alt="Projecting Embeddings onto 2D plot"}

Weâ€™ve done it! We have visualized our words, not without **first creating embeddings for them**, and then projecting into 2 Dimensions.

And letâ€™s have a look at what is whereâ€¦![](https://www.kaizen-r.com/wp-content/uploads/2024/10/firewall_2D-1024x683.png){alt=""}

Not bad, â€œfirewallsâ€ is near â€œfirewallâ€ (pfiu!). So are filter, trafficâ€¦ And maybe the rest is not great, but thatâ€™s what we came up with from (again) very little sample text.

## Mixing things up!

Last week, I [published a (simplistic) entry about DBScan](https://www.kaizen-r.com/2024/10/ml-concepts-unsupervised-learning-clustering-dbscan/) as an algorithm to cluster things. WHY NOT apply that here?!

![](https://www.kaizen-r.com/wp-content/uploads/2024/11/2D_All_Wiki_Clusters-1024x650.png){alt=""}

Could we have a look at one cluster, maybe one that contains the word â€œVirusâ€?

![](https://www.kaizen-r.com/wp-content/uploads/2024/11/virus_clust-1024x812.png){alt=""}

Things like â€œinfectionâ€, â€œexecutableâ€, â€œscannerâ€, â€œmaliciousâ€ are all in the areaâ€¦ And with the same color!

With more text and better cleanupâ€¦ Iâ€™m convinced the approach has its merits ğŸ™‚

As per 3D visuals, this time using Multi-dimensional Scaling (another algorithm that uses distances!) to project onto 3 dimensions, well it works, but there is a bit too much data, and maybe itâ€™s not super super usefulâ€¦ Still:

![](https://www.kaizen-r.com/wp-content/uploads/2024/11/wiki_MDS_3D-300x245.png){alt=""}

## Applications

What if instead of Wiki entries from the internet, we **had taken CVE text** (maybe along with their CVSS (or whatever scoring system you prefer))? We could probably do some sort of regression once we encode the text (maybe even mixing things up with other algorithms) and maybe estimate the score?

How about **classifying threat alerts into groups**?

What if we had taken **logs from a machine**. Could we maybe use all this and find specially **anomalous** logs, from the complete log file? (Imagine thousands of lines reviewed in seconds by your laptop like soâ€¦)

And consider this: Over this and the last Blog entry, weâ€™ve discussed enough to do some **basic ML**. But there is **much more than Clustering applied to text**. I just hope this helps give a **hint of the possibilities**. ğŸ™‚

## Conclusion

At this stage, I truly hope we understand what it means for us to be able to put words (or texts, they could be each filesâ€¦ why not!) into vectors (i.e. points in N-dimensional space), to be projected (or not) and *for which distances can be calculated to other words/texts*.

With that, we open a world of possibilities: *Topic Modelling, Sentiment Analysis*, etc. can all be done using distances between points ğŸ™‚ (There is **more to NLP**, though, and NOT only LLMs, please. More simple/traditional stuff is out there! One example I wrote about forever ago was [part of speech tagging](https://www.kaizen-r.com/2021/08/nlp-3-n-parts-of-speech-tagging/), for instance. Another time I used [TF-IDF to model a classifier of log files with supervised learning](https://www.kaizen-r.com/2021/02/logs-classification-using-ml-2-2/)â€¦) Maybe in a future post Iâ€™ll discuss more of these concepts, but Iâ€™d be very happy for now if somehow I helped someone out there understand a bit better how these things could actually work.

**By the by:** **GenAI** essentially does **self-supervised** learning on word embeddings. (WOW! What a bomb to leave at the end of a blog entry, â€œself-supervisedâ€???).

## Bonus: A word about GenAI

OK, OK, OKâ€¦ But real quick then.

First: **I donâ€™t particularly like** GenAI as a topic because â€“ mostly â€“ of the **hype, misunderstanding, risks**â€¦ but otherwise is undoubtedly incredibly powerful and Iâ€™ll admit it must have some cool applicationsâ€¦ **For expert users!** And although very slowly, I myself am *considering* using it as an assistant R-coderâ€¦ I have done tests and itâ€™s not bad at all, and it WOULD make me much fasterâ€¦ But Iâ€™m mostly resisting for now: Coding and thinking how to approach a problem is what I like, so why externalize thatâ€¦ Unless I really HAVE toâ€¦

That saidâ€¦ What the heck is â€œself-supervisedâ€ learning?Â 

Supervised learning needs to have a means of knowing whether itâ€™s doing a good job to rectify its own behaviour while in training.

If your job is to predict the best next word for a given textâ€¦ All you need is to **try to predict it, and then read the next word** (or â€œtokenâ€). If you guessed wrong, you rectify your behaviour for your next guess. **Then you read the next word**â€¦ And iterate. And in the above scenario, **nobody needs to â€œtagâ€ anything**, the information is self-contained! So you just ingest text one â€œtokenâ€ at a time (donâ€™t worry, say â€œone word at a timeâ€, and more or less youâ€™re good). All you need is text (and â€œattentionâ€, but thatâ€™s WAY beyond todayâ€™s objectives :D).

The more text, the more training examples you get ğŸ™‚

And yes: Your words/tokens, are **presented to your GenAI (well, LLMs, really) asâ€¦ Embbedings.**
