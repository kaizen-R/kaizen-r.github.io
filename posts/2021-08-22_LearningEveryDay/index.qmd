---
title: "Every day I keep learning"
author: "Nico"
date: "2021-08-22"
categories: [code]
---

## Intro

I missed writing an entry last week. Not that I havenâ€™t learned anything I could discuss, but rather because I have been learning and coding a bit too much latelyâ€¦

## Learning more about Postgres

Among many other things (that will make for potentially quite a few other entries in the Blog), I learned one detail about one trick (really going down the rabbit holes sometimesâ€¦) in â€œPostgresâ€:

â€œUpsertâ€ (â€œinsert â€¦ on conflictâ€¦â€) is a very neat trick. It makes my R code (a lot) cleaner & ensures atomic operations at the Database level, so I love it. ğŸ‘

What I recently learned about that wasnâ€™t a worry in the past (it keeps happening!) is the impact on serial IDs ğŸ§ (and it makes sense why Postgres works that way, Iâ€™m not complaining, I just wasnâ€™t aware until very recently):

Say I use it on \~210.000 (bear with me) rows a day, of which most of the time say \~209.900 are repeating entries (so they really should have been â€œupdatesâ€), on a table using a Serial ID. Those are very realistic numbers. And itâ€™s not Big Data (although maybe itâ€™s starting to be somewhere above â€œSmall Dataâ€â€¦)

It turns out Iâ€™ll â€œburnâ€ my serial identifiers in about \~10.000 days (a round number to justify why I used 210k as the example ğŸ¤ª), as each Upsert is increasing the IDs to the next value â€“ even when nothing at all needs change. So Iâ€™m â€œusingâ€ 210k IDs instead of 100 a day! Not too efficient (but then again: effective, as the code around the inserts and updates is shorter and cleanerâ€¦).

Maybe thatâ€™s OK? (I really donâ€™t know where Iâ€™ll be in \~27 years ğŸ˜…)

What if someone launches my script 10 times a day? Then everything will probably break (and I havenâ€™t even thought how) in 2.7 years? I really donâ€™t know yet if anyone but me would ever be using the same script on the same databaseâ€¦\
What if then 10 people use it 10 times a day (that would mean my script went way beyond any expected success, mind you)â€¦ Thatâ€™s 100 runs/dayâ€¦?ğŸ¤•

Should I use â€œBigserialâ€ â€“ in practice avoiding the issue (technically pushing it to a much much later future ğŸ˜‚)?

Or should I take care of trying to avoid â€œUpsertsâ€ where I can â€“ thereby making my code a bit more complex (e.g. making transactions my problem) â€“ my choice for now.

Whatâ€™s the impact of a very sparse & big index? ğŸ¤” I guess I need to keep testing and learningâ€¦

(Note: please remember: Iâ€™m not actually a developer per-se, I just happen to use code sometimes, sorry if this is too obvious ;))

## Other details

As you might expect, using a database is a bit different from using CSV files. Itâ€™s a bit harder to set up, but supposedly faster afterwardsâ€¦

But not if youâ€™re not carefulâ€¦

At one point I followed a logic of my code and did one update per entry that made sense.

Trying to avoid the Upsert thing, I would check for duplicates.

I donâ€™t have my laptop handy to reproduce the code here, but letâ€™s just say (from memory) it required some mix of rbind(), group_by_all(),Â filter(n()\>1) and ungroup()â€¦ And then some more filter(!(id %in% duplicated_id))â€¦\

Upsert would really make it easier! ğŸ˜…

Anyhow, to the point:

Working with all of a table in memory (one big Select) on many many rows, might very well be faster than even a few select for a few rowsâ€¦

In one case it turned out my assumption about â€œworking with few rows and few selectsâ€ vs â€œworking with lots of rows but only one selectâ€Â was wrong by a factor of a hundred!

Just saying: Some assumptions are wrong and testing stuff is a good idea.

## Conclusions

Here is to the â€œkeep learningâ€ attitude. Continuous improvement is not always easy, but hopefully it pays off to challenge one self on a regular basis.
