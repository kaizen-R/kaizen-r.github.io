---
title: "RLCS: A (shorter) Introduction"
subtitle: "Interpretable, Symbolic Machine Learning"
    
format: 
  revealjs:
    footer: "[kaizen-r blog on RLCS](https://kaizen-r.github.io/#category=RLCS)&nbsp;"
    theme: [simple]
    # toc: true
    preview-links: auto
    chalkboard: 
      boardmarker-width: 5
from: markdown+emoji
execute:
  echo: true
---

## An issue with "AI": explainability

<br/>Let's call it Machine Learning. As of today:

-   Mostly Neural networks, DNN, and that's all **black boxes**

There are [ways around that](https://a.co/d/ba1J0oh "Interpretable Machine Learning: A Guide For Making Black Box Models Explainable").

## An issue with "AI": explainability {.smaller}

<br/>**Interpretability/Explainability:**

-   **Model-specific** Methods (...)

-   **Local Model-Agnostic** Methods: Counterfactuals, Local Surrogate Model (LIME), Shapley Value, Shapley Additive exPlanation (SHAP)...

-   **Global Model-Agnostic** Methods: Partial Dependence Plot (PDP), Accumulated Local Effects (ALE), H-Statistic, Surrogate Models...

-   **Interpretable Models**: Trees, LM/GLM, KNN, and **rule-based**

<br/>Recommended reading: [Molnar C. (2022) "Interpretable Machine Learning"](https://a.co/d/ba1J0oh)

## A new R package

![](images/books.jpeg){fig-align="center" width="500"}

<br/>[John H. Holland proposed an algorithm with the Cognitive System One](https://en.wikipedia.org/wiki/Learning_classifier_system#Early_years) program (1976). Later, people came up with variations... Today we focus on [Michigan-style LCS](https://en.wikipedia.org/wiki/Learning_classifier_system#Michigan-Style_Learning_Classifier_System).

# A preview

## Can you guess the "rule"?

<br/>

A **Data Mining** Exercise with RLCS

``` r
> library(RLCS)
> demo_env1 <- rlcs_example_secret1()
> sample_of_rows <- sample(1:nrow(demo_env1), 10, replace=F)
> print(demo_env1[sample_of_rows,], row.names = F)
 state class
 01010     0
 00111     0
 11010     0
 10001     1
 00010     0
 00100     1
 10100     1
 01111     0
 01100     1
 00001     1
```

## Did you guess right?

``` r
> demo_params <- RLCS_hyperparameters(n_epochs = 280, deletion_trigger = 40, deletion_threshold = 0.9)
> rlcs_model1 <- rlcs_train_sl(demo_env1, demo_params, NULL, F)
[1] "Epoch: 40 Progress Exposure: 1280 Classifiers Count: 14"
[1] "Epoch: 80 Progress Exposure: 2560 Classifiers Count: 8"
(...)
[1] "Epoch: 280 Progress Exposure: 8960 Classifiers Count: 2"
```

. . .

<br/> Condition/Action is the rule (the rest are quality metrics)

``` r
> print(rlcs_model1)
  condition action match_count correct_count accuracy numerosity reward first_seen
1     ###1#      0        3560          3560        1        122      5       1843
2     ###0#      1        2770          2770        1         94      5       3421
```

This model expresses: "Action/Class is Not bit 4"

## Learning rules as model(s)

-   if **A & NOT(B)** then **Class=X**

-   if **D** then **Class=Y**

. . .

"Human-readable", "interpretable", **good for:**

-   **Mitigating bias**(es) (in training data, at least)

-   Increased **trust** (justifying decisions)

. . .

Learning about the data (data mining), better decisions, regulatory compliance, ethical/legal matters, possible adversarial attack robustness...

# Interlude: Get the package

## Download and install RLCS

<https://github.com/kaizen-R/RLCS>

<br/>

To get the package from GitHub:

``` r
library(devtools)
install_github("kaizen-R/RLCS")
```

. . .

Run your first test

``` r
library(RLCS)
demo_params <- RLCS_hyperparameters(n_epochs = 400, deletion_trigger = 40, deletion_threshold = 0.9)
demo_env2 <- rlcs_example_secret2()
print(demo_env2, row.names = F)
rlcs_model2 <- rlcs_train_sl(demo_env2, demo_params, NULL, F)
print(rlcs_model2)
plot(rlcs_model2)
```

## Binary input: Example

Neural Networks accept numerical input. **Currently, RLCS accepts Binary strings.**

![Rosetta Stone "binning" for numerical variables (2 bits explanation)](images/demo_encoding_two_bits.jpg)

# LCS: Algorithm Key Concepts

## Keywords

![](images/LCS_Keywords.png)

## Generating a rule

<br/>

> **The key: "\#" means "I don't care"**

<br/>**Covering** a state with a **probability** of "**\#**" values means making a **rule** that **matches** the input state and class/action.

Something that **could** match other (partially) similar input:

``` r
> generate_cover_rule_for_unmatched_instance('010001', 0.2)
[1] "0#0001"
> generate_cover_rule_for_unmatched_instance('010001', 0.8)
[1] "##0###"
```

::: notes
You receive an **instance of the environment** (a binary string **state** and a **class**).

The class here is defined already.
:::

## Matching

-   When you see a new environment **instance** that **does not match any rule** in your population yet -\> generate a **new rule.**

. . .

-   If one(+) rule(s) in your population **matches** your new instance **state** -\> increase the **match count of the corresponding classifier.**

-   If one(+) rule(s) in your population matches your new instance **state && class/action** â€“\> increase the **correct count.**

::: notes
including their corresponding actions
:::

## Rule Discovery (GA)

![](images/GA_Explained.png)

``` r
mut_point <- which(runif(nchar(t_instance_state)) < mut_prob)
```

## Prediction

Imagine a new sample/instance, never seen before. (Test environment)

Prediction is about **returning the match set** for that new instance.

``` r
RLCS::get_match_set(sample_state, population_of_classifiers)
```

. . .

<br/>The **prediction** will be the **majority** (possibly *weighted* by numerosity, accuracy...) of the **proposed class**/action. **That's it!** It also means, this is natively an **ensemble learning** algorithm.

# What can RLCS do?

## Data Mining

<br/>

We've seen simple examples. A few more are included with the package, have a look at the demos.

## Supervised Learning: Iris {.smaller}

::: {layout="[[1,1]]"}
![Iris Setosa (https://commons.wikimedia.org/w/index.php?curid=57593826)](images/960px-Irissetosa1.jpg){width="267"}

![Iris Sample Dataset](images/iris_plot.png){width="241"}
:::

``` r
Time difference of 13.13619 secs ## Training Runtime.
            predicted
class        setosa versicolor virginica
  setosa         13          0         0
  versicolor      0          5         3
  virginica       0          0         9
```

## Supervised Learning: Iris {.smaller}

![visualizing one classifier - iris](images/iris_one_classifier.png){fig-align="center"}

## Supervised Learning: Images Classifier

``` r
[1] "Accuracy: 0.98"
> table(test_mnist_bin01_49b[, c("class", "predicted")])
     predicted
class    0    1 rlcs_no_match
    0 1716   65             5
    1    5 2008             0
>
> ## Training time on 800 samples:
> print(t_end - t_start)
Time difference of 1.937979 mins
```

. . .

<br/>

**!! Magic Trick:** Parallelism: By splitting training data, and then consolidating sub-models! (Take that, Neural network :D)

## Supervised Learning: Images Classifier

![](images/mnist_example_visu_explained.jpg){fig-align="center"}

## Reinforcement Learning, TOO! {.smaller}

This example is in fact not learning probability distributions. And it uses reward-shaping. But it works!

::: {layout="[[1,1], [1]]"}
{{< video videos/RL_RLCS_WORLD1_V001.mp4 title="Reinforcement Learning using LCS in R, first results" width="250" height="175">}}

{{< video videos/RL_progress_stats.mp4 title="Actual learning..." width="250" height="175">}}

![](images/RL_Board.jpg){fig-align="center" width="406"}
:::

## Then again

<br/>**Hyperparameters tuning** is not easy, and impacts **performance**. **Large Populations** of rules are in fact hard to interpret, in spite of being readable.

<br/>This is all **work in progress**. I plan to try and make it into a **CRAN Package**. (So: document more, write more tests, reorganize functions...) I will keep working on **better mechanics** (fitness sharing, deletion, etc.), **parallel processing**, help with **encoding**/decoding, etc.

# More Resources

-   [THE BEST INTRO to the LCS Algorithm out-there (hands down!)](https://youtu.be/CRge_cZ2cJc?feature=shared) (12' video)

-   [Wikipedia on LCS](https://en.wikipedia.org/wiki/Learning_classifier_system)

-   [Advanced R is ALWAYS great](https://adv-r.hadley.nz/function-factories.html)

# Thank you!
