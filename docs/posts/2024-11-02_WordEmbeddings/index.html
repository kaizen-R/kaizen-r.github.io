<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nico">
<meta name="dcterms.date" content="2024-11-02">

<title>ML Concepts: Word Embeddings â€“ Kaizen-R.com new home</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Kaizen-R.com new home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kaizen-R"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">ML Concepts: Word Embeddings</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">ML</div>
                <div class="quarto-category">NLP</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Nico </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 2, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="continuing-on-text-for-cybersecurity-ml" class="level2">
<h2 class="anchored" data-anchor-id="continuing-on-text-for-cybersecurity-ml">Continuing on text for Cybersecurity ML</h2>
<p>I discussed an unsupervised clustering algorithm, DBScan. We used multi-dimensional points (coordinates) in space. But what if our data is text? Thatâ€™s common in â€œgeneralâ€, but also in Cybersecurity. So the question becomes: Can we treat â€œtextâ€ as points on which to apply such (or similar) algorithms?</p>
<p>Enters the concept of <strong>embedding</strong>.</p>
<p><img src="images/2D_All_Wiki_Clusters-768x488.png" class="img-fluid"></p>
</section>
<section id="conceptual-understanding" class="level2">
<h2 class="anchored" data-anchor-id="conceptual-understanding">Conceptual understanding</h2>
<p>Once again, who am I to â€œteachâ€ you what an embedding is, hu? Itâ€™s probably better to go to <a href="https://en.wikipedia.org/wiki/Word_embedding">the definition</a>, which hopefully, thanks to the context provided in the last entry, can help intuit where weâ€™re going with all this:</p>
<p><em>â€œIn&nbsp;natural language processing, a&nbsp;<strong>word embedding</strong>&nbsp;is a representation of a word. The embedding is used in text analysis. Typically, the representation is a&nbsp;real-valued&nbsp;vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning.â€</em></p>
<p>So two things: Today is actually about <em>Natural Language Processing</em>, or NLP in short. Not a new topic in this Blog, but hey.</p>
<p>Second, weâ€™re looking for a representation of a word as a â€œreal-valued vectorâ€. So think of it like so: A â€œvectorâ€ can represent many things, but today weâ€™re going to consider it a set of coordinates.</p>
<p>So in 3 dimensions (3D), a word embedding would represent <em>a word as 3 numbers</em>, representing <em>each a coordinate of the (x, y, z) space</em>. Sounds familiar? Check again the entry on Clustering if not, please, it makes more sense to consider both entries togetherâ€¦</p>
<p>Youâ€™re back?</p>
<p>OK. For very large text, maybe the information of a word with only 3 dimensions is not enough to â€œencodeâ€ its relationship to other words. So you would go into higher dimensions, say 15, 30 or 1000 dimensions (again, why not? You and I canâ€™t visualize 30 dimensions in our heads, but itâ€™s easy for a computerâ€¦ Fun, ainâ€™t it?)</p>
<p>And so with a set of vectors, each representing a word, we get in fact a set of points in an N-dimensional space. And thenâ€¦</p>
<p><strong>Why not</strong> use algorithms on these â€œembeddingsâ€, say the DBScan algorithm?</p>
</section>
<section id="in-practice-embedding-from-texts" class="level2">
<h2 class="anchored" data-anchor-id="in-practice-embedding-from-texts">In practice: Embedding from text(s)</h2>
<p>Weâ€™re not going to discuss the current algorithms for embeddings in detail. They useâ€¦ Neural Networks, of all things. The general accepted version of Word2Vec for instance in essence takes pairs of words (out of â€œtraining textâ€) and transforms the â€œwordsâ€ into vectors of 300 dimensions (if I remember correctly). Better even, you can get the embeddings, so you need not train anything yourself (thanks Google AI!).</p>
<p>But for today, we might just want to go ahead and actually train our own embeddings. Letâ€™s go for it!</p>
<p>By the way: <a href="https://github.com/kaizen-R/R/blob/master/Sample/NLP/word_embedding_cyber_wiki_v001.R">Here the code for today.</a></p>
<p>Now one important concept for Machine Learning: <strong>â€œGarbage IN? Garbage OUT!â€</strong> So IF I use cra*py (pardon my french) text as input, I shouldnâ€™t expect much of a result as an output.&nbsp;</p>
<p>Letâ€™s say I consider the Wikipedia to hold â€œgoodâ€ text about some IT and Cybersecurity concepts. If so, I could use for instance the R Package wikipediR. (At least thatâ€™s the one I used for today)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WikipediR) <span class="co"># Get Wiki data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Simple wrapper</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>my_page_content <span class="ot">&lt;-</span> <span class="cf">function</span>(keywords) {</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">page_content</span>(<span class="at">language =</span> <span class="st">"en"</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">project =</span> <span class="st">"wikipedia"</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">page_name =</span> keywords,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">as_wikitext =</span> <span class="cn">FALSE</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">clean_response =</span> <span class="cn">TRUE</span>) <span class="sc">|&gt;</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">clean_text_set</span>()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Explicitly for explanation:</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>firewall_wiki <span class="ot">&lt;-</span> <span class="fu">my_page_content</span>(<span class="st">"firewall (computing)"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>firewall_wiki <span class="ot">&lt;-</span> firewall_wiki[<span class="dv">1</span><span class="sc">:</span><span class="dv">82</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>switch_wiki <span class="ot">&lt;-</span> <span class="fu">my_page_content</span>(<span class="st">"network switch"</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>switch_wiki <span class="ot">&lt;-</span> switch_wiki[<span class="dv">2</span><span class="sc">:</span><span class="dv">96</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>And it goes on, with a few other keywords of interest (say â€œrouterâ€, â€œhackerâ€â€¦). You have the details of this example in the code.</p>
<p>Why I filter and keep only 82 paragraphs of the Firewall entry? Just a matter of cleaner stuff, the way I parse the Wiki entries (which is detailed in â€œclean_text_set()â€ function), some lines contain a bit of only pairs of words, others a few references with proper Names, etc.&nbsp; Consider this a manual process in this case because Iâ€™m lazy. In the real world, I would look for the key words that mark the section of the Wiki Entry that do not interest me, and keep the ones above that only. And clean those. Like it or not, in this case, I needed to extract meaningful sentences of some HTML pages. It requires a bit of work (my â€œcleant_text_set()â€ function, created for todayâ€™s exercise specifically, hopefully can show how one has to work, itâ€™s not always as simple as running a function callâ€¦). All in all, after pre-processing, Iâ€™ll end up with 692 sentences. <strong>In traditional ML, a good part of the work is about getting the right data in the right format.</strong> And thatâ€™s all I say about that today. Moving on.</p>
<p>The next step will be to use our data. Here Iâ€™m not going to implement anything myself, itâ€™s beyond my point. Suffice to say Iâ€™m going to use the â€œContinuous Bag of Wordsâ€, that looks at words AROUND one word to assign positions in space. In concept and simplifying a lot, weâ€™re seeing if â€œblockâ€ and â€œfirewallâ€ appear in the training text near one another more often than â€œrestaurantâ€ and â€œfirewallâ€. (Iâ€™d guess thatâ€™s about right :D)</p>
<p>Now we can use R to train our own Word2Vec Neural Network, with Bag Of Words, on our sample text (692 small blocks of text) and ask it to come up with vectors of 30 dimensions as embeddings for the main keywords found in the text.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Lets' move on to something more... Substantial:</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>full_text <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  switch_wiki,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  router_wiki,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  firewall_wiki,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  hacker_wiki,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  computer_wiki,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  cpu_wiki,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  virus_wiki</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">word2vec</span>(full_text, <span class="at">type=</span><span class="st">"cbow"</span>, <span class="at">dim=</span><span class="dv">30</span>, <span class="at">iter =</span> <span class="dv">50</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>embeddings <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(model)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>embeddings</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>And yes, it requires a few things (the â€œword2vecâ€ R package, for one), a bit of understanding (or trial and error, but understanding is better!). But if all goes as planned youâ€™ll get something like this:</p>
<p><br>
<img src="images/embeddings_screenshot_1-2048x237.png" class="img-fluid" alt=""></p>
<p><img src="images/embeddings_dims.png" class="img-fluid" alt=""></p>
<p>So 970 words have been transformed into their corresponding 30-dimensional vectors! Good!</p>
<p>30 dimensions is going to be hard to â€œlook atâ€, but letâ€™s do it anyway. What we really want is <strong>to understand the similarity of things here</strong>. So for instance, FROM OUR SELECTED WIKI text (very small sample, if you ask me, and yetâ€¦):</p>
<p><img src="images/lookslike_5.png" class="img-fluid" alt=""></p>
<p>Now I wouldnâ€™t necessarily agree that â€œpixâ€ is the best nearest word for â€œfirewallâ€ butâ€¦ Thatâ€™s what our sample text says, it would seem. Anyhow, it doesnâ€™t sound completely crazy either. (e.g.&nbsp;â€œRestaurantâ€, had it been in the sample text, hopefully wouldnâ€™t appear in the top 5 â€œnearestâ€ terms for Firewallâ€¦)</p>
<p>30 dimensions is going to be hard to â€œlook atâ€, but letâ€™s do it anyway. What we really want is <strong>to understand the similarity of things here</strong>. So for instance, FROM OUR SELECTED WIKI text (very small sample, if you ask me, and yetâ€¦):</p>
<p>Now I wouldnâ€™t necessarily agree that â€œpixâ€ is the best nearest word for â€œfirewallâ€ butâ€¦ Thatâ€™s what our sample text says, it would seem. Anyhow, it doesnâ€™t sound completely crazy either. (e.g.&nbsp;â€œRestaurantâ€, had it been in the sample text, hopefully wouldnâ€™t appear in the top 5 â€œnearestâ€ terms for Firewallâ€¦)</p>
</section>
<section id="going-2d-and-hint-for-the-future" class="level2">
<h2 class="anchored" data-anchor-id="going-2d-and-hint-for-the-future">Going 2D and hint for the future</h2>
<p>Iâ€™m going to finish this with one visualization, and then hopefully everything will come together. Now Iâ€™ll say this first: I know you can bring 30 dimensions into 2, yes. I was going to try â€œmulti-dimensional scalingâ€ (as PCA is probably too lossy for such a reduction), or look into some algorithm for that. <a href="https://www.rdocumentation.org/packages/word2vec/versions/0.4.0">But then I came across examples here</a>, and heck, dimensionality reduction was beyond the point for today, and so <em>I skipped doing it myself.</em> (To this day, I havenâ€™t looked at how the umap() function works. I know, shame on me.)</p>
<p>But here is the <strong>key of all the conversation for today</strong>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/embeddings_2D-1536x742.png" class="img-fluid figure-img"></p>
<figcaption>Projecting Embeddings onto 2D plot</figcaption>
</figure>
</div>
<p>Weâ€™ve done it! We have visualized our words, not without <strong>first creating embeddings for them</strong>, and then projecting into 2 Dimensions.</p>
<p>And letâ€™s have a look at what is whereâ€¦<img src="images/firewall_2D-1536x1025.png" class="img-fluid" alt=""></p>
<p>Not bad, â€œfirewallsâ€ is near â€œfirewallâ€ (pfiu!). So are filter, trafficâ€¦ And maybe the rest is not great, but thatâ€™s what we came up with from (again) very little sample text.</p>
</section>
<section id="mixing-things-up" class="level2">
<h2 class="anchored" data-anchor-id="mixing-things-up">Mixing things up!</h2>
<p>Last week, I <a href="https://www.kaizen-r.com/2024/10/ml-concepts-unsupervised-learning-clustering-dbscan/">published a (simplistic) entry about DBScan</a> as an algorithm to cluster things. WHY NOT apply that here?!</p>
<p><img src="images/2D_All_Wiki_Clusters.png" class="img-fluid" alt=""></p>
<p>Could we have a look at one cluster, maybe one that contains the word â€œVirusâ€?</p>
<p><img src="images/virus_clust-1536x1218.png" class="img-fluid" alt=""></p>
<p>Things like â€œinfectionâ€, â€œexecutableâ€, â€œscannerâ€, â€œmaliciousâ€ are all in the areaâ€¦ And with the same color!</p>
<p>With more text and better cleanupâ€¦ Iâ€™m convinced the approach has its merits ğŸ™‚</p>
<p>As per 3D visuals, this time using Multi-dimensional Scaling (another algorithm that uses distances!) to project onto 3 dimensions, well it works, but there is a bit too much data, and maybe itâ€™s not super super usefulâ€¦ Still:</p>
<p><img src="images/wiki_MDS_3D-768x627.png" class="img-fluid" alt=""></p>
</section>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">Applications</h2>
<p>What if instead of Wiki entries from the internet, we <strong>had taken CVE text</strong> (maybe along with their CVSS (or whatever scoring system you prefer))? We could probably do some sort of regression once we encode the text (maybe even mixing things up with other algorithms) and maybe estimate the score?</p>
<p>How about <strong>classifying threat alerts into groups</strong>?</p>
<p>What if we had taken <strong>logs from a machine</strong>. Could we maybe use all this and find specially <strong>anomalous</strong> logs, from the complete log file? (Imagine thousands of lines reviewed in seconds by your laptop like soâ€¦)</p>
<p>And consider this: Over this and the last Blog entry, weâ€™ve discussed enough to do some <strong>basic ML</strong>. But there is <strong>much more than Clustering applied to text</strong>. I just hope this helps give a <strong>hint of the possibilities</strong>. ğŸ™‚</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>At this stage, I truly hope we understand what it means for us to be able to put words (or texts, they could be each filesâ€¦ why not!) into vectors (i.e.&nbsp;points in N-dimensional space), to be projected (or not) and <em>for which distances can be calculated to other words/texts</em>.</p>
<p>With that, we open a world of possibilities: <em>Topic Modelling, Sentiment Analysis</em>, etc. can all be done using distances between points ğŸ™‚ (There is <strong>more to NLP</strong>, though, and NOT only LLMs, please. More simple/traditional stuff is out there! One example I wrote about forever ago was <a href="https://www.kaizen-r.com/2021/08/nlp-3-n-parts-of-speech-tagging/">part of speech tagging</a>, for instance. Another time I used <a href="https://www.kaizen-r.com/2021/02/logs-classification-using-ml-2-2/">TF-IDF to model a classifier of log files with supervised learning</a>â€¦) Maybe in a future post Iâ€™ll discuss more of these concepts, but Iâ€™d be very happy for now if somehow I helped someone out there understand a bit better how these things could actually work.</p>
<p><strong>By the by:</strong> <strong>GenAI</strong> essentially does <strong>self-supervised</strong> learning on word embeddings. (WOW! What a bomb to leave at the end of a blog entry, â€œself-supervisedâ€???).</p>
</section>
<section id="bonus-a-word-about-genai" class="level2">
<h2 class="anchored" data-anchor-id="bonus-a-word-about-genai">Bonus: A word about GenAI</h2>
<p>OK, OK, OKâ€¦ But real quick then.</p>
<p>First: <strong>I donâ€™t particularly like</strong> GenAI as a topic because â€“ mostly â€“ of the <strong>hype, misunderstanding, risks</strong>â€¦ but otherwise is undoubtedly incredibly powerful and Iâ€™ll admit it must have some cool applicationsâ€¦ <strong>For expert users!</strong> And although very slowly, I myself am <em>considering</em> using it as an assistant R-coderâ€¦ I have done tests and itâ€™s not bad at all, and it WOULD make me much fasterâ€¦ But Iâ€™m mostly resisting for now: Coding and thinking how to approach a problem is what I like, so why externalize thatâ€¦ Unless I really HAVE toâ€¦</p>
<p>That saidâ€¦ What the heck is â€œself-supervisedâ€ learning?&nbsp;</p>
<p>Supervised learning needs to have a means of knowing whether itâ€™s doing a good job to rectify its own behaviour while in training.</p>
<p>If your job is to predict the best next word for a given textâ€¦ All you need is to <strong>try to predict it, and then read the next word</strong> (or â€œtokenâ€). If you guessed wrong, you rectify your behaviour for your next guess. <strong>Then you read the next word</strong>â€¦ And iterate. And in the above scenario, <strong>nobody needs to â€œtagâ€ anything</strong>, the information is self-contained! So you just ingest text one â€œtokenâ€ at a time (donâ€™t worry, say â€œone word at a timeâ€, and more or less youâ€™re good). All you need is text (and â€œattentionâ€, but thatâ€™s WAY beyond todayâ€™s objectives :D).</p>
<p>The more text, the more training examples you get ğŸ™‚</p>
<p>And yes: Your words/tokens, are <strong>presented to your GenAI (well, LLMs, really) asâ€¦ Embbedings.</strong></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>