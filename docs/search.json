[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Today I just started something simple: An Object to represent a simulated World in which an Agent can move.\nAs the ‚ÄúAgent‚Äù will need to interact with the World, and the World should return rewards, and understand positions, etc., I decided in this case to go the way of R Objects, more specifically ‚ÄúsetRefClass‚Äù.\nFor today, I just created a basic object to generate a ‚Äúrandom world‚Äù that can have walls, food, even enemy(ies) and in which we can throw an agent at random in any empty space.\nEDIT: Added Agent movements and visualizations\nA few peculiarities maybe: I think it will be simpler in the future to not have to deal with ‚Äúborders‚Äù from the Agent perspective, and so I set my Worlds to necessarily be walls all around.\n\n\n\nLet‚Äôs start today with a simple video showing the results. The agent is the blue thing. Food is green, enemy is red, black are walls.\nAll is initialized randomly, and so are the moves of the current agent. On the left, I show the agent‚Äôs perspective/perception (allocentric) of the World. On the right, the World as it is at each step:\n\nVideos are always more satisfying than matrices, but to understand what happens, let‚Äôs have a look under the hood‚Ä¶\n\n\n\nSo in my matrix, I choose 0 = empty cell, 1 is a wall, 2 is food, 3 is an enemy (defaults to no enemies). And 4 will be an agent. This is all arbitrary, but when in the future I encode into binary there will be a specific order, as this will be relevant for the GA to work with.\nAnyhow, it looks like so:\n\n\n\nA simple world object\n\n\nIn this ‚Äúworld‚Äù, I needed to add methods to move around (from-to positions, or just ‚Äúto‚Äù), control for ilegal moves (can‚Äôt move into a wall), return rewards: good +10 for food, bad -100 if enemy, and empty and walls, well, to be refined.\nAll very simple, but cool at the same time: I can already render ‚Äúanimations‚Äù of said worlds‚Ä¶ Albeit with a very stupid agent for now‚Ä¶\n\n\n\nWell, I‚Äôm afraid now I have most of the pieces. But still:\n\nI will need to implement a variation of my existing RLCS code to account for four possible actions (for now, actions are binary, that‚Äôs not enough).\nI will need to add a sensible binary encoding for the agent to interpret its visible environment\nI will need to account for rewards, change the design for the RL case where I need to add prediction and an Action Set\n‚Ä¶\n\nQuite a bit of work still. But all fun.\nTO BE CONTINUED‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#the-world-should-be-easy-to-interact-with",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#the-world-should-be-easy-to-interact-with",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Today I just started something simple: An Object to represent a simulated World in which an Agent can move.\nAs the ‚ÄúAgent‚Äù will need to interact with the World, and the World should return rewards, and understand positions, etc., I decided in this case to go the way of R Objects, more specifically ‚ÄúsetRefClass‚Äù.\nFor today, I just created a basic object to generate a ‚Äúrandom world‚Äù that can have walls, food, even enemy(ies) and in which we can throw an agent at random in any empty space.\nEDIT: Added Agent movements and visualizations\nA few peculiarities maybe: I think it will be simpler in the future to not have to deal with ‚Äúborders‚Äù from the Agent perspective, and so I set my Worlds to necessarily be walls all around."
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#a-random-policy-agent-in-a-random-world",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#a-random-policy-agent-in-a-random-world",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Let‚Äôs start today with a simple video showing the results. The agent is the blue thing. Food is green, enemy is red, black are walls.\nAll is initialized randomly, and so are the moves of the current agent. On the left, I show the agent‚Äôs perspective/perception (allocentric) of the World. On the right, the World as it is at each step:\n\nVideos are always more satisfying than matrices, but to understand what happens, let‚Äôs have a look under the hood‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#todays-results-in-matrix-format",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#todays-results-in-matrix-format",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "So in my matrix, I choose 0 = empty cell, 1 is a wall, 2 is food, 3 is an enemy (defaults to no enemies). And 4 will be an agent. This is all arbitrary, but when in the future I encode into binary there will be a specific order, as this will be relevant for the GA to work with.\nAnyhow, it looks like so:\n\n\n\nA simple world object\n\n\nIn this ‚Äúworld‚Äù, I needed to add methods to move around (from-to positions, or just ‚Äúto‚Äù), control for ilegal moves (can‚Äôt move into a wall), return rewards: good +10 for food, bad -100 if enemy, and empty and walls, well, to be refined.\nAll very simple, but cool at the same time: I can already render ‚Äúanimations‚Äù of said worlds‚Ä¶ Albeit with a very stupid agent for now‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#what-next",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#what-next",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Well, I‚Äôm afraid now I have most of the pieces. But still:\n\nI will need to implement a variation of my existing RLCS code to account for four possible actions (for now, actions are binary, that‚Äôs not enough).\nI will need to add a sensible binary encoding for the agent to interpret its visible environment\nI will need to account for rewards, change the design for the RL case where I need to add prediction and an Action Set\n‚Ä¶\n\nQuite a bit of work still. But all fun.\nTO BE CONTINUED‚Ä¶"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html",
    "href": "posts/2024-11-02_WordEmbeddings/index.html",
    "title": "ML Concepts: Word Embeddings",
    "section": "",
    "text": "I discussed an unsupervised clustering algorithm, DBScan. We used multi-dimensional points (coordinates) in space. But what if our data is text? That‚Äôs common in ‚Äúgeneral‚Äù, but also in Cybersecurity. So the question becomes: Can we treat ‚Äútext‚Äù as points on which to apply such (or similar) algorithms?\nEnters the concept of embedding."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#continuing-on-text-for-cybersecurity-ml",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#continuing-on-text-for-cybersecurity-ml",
    "title": "ML Concepts: Word Embeddings",
    "section": "",
    "text": "I discussed an unsupervised clustering algorithm, DBScan. We used multi-dimensional points (coordinates) in space. But what if our data is text? That‚Äôs common in ‚Äúgeneral‚Äù, but also in Cybersecurity. So the question becomes: Can we treat ‚Äútext‚Äù as points on which to apply such (or similar) algorithms?\nEnters the concept of embedding."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#conceptual-understanding",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#conceptual-understanding",
    "title": "ML Concepts: Word Embeddings",
    "section": "Conceptual understanding",
    "text": "Conceptual understanding\nOnce again, who am I to ‚Äúteach‚Äù you what an embedding is, hu? It‚Äôs probably better to go to the definition, which hopefully, thanks to the context provided in the last entry, can help intuit where we‚Äôre going with all this:\n‚ÄúIn¬†natural language processing, a¬†word embedding¬†is a representation of a word. The embedding is used in text analysis. Typically, the representation is a¬†real-valued¬†vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning.‚Äù\nSo two things: Today is actually about Natural Language Processing, or NLP in short. Not a new topic in this Blog, but hey.\nSecond, we‚Äôre looking for a representation of a word as a ‚Äúreal-valued vector‚Äù. So think of it like so: A ‚Äúvector‚Äù can represent many things, but today we‚Äôre going to consider it a set of coordinates.\nSo in 3 dimensions (3D), a word embedding would represent a word as 3 numbers, representing each a coordinate of the (x, y, z) space. Sounds familiar? Check again the entry on Clustering if not, please, it makes more sense to consider both entries together‚Ä¶\nYou‚Äôre back?\nOK. For very large text, maybe the information of a word with only 3 dimensions is not enough to ‚Äúencode‚Äù its relationship to other words. So you would go into higher dimensions, say 15, 30 or 1000 dimensions (again, why not? You and I can‚Äôt visualize 30 dimensions in our heads, but it‚Äôs easy for a computer‚Ä¶ Fun, ain‚Äôt it?)\nAnd so with a set of vectors, each representing a word, we get in fact a set of points in an N-dimensional space. And then‚Ä¶\nWhy not use algorithms on these ‚Äúembeddings‚Äù, say the DBScan algorithm?"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#in-practice-embedding-from-texts",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#in-practice-embedding-from-texts",
    "title": "ML Concepts: Word Embeddings",
    "section": "In practice: Embedding from text(s)",
    "text": "In practice: Embedding from text(s)\nWe‚Äôre not going to discuss the current algorithms for embeddings in detail. They use‚Ä¶ Neural Networks, of all things. The general accepted version of Word2Vec for instance in essence takes pairs of words (out of ‚Äútraining text‚Äù) and transforms the ‚Äúwords‚Äù into vectors of 300 dimensions (if I remember correctly). Better even, you can get the embeddings, so you need not train anything yourself (thanks Google AI!).\nBut for today, we might just want to go ahead and actually train our own embeddings. Let‚Äôs go for it!\nBy the way: Here the code for today.\nNow one important concept for Machine Learning: ‚ÄúGarbage IN? Garbage OUT!‚Äù So IF I use cra*py (pardon my french) text as input, I shouldn‚Äôt expect much of a result as an output.¬†\nLet‚Äôs say I consider the Wikipedia to hold ‚Äúgood‚Äù text about some IT and Cybersecurity concepts. If so, I could use for instance the R Package wikipediR. (At least that‚Äôs the one I used for today)\nlibrary(WikipediR) # Get Wiki data\n## Simple wrapper\nmy_page_content &lt;- function(keywords) {\n  page_content(language = \"en\",\n    project = \"wikipedia\",\n    page_name = keywords,\n    as_wikitext = FALSE,\n    clean_response = TRUE) |&gt;\n  clean_text_set()\n}\n## Explicitly for explanation:\nfirewall_wiki &lt;- my_page_content(\"firewall (computing)\")\nfirewall_wiki &lt;- firewall_wiki[1:82]\nswitch_wiki &lt;- my_page_content(\"network switch\")\nswitch_wiki &lt;- switch_wiki[2:96]\nAnd it goes on, with a few other keywords of interest (say ‚Äúrouter‚Äù, ‚Äúhacker‚Äù‚Ä¶). You have the details of this example in the code.\nWhy I filter and keep only 82 paragraphs of the Firewall entry? Just a matter of cleaner stuff, the way I parse the Wiki entries (which is detailed in ‚Äúclean_text_set()‚Äù function), some lines contain a bit of only pairs of words, others a few references with proper Names, etc.¬† Consider this a manual process in this case because I‚Äôm lazy. In the real world, I would look for the key words that mark the section of the Wiki Entry that do not interest me, and keep the ones above that only. And clean those. Like it or not, in this case, I needed to extract meaningful sentences of some HTML pages. It requires a bit of work (my ‚Äúcleant_text_set()‚Äù function, created for today‚Äôs exercise specifically, hopefully can show how one has to work, it‚Äôs not always as simple as running a function call‚Ä¶). All in all, after pre-processing, I‚Äôll end up with 692 sentences. In traditional ML, a good part of the work is about getting the right data in the right format. And that‚Äôs all I say about that today. Moving on.\nThe next step will be to use our data. Here I‚Äôm not going to implement anything myself, it‚Äôs beyond my point. Suffice to say I‚Äôm going to use the ‚ÄúContinuous Bag of Words‚Äù, that looks at words AROUND one word to assign positions in space. In concept and simplifying a lot, we‚Äôre seeing if ‚Äúblock‚Äù and ‚Äúfirewall‚Äù appear in the training text near one another more often than ‚Äúrestaurant‚Äù and ‚Äúfirewall‚Äù. (I‚Äôd guess that‚Äôs about right :D)\nNow we can use R to train our own Word2Vec Neural Network, with Bag Of Words, on our sample text (692 small blocks of text) and ask it to come up with vectors of 30 dimensions as embeddings for the main keywords found in the text.\n## Lets' move on to something more... Substantial:\nfull_text &lt;- c(\n  switch_wiki,\n  router_wiki,\n  firewall_wiki,\n  hacker_wiki,\n  computer_wiki,\n  cpu_wiki,\n  virus_wiki\n)\n\nmodel &lt;- word2vec(full_text, type=\"cbow\", dim=30, iter = 50)\nembeddings &lt;- as.matrix(model)\nembeddings\nAnd yes, it requires a few things (the ‚Äúword2vec‚Äù R package, for one), a bit of understanding (or trial and error, but understanding is better!). But if all goes as planned you‚Äôll get something like this:\n\n\n\nSo 970 words have been transformed into their corresponding 30-dimensional vectors! Good!\n30 dimensions is going to be hard to ‚Äúlook at‚Äù, but let‚Äôs do it anyway. What we really want is to understand the similarity of things here. So for instance, FROM OUR SELECTED WIKI text (very small sample, if you ask me, and yet‚Ä¶):\n\nNow I wouldn‚Äôt necessarily agree that ‚Äúpix‚Äù is the best nearest word for ‚Äúfirewall‚Äù but‚Ä¶ That‚Äôs what our sample text says, it would seem. Anyhow, it doesn‚Äôt sound completely crazy either. (e.g.¬†‚ÄúRestaurant‚Äù, had it been in the sample text, hopefully wouldn‚Äôt appear in the top 5 ‚Äúnearest‚Äù terms for Firewall‚Ä¶)\n30 dimensions is going to be hard to ‚Äúlook at‚Äù, but let‚Äôs do it anyway. What we really want is to understand the similarity of things here. So for instance, FROM OUR SELECTED WIKI text (very small sample, if you ask me, and yet‚Ä¶):\nNow I wouldn‚Äôt necessarily agree that ‚Äúpix‚Äù is the best nearest word for ‚Äúfirewall‚Äù but‚Ä¶ That‚Äôs what our sample text says, it would seem. Anyhow, it doesn‚Äôt sound completely crazy either. (e.g.¬†‚ÄúRestaurant‚Äù, had it been in the sample text, hopefully wouldn‚Äôt appear in the top 5 ‚Äúnearest‚Äù terms for Firewall‚Ä¶)"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#going-2d-and-hint-for-the-future",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#going-2d-and-hint-for-the-future",
    "title": "ML Concepts: Word Embeddings",
    "section": "Going 2D and hint for the future",
    "text": "Going 2D and hint for the future\nI‚Äôm going to finish this with one visualization, and then hopefully everything will come together. Now I‚Äôll say this first: I know you can bring 30 dimensions into 2, yes. I was going to try ‚Äúmulti-dimensional scaling‚Äù (as PCA is probably too lossy for such a reduction), or look into some algorithm for that. But then I came across examples here, and heck, dimensionality reduction was beyond the point for today, and so I skipped doing it myself. (To this day, I haven‚Äôt looked at how the umap() function works. I know, shame on me.)\nBut here is the key of all the conversation for today:\n\n\n\nProjecting Embeddings onto 2D plot\n\n\nWe‚Äôve done it! We have visualized our words, not without first creating embeddings for them, and then projecting into 2 Dimensions.\nAnd let‚Äôs have a look at what is where‚Ä¶\nNot bad, ‚Äúfirewalls‚Äù is near ‚Äúfirewall‚Äù (pfiu!). So are filter, traffic‚Ä¶ And maybe the rest is not great, but that‚Äôs what we came up with from (again) very little sample text."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#mixing-things-up",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#mixing-things-up",
    "title": "ML Concepts: Word Embeddings",
    "section": "Mixing things up!",
    "text": "Mixing things up!\nLast week, I published a (simplistic) entry about DBScan as an algorithm to cluster things. WHY NOT apply that here?!\n\nCould we have a look at one cluster, maybe one that contains the word ‚ÄúVirus‚Äù?\n\nThings like ‚Äúinfection‚Äù, ‚Äúexecutable‚Äù, ‚Äúscanner‚Äù, ‚Äúmalicious‚Äù are all in the area‚Ä¶ And with the same color!\nWith more text and better cleanup‚Ä¶ I‚Äôm convinced the approach has its merits üôÇ\nAs per 3D visuals, this time using Multi-dimensional Scaling (another algorithm that uses distances!) to project onto 3 dimensions, well it works, but there is a bit too much data, and maybe it‚Äôs not super super useful‚Ä¶ Still:"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#applications",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#applications",
    "title": "ML Concepts: Word Embeddings",
    "section": "Applications",
    "text": "Applications\nWhat if instead of Wiki entries from the internet, we had taken CVE text (maybe along with their CVSS (or whatever scoring system you prefer))? We could probably do some sort of regression once we encode the text (maybe even mixing things up with other algorithms) and maybe estimate the score?\nHow about classifying threat alerts into groups?\nWhat if we had taken logs from a machine. Could we maybe use all this and find specially anomalous logs, from the complete log file? (Imagine thousands of lines reviewed in seconds by your laptop like so‚Ä¶)\nAnd consider this: Over this and the last Blog entry, we‚Äôve discussed enough to do some basic ML. But there is much more than Clustering applied to text. I just hope this helps give a hint of the possibilities. üôÇ"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#conclusion",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#conclusion",
    "title": "ML Concepts: Word Embeddings",
    "section": "Conclusion",
    "text": "Conclusion\nAt this stage, I truly hope we understand what it means for us to be able to put words (or texts, they could be each files‚Ä¶ why not!) into vectors (i.e.¬†points in N-dimensional space), to be projected (or not) and for which distances can be calculated to other words/texts.\nWith that, we open a world of possibilities: Topic Modelling, Sentiment Analysis, etc. can all be done using distances between points üôÇ (There is more to NLP, though, and NOT only LLMs, please. More simple/traditional stuff is out there! One example I wrote about forever ago was part of speech tagging, for instance. Another time I used TF-IDF to model a classifier of log files with supervised learning‚Ä¶) Maybe in a future post I‚Äôll discuss more of these concepts, but I‚Äôd be very happy for now if somehow I helped someone out there understand a bit better how these things could actually work.\nBy the by: GenAI essentially does self-supervised learning on word embeddings. (WOW! What a bomb to leave at the end of a blog entry, ‚Äúself-supervised‚Äù???)."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#bonus-a-word-about-genai",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#bonus-a-word-about-genai",
    "title": "ML Concepts: Word Embeddings",
    "section": "Bonus: A word about GenAI",
    "text": "Bonus: A word about GenAI\nOK, OK, OK‚Ä¶ But real quick then.\nFirst: I don‚Äôt particularly like GenAI as a topic because ‚Äì mostly ‚Äì of the hype, misunderstanding, risks‚Ä¶ but otherwise is undoubtedly incredibly powerful and I‚Äôll admit it must have some cool applications‚Ä¶ For expert users! And although very slowly, I myself am considering using it as an assistant R-coder‚Ä¶ I have done tests and it‚Äôs not bad at all, and it WOULD make me much faster‚Ä¶ But I‚Äôm mostly resisting for now: Coding and thinking how to approach a problem is what I like, so why externalize that‚Ä¶ Unless I really HAVE to‚Ä¶\nThat said‚Ä¶ What the heck is ‚Äúself-supervised‚Äù learning?¬†\nSupervised learning needs to have a means of knowing whether it‚Äôs doing a good job to rectify its own behaviour while in training.\nIf your job is to predict the best next word for a given text‚Ä¶ All you need is to try to predict it, and then read the next word (or ‚Äútoken‚Äù). If you guessed wrong, you rectify your behaviour for your next guess. Then you read the next word‚Ä¶ And iterate. And in the above scenario, nobody needs to ‚Äútag‚Äù anything, the information is self-contained! So you just ingest text one ‚Äútoken‚Äù at a time (don‚Äôt worry, say ‚Äúone word at a time‚Äù, and more or less you‚Äôre good). All you need is text (and ‚Äúattention‚Äù, but that‚Äôs WAY beyond today‚Äôs objectives :D).\nThe more text, the more training examples you get üôÇ\nAnd yes: Your words/tokens, are presented to your GenAI (well, LLMs, really) as‚Ä¶ Embbedings."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "This is quite literally my first Reinforcement Learning exercise ever. Coded from scratch. With no Neural Network whatsoever, but instead, using my own implementation of the Learning Classifier System(s) I have been discussing lately.\nAnd the point is this: It works!\nHere I present the first actual working results.\n\n\n\nSo we have an agent (blue dot) that can move in any of {up, down, right, left} direction.\nWe have a simulated world, with walls (black dots), some rewarding food (green dots) and some hurting enemies/poison (red dots).\nIndeed the aestetics of this demo are not marvelous, but I‚Äôve never cared much about look&feel, I care about what happens under the hood here.\n\n\n\nmonitoring the learning process\n\n\nSo our agent moves, and gets a reward as informed by our world. For instance:\n\nNo reward if moving to an empty cell\nSmall negative reward if moving backwards (to the cell it was in at immediately anterior step)\nsomewhat larger negative reward if ‚Äúbumping‚Äù into a wall\nlarge positive reward if finding food (obviously)\nand large negative reward if moving onto an enemy/poison cell (duh!)\n\nThe trick is: We do NOT tell the agent what to do! It must learn how to behave on its own, just by maximizing its reward.\nAnd here, the agent will use my implementation of a Learning Classifier System. No neural network or deep learning or anything fancy like that :D\nOh, and every line of code is in R.\n\n\n\nSo there is a bit more to it. I will not explain today in detail my whole implementation of an LCS algorithm to do RL. But some of the logic available to the agent is important to understand current results:\n\nI chose to implement a ‚Äúsmall memory‚Äù for my agent, so that a reward will inform the current step and be ‚Äúpassed onto‚Äù the last action, divided by 2.\nThe reason to do that is the following: the agent only sees one cell in all 8 directions (inlcuding diagonals), and it can move in 4 main directions. The idea of the small memory was to allow the machine to learn rules to choose to ‚Äúmove diagonally‚Äù. I haven‚Äôt validated exactly if this is the result. But it seems the agent does work better with that little bit of ‚Äúmemory‚Äù.\n\nI‚Äôll discuss the complete algorithm later on, but today let‚Äôs look at the result in a short video!\n\n\n\nHere is a small video I made of the current results‚Ä¶\n\n\n\n\nI am unclear at this point about whether I just implemented a version ‚Äì or a mix ‚Äì of TD, Q-Learning, XCS, CS01‚Ä¶ That‚Äôs the theory part, and I‚Äôll need to dig deeper into that now, to frame better what I just created.\n(Also, the code is very‚Ä¶ messy, right now :D I just wanted to see if I could get it to work‚Ä¶)\nBut from an experimental perspective, this was very satisfying already."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#reinforcement-learning-with-lcs",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#reinforcement-learning-with-lcs",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "This is quite literally my first Reinforcement Learning exercise ever. Coded from scratch. With no Neural Network whatsoever, but instead, using my own implementation of the Learning Classifier System(s) I have been discussing lately.\nAnd the point is this: It works!\nHere I present the first actual working results."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#limited-agent-simple-rules",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#limited-agent-simple-rules",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "So we have an agent (blue dot) that can move in any of {up, down, right, left} direction.\nWe have a simulated world, with walls (black dots), some rewarding food (green dots) and some hurting enemies/poison (red dots).\nIndeed the aestetics of this demo are not marvelous, but I‚Äôve never cared much about look&feel, I care about what happens under the hood here.\n\n\n\nmonitoring the learning process\n\n\nSo our agent moves, and gets a reward as informed by our world. For instance:\n\nNo reward if moving to an empty cell\nSmall negative reward if moving backwards (to the cell it was in at immediately anterior step)\nsomewhat larger negative reward if ‚Äúbumping‚Äù into a wall\nlarge positive reward if finding food (obviously)\nand large negative reward if moving onto an enemy/poison cell (duh!)\n\nThe trick is: We do NOT tell the agent what to do! It must learn how to behave on its own, just by maximizing its reward.\nAnd here, the agent will use my implementation of a Learning Classifier System. No neural network or deep learning or anything fancy like that :D\nOh, and every line of code is in R."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#a-few-more-details",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#a-few-more-details",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "So there is a bit more to it. I will not explain today in detail my whole implementation of an LCS algorithm to do RL. But some of the logic available to the agent is important to understand current results:\n\nI chose to implement a ‚Äúsmall memory‚Äù for my agent, so that a reward will inform the current step and be ‚Äúpassed onto‚Äù the last action, divided by 2.\nThe reason to do that is the following: the agent only sees one cell in all 8 directions (inlcuding diagonals), and it can move in 4 main directions. The idea of the small memory was to allow the machine to learn rules to choose to ‚Äúmove diagonally‚Äù. I haven‚Äôt validated exactly if this is the result. But it seems the agent does work better with that little bit of ‚Äúmemory‚Äù.\n\nI‚Äôll discuss the complete algorithm later on, but today let‚Äôs look at the result in a short video!"
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#the-result",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#the-result",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "Here is a small video I made of the current results‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#conclusions",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#conclusions",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "I am unclear at this point about whether I just implemented a version ‚Äì or a mix ‚Äì of TD, Q-Learning, XCS, CS01‚Ä¶ That‚Äôs the theory part, and I‚Äôll need to dig deeper into that now, to frame better what I just created.\n(Also, the code is very‚Ä¶ messy, right now :D I just wanted to see if I could get it to work‚Ä¶)\nBut from an experimental perspective, this was very satisfying already."
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html",
    "title": "RLCS: Improving‚Ä¶ And getting worse (at the same time!)",
    "section": "",
    "text": "Well, it‚Äôs been a couple of weeks (I was a bit offline last couple of weekends, I guess). So I‚Äôm back at it and, first, I needed to clean things up. It just was messy code.\nSo I did that, removed some duplicate code with functions, unused variables, found out some of it was in fact quite wrong‚Ä¶\nHeck, I half-wonder how it actually worked so well a couple of weeks back learning, that simple agent in that simple small ‚Äúworld‚Äù moving around looking for ‚Äúfood‚Äù‚Ä¶\nI also studied a bit more on RL (although, I have not read as much as I wanted, I had a couple of other books under way‚Ä¶ Anyhow), and modified things so that the reward distribution would better follow the ideas of temporal difference (TD).\nNow code is a bit cleaner, probably running smoother, less cluttered‚Ä¶\nBUT!\n\n\n\nAs it turns out, in fixing the code, I made the learning worse‚Ä¶ It still kind-of works, but from the last few runs, it‚Ä¶ well, it feels it learns worse.\nAnd I‚Äôm really wondering which of the many parameters I have changed, on top of the fixes, have made it bad.\n\n\n\nThere is still stuff to clean. I‚Äôm not advancing as much as I wanted with setting things up in an actual package yet. And while the Supervised Learning is working fine (maybe even better), the Reinforcement Learning example is worse off after this weekend.\nNo matter, I‚Äôll review again. And I‚Äôll fix it. I‚Äôm just slower than I wanted. Oh well‚Ä¶"
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#better-code",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#better-code",
    "title": "RLCS: Improving‚Ä¶ And getting worse (at the same time!)",
    "section": "",
    "text": "Well, it‚Äôs been a couple of weeks (I was a bit offline last couple of weekends, I guess). So I‚Äôm back at it and, first, I needed to clean things up. It just was messy code.\nSo I did that, removed some duplicate code with functions, unused variables, found out some of it was in fact quite wrong‚Ä¶\nHeck, I half-wonder how it actually worked so well a couple of weeks back learning, that simple agent in that simple small ‚Äúworld‚Äù moving around looking for ‚Äúfood‚Äù‚Ä¶\nI also studied a bit more on RL (although, I have not read as much as I wanted, I had a couple of other books under way‚Ä¶ Anyhow), and modified things so that the reward distribution would better follow the ideas of temporal difference (TD).\nNow code is a bit cleaner, probably running smoother, less cluttered‚Ä¶\nBUT!"
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#worse-results",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#worse-results",
    "title": "RLCS: Improving‚Ä¶ And getting worse (at the same time!)",
    "section": "",
    "text": "As it turns out, in fixing the code, I made the learning worse‚Ä¶ It still kind-of works, but from the last few runs, it‚Ä¶ well, it feels it learns worse.\nAnd I‚Äôm really wondering which of the many parameters I have changed, on top of the fixes, have made it bad."
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#conclusions",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#conclusions",
    "title": "RLCS: Improving‚Ä¶ And getting worse (at the same time!)",
    "section": "",
    "text": "There is still stuff to clean. I‚Äôm not advancing as much as I wanted with setting things up in an actual package yet. And while the Supervised Learning is working fine (maybe even better), the Reinforcement Learning example is worse off after this weekend.\nNo matter, I‚Äôll review again. And I‚Äôll fix it. I‚Äôm just slower than I wanted. Oh well‚Ä¶"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "So I wasn‚Äôt kidding. I really like this ‚ÄúLearning Classifier System‚Äù (aka LCS) idea, and I really want to develop the R package for it, which I will call ‚ÄúRLCS‚Äù (clever, hu?).\nNow I have to start somewhere, and so I will keep it simple for now. I will first focus on an LCS that does supervised learning only (that means, no reinforcement learning for now.)\nAlso, I will focus on the simplest binary environments. In LCS lingo, for supervised classification, environment is equivalent to saying dataset. Each entry to classify is presented as binary strings, and the class (aka action) is, for simplicity, also a simple 0 or 1.\nSo binary classification of binary strings.\nThis might sound simplistic (and it is somewhat, yes) but the feature engineering and codificatioin into GA-compatible individuals is often harder than the actual training of a ML algorithm.\nAnyhow. I will forego some details for now, but suffice to say I‚Äôm working right now towards a Michigan-style LCS.\n\n\n\nNow we will receive an environment/dataset as input that has ‚Äústates‚Äù in binary format, and actions/classes as either 0 or 1.\n## Suppose test data is:\ntest_environment &lt;- data.frame(state = c('00011', '10010', '01110', '00001'),\n                               class = c(0, 0, 0, 1))\n(It‚Äôs not enough info to conclude that, but these examples all agree with a classifier that would say, in human readable format: ‚ÄúThe Class is the negated bit 4 of the input‚Äù.)\nOne of the key aspects of an LCS, is that it will produce rules. A rule is a pair of a condition and an action.\nA condition in turn is written (in our current approach) as a string of a ternary alphabet, namely characters from the set {0, 1, #}, where # means wildcard and matches either 0 or 1.\nNow for each input of the environment (each entry of the dataset), for today‚Äôs purposes, we will want to know which rules it matches (regardless of the action). In other words, we want to match an input to condition(s). That will tell us the ‚ÄúMatch set‚Äù.\nMatching is one key aspect of the LCS overall algorithm, one that will be run a lot, and therefore I worry that it could be slow.\nA couple more keywords: A classifier is a rule and its statistics. (We will skip the statistics part for today.) And a population is our set of classifiers.\nShort of going the way of C++, I have coded one first simplistic function that takes a classifier set as a dataframe, and another one that uses a list of lists instead.\nConditions can easily be written as strings:\nconditions &lt;- c('#001#', '10#10', '##110', '###0#')\nactions &lt;- c(0, 0, 0, 1)\nFor the first approach, it was quite straightforward:\nlcs_classifier1 &lt;- data.frame(id = 1:4,\n                      condition_string = conditions,\n                      action = actions, #simplest is binary for supervised\n                      accuracy = 1,\n                      numerosity = 1,\n                      first_seen = c(1, 2, 3, 4) ## I'm inventing a coverage trigger\n                      )\n\n## Find Matching Rules\nget_match_set_v001 &lt;- function(ti_condition, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- unlist(strsplit(ti_condition, \"\"))\n    pop &lt;- pop[, c(\"id\", \"condition_string\")] # Same\n\n    which(sapply(1:nrow(pop), function(i) {\n\n        t_cond &lt;- unlist(strsplit(pop$condition_string[i], \"\"))\n        relevant_positions &lt;- which(t_cond %in% c(\"0\", \"1\"))\n        \n        all(t_cond[relevant_positions] == ti_cond[relevant_positions])\n    }))\n}\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v001(test_environment$state[1], lcs_classifier1), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v001(test_environment$state[2], lcs_classifier1),\n        c(1, 2)\n    )\n})\n\n(OUTPUT:) Test passed ü•≥\nFor the second approach, I needed some more supporting functions:\n######\n## Option 2: Use list instead of Data.frame and pre-calculate match sets\n\n## Function to create a new rule from a condition string and an action\n## THIS IS VALID FOR CLASSIFICATION, NOT FOR Reinforcement Learning!!!\n## In our case, a new rule has: A condition string, an (correct!) action\n## Since action is correct, this is useful for future \"covering\"\n## AND we set accuracy to 1\nnew_supervised_rule &lt;- function(condition_string, action) {\n    list(id = 1,\n         condition_string = condition_string,\n         condition_length = nchar(condition_string),\n         condition_list = list(\"0\" = which(unlist(strsplit(condition_string, \"\")) == \"0\"),\n                               \"1\" = which(unlist(strsplit(condition_string, \"\")) == \"1\")),\n         action = action,\n         accuracy = 1,\n         numerosity = 1,\n         first_seen = 1)\n}\n\n## Function to return max id from a classifier set in list format\nmax_id_in_classifier &lt;- function(pop) {\n    max(sapply(pop, function(x) x$id))\n}\n\n## Function to add rule to a population.\n## date_rule_born will be useful stat for future, setting as parameter for now.\nadd_valid_rule_to_pop &lt;- function(pop, condition_string, action, date_rule_born = 1) {\n    \n    t_rule &lt;- new_supervised_rule(condition_string, action)\n    t_rule$id &lt;- max_id_in_classifier(pop)+1\n    t_rule$first_seen &lt;- date_rule_born\n    \n    pop[[length(pop)+1]] &lt;- t_rule\n    \n    pop\n}\n\n## Support function for human-compatible printing:\nmake_pop_printable &lt;- function(classifier) {\n    data.frame\n    rbind.fill(lapply(1:length(classifier), function(i) {\n        t_c &lt;- classifier[[i]]\n        data.frame(id = t_c$id,\n                   condition = t_c$condition_string,\n                   action = t_c$action,\n                   accuracy = t_c$accuracy,\n                   numerosity = t_c$numerosity,\n                   first_seen = t_c$first_seen)\n    }))\n}\n\n## Matching with list formatted population.\nget_match_set_v002 &lt;- function(input_state, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- as.integer(unlist(strsplit(input_state, \"\")))\n    \n    which(sapply(1:length(pop), function(i) {\n        rule &lt;- pop[[i]]$condition_list\n        all(ti_cond[rule$'0'] == 0, ti_cond[rule$'1'] == 1)\n    }))\n}\n\nlcs_classifier2 &lt;- list(new_supervised_rule(conditions[1], actions[1]))\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[2], actions[2])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[3], actions[3])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[4], actions[4])\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v002(test_environment$state[1], lcs_classifier2), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v002(test_environment$state[2], lcs_classifier2),\n        c(1, 2)\n    )\n})\n\n\n\nAn example population\n\n\n\n\n\nmicrobenchmark(\n    sapply(test_environment$state, function(x) { get_match_set_v001(x, lcs_classifier1)}),\n    sapply(test_environment$state, function(x) { get_match_set_v002(x, lcs_classifier2)}),\n    times = 10000L\n)\n\n\n\nlists and pre-calculated match positions is faster\n\n\nLists are more flexible and data.frames are not the fastest option out there anyway. Here the pre-calculations in the second format also helps doing faster matching.\nHowever I know this choice might have an impact, particularly when time will come to implement deletion, subsumption and compaction (all useful for better LCS performance/value). And another idea I have in mind will require further lists management (but it‚Äôs too early to discuss that).\nBut that‚Äôs a problem for future me (and nothing that hasn‚Äôt been solved already).\n\n\n\nWell well: Day 1, and we have covered concepts of environment, states, actions, conditions, rules, classifiers, population and matching/match set.\nAlong with developping a functional (basic) list for storing classifiers, and a matching function.\nNot too bad (albeit all too simplistic and not going for ‚Äúfull-power‚Äù flexible LCS implementation).\n\n\n\nI‚Äôll keep referring to this video (although the book is better to get acquainted with the vocabulary and concepts): intro explaining the algorithm ."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#intro",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#intro",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "So I wasn‚Äôt kidding. I really like this ‚ÄúLearning Classifier System‚Äù (aka LCS) idea, and I really want to develop the R package for it, which I will call ‚ÄúRLCS‚Äù (clever, hu?).\nNow I have to start somewhere, and so I will keep it simple for now. I will first focus on an LCS that does supervised learning only (that means, no reinforcement learning for now.)\nAlso, I will focus on the simplest binary environments. In LCS lingo, for supervised classification, environment is equivalent to saying dataset. Each entry to classify is presented as binary strings, and the class (aka action) is, for simplicity, also a simple 0 or 1.\nSo binary classification of binary strings.\nThis might sound simplistic (and it is somewhat, yes) but the feature engineering and codificatioin into GA-compatible individuals is often harder than the actual training of a ML algorithm.\nAnyhow. I will forego some details for now, but suffice to say I‚Äôm working right now towards a Michigan-style LCS."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#the-ternary-alphabet-for-conditions",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#the-ternary-alphabet-for-conditions",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "Now we will receive an environment/dataset as input that has ‚Äústates‚Äù in binary format, and actions/classes as either 0 or 1.\n## Suppose test data is:\ntest_environment &lt;- data.frame(state = c('00011', '10010', '01110', '00001'),\n                               class = c(0, 0, 0, 1))\n(It‚Äôs not enough info to conclude that, but these examples all agree with a classifier that would say, in human readable format: ‚ÄúThe Class is the negated bit 4 of the input‚Äù.)\nOne of the key aspects of an LCS, is that it will produce rules. A rule is a pair of a condition and an action.\nA condition in turn is written (in our current approach) as a string of a ternary alphabet, namely characters from the set {0, 1, #}, where # means wildcard and matches either 0 or 1.\nNow for each input of the environment (each entry of the dataset), for today‚Äôs purposes, we will want to know which rules it matches (regardless of the action). In other words, we want to match an input to condition(s). That will tell us the ‚ÄúMatch set‚Äù.\nMatching is one key aspect of the LCS overall algorithm, one that will be run a lot, and therefore I worry that it could be slow.\nA couple more keywords: A classifier is a rule and its statistics. (We will skip the statistics part for today.) And a population is our set of classifiers.\nShort of going the way of C++, I have coded one first simplistic function that takes a classifier set as a dataframe, and another one that uses a list of lists instead.\nConditions can easily be written as strings:\nconditions &lt;- c('#001#', '10#10', '##110', '###0#')\nactions &lt;- c(0, 0, 0, 1)\nFor the first approach, it was quite straightforward:\nlcs_classifier1 &lt;- data.frame(id = 1:4,\n                      condition_string = conditions,\n                      action = actions, #simplest is binary for supervised\n                      accuracy = 1,\n                      numerosity = 1,\n                      first_seen = c(1, 2, 3, 4) ## I'm inventing a coverage trigger\n                      )\n\n## Find Matching Rules\nget_match_set_v001 &lt;- function(ti_condition, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- unlist(strsplit(ti_condition, \"\"))\n    pop &lt;- pop[, c(\"id\", \"condition_string\")] # Same\n\n    which(sapply(1:nrow(pop), function(i) {\n\n        t_cond &lt;- unlist(strsplit(pop$condition_string[i], \"\"))\n        relevant_positions &lt;- which(t_cond %in% c(\"0\", \"1\"))\n        \n        all(t_cond[relevant_positions] == ti_cond[relevant_positions])\n    }))\n}\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v001(test_environment$state[1], lcs_classifier1), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v001(test_environment$state[2], lcs_classifier1),\n        c(1, 2)\n    )\n})\n\n(OUTPUT:) Test passed ü•≥\nFor the second approach, I needed some more supporting functions:\n######\n## Option 2: Use list instead of Data.frame and pre-calculate match sets\n\n## Function to create a new rule from a condition string and an action\n## THIS IS VALID FOR CLASSIFICATION, NOT FOR Reinforcement Learning!!!\n## In our case, a new rule has: A condition string, an (correct!) action\n## Since action is correct, this is useful for future \"covering\"\n## AND we set accuracy to 1\nnew_supervised_rule &lt;- function(condition_string, action) {\n    list(id = 1,\n         condition_string = condition_string,\n         condition_length = nchar(condition_string),\n         condition_list = list(\"0\" = which(unlist(strsplit(condition_string, \"\")) == \"0\"),\n                               \"1\" = which(unlist(strsplit(condition_string, \"\")) == \"1\")),\n         action = action,\n         accuracy = 1,\n         numerosity = 1,\n         first_seen = 1)\n}\n\n## Function to return max id from a classifier set in list format\nmax_id_in_classifier &lt;- function(pop) {\n    max(sapply(pop, function(x) x$id))\n}\n\n## Function to add rule to a population.\n## date_rule_born will be useful stat for future, setting as parameter for now.\nadd_valid_rule_to_pop &lt;- function(pop, condition_string, action, date_rule_born = 1) {\n    \n    t_rule &lt;- new_supervised_rule(condition_string, action)\n    t_rule$id &lt;- max_id_in_classifier(pop)+1\n    t_rule$first_seen &lt;- date_rule_born\n    \n    pop[[length(pop)+1]] &lt;- t_rule\n    \n    pop\n}\n\n## Support function for human-compatible printing:\nmake_pop_printable &lt;- function(classifier) {\n    data.frame\n    rbind.fill(lapply(1:length(classifier), function(i) {\n        t_c &lt;- classifier[[i]]\n        data.frame(id = t_c$id,\n                   condition = t_c$condition_string,\n                   action = t_c$action,\n                   accuracy = t_c$accuracy,\n                   numerosity = t_c$numerosity,\n                   first_seen = t_c$first_seen)\n    }))\n}\n\n## Matching with list formatted population.\nget_match_set_v002 &lt;- function(input_state, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- as.integer(unlist(strsplit(input_state, \"\")))\n    \n    which(sapply(1:length(pop), function(i) {\n        rule &lt;- pop[[i]]$condition_list\n        all(ti_cond[rule$'0'] == 0, ti_cond[rule$'1'] == 1)\n    }))\n}\n\nlcs_classifier2 &lt;- list(new_supervised_rule(conditions[1], actions[1]))\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[2], actions[2])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[3], actions[3])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[4], actions[4])\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v002(test_environment$state[1], lcs_classifier2), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v002(test_environment$state[2], lcs_classifier2),\n        c(1, 2)\n    )\n})\n\n\n\nAn example population"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#well-keep-the-list-format-for-now",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#well-keep-the-list-format-for-now",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "microbenchmark(\n    sapply(test_environment$state, function(x) { get_match_set_v001(x, lcs_classifier1)}),\n    sapply(test_environment$state, function(x) { get_match_set_v002(x, lcs_classifier2)}),\n    times = 10000L\n)\n\n\n\nlists and pre-calculated match positions is faster\n\n\nLists are more flexible and data.frames are not the fastest option out there anyway. Here the pre-calculations in the second format also helps doing faster matching.\nHowever I know this choice might have an impact, particularly when time will come to implement deletion, subsumption and compaction (all useful for better LCS performance/value). And another idea I have in mind will require further lists management (but it‚Äôs too early to discuss that).\nBut that‚Äôs a problem for future me (and nothing that hasn‚Äôt been solved already)."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#conclusions",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#conclusions",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "Well well: Day 1, and we have covered concepts of environment, states, actions, conditions, rules, classifiers, population and matching/match set.\nAlong with developping a functional (basic) list for storing classifiers, and a matching function.\nNot too bad (albeit all too simplistic and not going for ‚Äúfull-power‚Äù flexible LCS implementation)."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#references",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#references",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "I‚Äôll keep referring to this video (although the book is better to get acquainted with the vocabulary and concepts): intro explaining the algorithm ."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "This is probably one of the last entries from my preparation to introduce ‚ÄúBackground of ML for Cybersecurity‚Äù. This time, it‚Äôs really about what should probably have been the first entry of the series. Also, it‚Äôs not really put in context of Cybersecurity: I‚Äôm just trying to show one needs not be afraid about the math.\nI‚Äôm having lots of doubts about this one, too: Can I even use the ‚ÄúMachine Learning‚Äù tag? After all, this predates ML by quite a bit. It‚Äôs really of the realm of statistics. Then again, the limit of what qualifies as ML and what doesn‚Äôt is somewhat blurry (at least to me).\nAnd some of it is very very simple, and maybe shouldn‚Äôt warrant writing about it. But I like to write things down, it helps organize my thoughts sometimes, and I believe in the idea that really understanding the basics is helpful to grasp concepts when things get more complicated.\n\n\nIF (and that‚Äôs a reasonably big if) you can find a linear relationship between two numerical vectors, then using a linear regression can help ‚Äúencode‚Äù the whole pair of vectors through ‚Äújust‚Äù two numbers: A slope, and an intercept.\nIn effect, you‚Äôre ‚Äúmodelling‚Äù a relationship - and in doing so, you‚Äôre actually compressing information.\nMore to the point, if you have an intuition that such relationship holds true for other points you were not given, then you can interpolate (if the new points are in the range of original vectors - otherwise it‚Äôs called ‚Äúextrapolation‚Äù) an expected output from a given input.\nIn other words: If you know (or expect) there is a linear (straight line) relationship between a webserver load and number of concurrently opened sessions, you could estimate a load (say % CPU) from knowing only the number of users currently connected (or the other way around, obviously). You‚Äôd expect the load to grow with the number of users.\nThe concept of regression (linear or otherwise) are important in ML. It‚Äôs a core part of most models. You doubt? Let me put it this way: Regression is a core part of how training a neural network works. There. Believe me now? (I guess I just justified myself in tagging this entry as Machine Learning, after all.)\n\n\n\nWe‚Äôll start off very easy today. Imagine to begin with you only have two points. So two pairs\\((x_1, y_1), (x_2, y_2)\\) of coordinates then. IF (again, big IF) you somehow expects a straight line could be used to estimate a third \\(y_3\\) from a third \\(x_3\\) coordinate in the range of the two original points‚Ä¶ How can you estimate said value of \\(y_3\\).\nWell, it‚Äôs basic (elementary? high school? Idk) math, you want to find \\(a, b\\) in the following:\n\\[\ny = ax + b\n\\]\nThere are two numbers to find (a slope, and an intercept), and we have two points. That‚Äôs a system of two equations. But instead of just doing the math, let‚Äôs think about this from a conceptual perspective.\nYou can probably skip this, as it‚Äôs really very basic, but I like to make sure I understand things, not just use the math I can find out there, and so I‚Äôm going to explain it nevertheless.\nWhat is a ‚Äúslope‚Äù?\nThink of a bicycle ride, and you‚Äôre faced with a 8% upwards (so ‚Äúpositive‚Äù) slope; what does that mean? That for every 100 meters you advance horizontally, you should expect to gain 8 meters in elevation. So if you gain 8 units in the y direction, you have moved 100 units in the x direction.\nWith two points: \\(y_2 - y_1 = 8, x_2 - x_1 = 100\\). And you get to the slope by dividing both numbers (which is why we expressed the slope in percentage). So you need then:\n\\[\na = slope = {(y_2 - y_1) \\over (x_2 - x_1)}\n\\]\nNot soooo misterious now, is it? If the slope is downwards, \\(y_2 &lt; y_1\\) and \\(a &lt; 0\\), that‚Äôs all.\nAnd what is the ‚Äúintercept‚Äù?\nIf you show lines on a two dimensional graph, you often will see the \\(y\\) and \\(x\\) axis. The ‚Äúintercept‚Äù is simply the point at which our line crosses (‚Äúintercepts‚Äù) the \\(y\\) axis. Which happens when \\(x = 0\\).\nAnd by now, if we take any of our points, we have the following bits \\(y_1, a, x_1\\) for our equation \\(y = ax + b\\). Can we get to \\(b\\)?\n\\[\nb = y_1 - a x_1\n\\]So that for \\(x = 0\\), we have \\(b\\) as the value of \\(y\\). Which is what we were looking for.\nThis was very very very basic stuff. But hopefully for those that don‚Äôt do math in their day-to-day life, or those that use formulas without thinking (or use Excel to estimate these things, if at all), or simply as a refresher‚Ä¶ I don‚Äôt know. Sorry if that was boring.\n\n## Suppose you have any two points:\nmy_x &lt;- c(1.5, 2.5)\nmy_y &lt;- c(5, 2)\n\n# Line goes through two points\n## By definition of \"slope\":\nmy_line_slope &lt;- (my_y[2] - my_y[1]) / (my_x[2]- my_x[1])\n## Then just replace with one point to get intercept:\nmy_intercept &lt;- my_y[1] - (my_line_slope * my_x[1])\n\ncalculate_resulting_y &lt;- function(x) {\n    my_line_slope * x + my_intercept\n}\n\n## Say then we have any value of x (\"independent variable\"):\nmy_interpolation_x_set &lt;- seq(0, 5, 0.1)\n## In a perfect World, we could then get any dependent value:\nmy_interpolation_y_set &lt;- sapply(my_interpolation_x_set, calculate_resulting_y)\n\nplot(my_interpolation_y_set ~ my_interpolation_x_set, col = \"blue\", type=\"l\")\npoints(my_y ~ my_x, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nIn the above, we took a pretty big leap of faith: We just somehow knew/assumed that two points would represent a linear relationship.\nBut we‚Äôd probably feel more confident if we had more points to confirm our expectations first.\nToday we‚Äôll stay with the ‚ÄúSimple Linear Regression‚Äù, which is nicely explained in the Wikipedia.\nWe want to find \\(\\alpha, \\beta\\) that best describe not two points but instead estimate a line for several of them, so that we solve \\(y = \\alpha + \\beta x\\) that best estimate all \\(x_i, y_i\\) pairs we‚Äôre given (provided they look like a straight line).\nAnd I‚Äôve said ‚Äúestimate‚Äù twice above, as there will be some error, and that‚Äôs key here: We essentially try to minimize the error that our estimated \\(\\alpha, \\beta\\) introduce, for each point. Written ‚Äúfancily‚Äù:\n\\[\n\\widehat{\\epsilon}_i = y_i - (\\alpha + \\beta x_i), \\forall i \\in [1..n], n \\in \\mathbb{N}\n\\]\nWhere n is the number of your available points‚Ä¶\n\nMinimizing the error is an important concept in supervised machine learning.\n\nThen using the ‚ÄúSum of Square‚Äù of the residuals, we‚Äôre going to try to minimize that.\n\n\n\nI will take the time to dive into this a bit, as it is relevant to understand for other cases. Although the math is better explained here, that approach of minimizing sum of squares is useful in other contexts and so here goes.\nSo today we are trying to ‚Äúfit‚Äù a line to a set of points that are not necessarily exactly on said line (see ‚Äúthe result‚Äù below). But where do we start? How do we go about finding one line out of an infinite set of possible lines?\nMore or less, we find the parameters for our line equation that minimizes overall distance from all the points. So we try to minimize the sum of the error of our line from each point. It is a minimization exercise, but sometimes the points will be under the line, and others they‚Äôll be above. To correct for that, we actually minimize the sum of distances, each one squared! That way, all values are positive, and we really try to minimize something.\nThere is another important point hidden right here: The key to understand ‚ÄúGradient Descent‚Äù!\nAs we will see with the ‚Äúresults‚Äù below, the errors, when squared, are shaped in a sort of U. But what‚Äôs important is that the function behind that (parabolic) shape is that it‚Äôs differentiable.\nDifferentiating (that‚Äôs calculus) here will mean that we know in which direction (up, down, horizontal) said curve goes for a given point on the x axis. If we know that, we know that if the curve goes upwards, to minimize the thing, we want to go in the opposite direction.\nAnd that‚Äôs exactly what ‚Äúdescending the gradient‚Äù is all about (although in multivariate settings: A gradient in vector calculus is ‚Äúthe direction and the rate of fastest increase‚Äù, so to ‚Äúdescend a gradient‚Äù, you want to move in the opposite direction).\nFor linear regression, it‚Äôs cool because we can do partial derivatives (as shown on the referenced Wiki page above) to find our \\(\\alpha\\) and \\(\\beta\\) parameters that describe the line that ‚Äúbest fit the data‚Äù.\nIt‚Äôs quite clever, mind you: You minimize the error of your predicted value compared to the actual value. That‚Äôs how your machine can learn! But it‚Äôs also what people have been using to train Neural Networks!\nFor today, let‚Äôs just store the following information: The ‚ÄúLeast Square Minimization‚Äù can help find a minimum error through differentiation.\nAnd for a future conversation: Gradient Descent (hidden in today‚Äôs Sum of Square Minimization) is one of the two mathematical keys for back-propagation. (And back-propagation was the before and after of connectionism in Machine Learning theory).\nAlright, let‚Äôs move on and return to our very simple example.\n\n\n\nRemember how we said at some point that in ML, more data points is often better?\nWe‚Äôre going to find the line that best fits a set of points (I know a simple straight line will work here, because I‚Äôm the one generating said line‚Ä¶ :D)\nWe‚Äôre also going to show the square of errors, how it looks like a U shape, and how it‚Äôs clear there is a minimum to it‚Ä¶\n\n## Some points that can be used:\nmy_regression_x_set &lt;- seq(0, 5, 0.1) + rnorm(51,mean = 0, sd = 0.2)\nmy_regression_y_set &lt;- my_interpolation_y_set + rnorm(51, mean = 0, sd = 0.2)\n\n## SIMPLE linear regression. Using Least Squares distance.\n## Key idea: Minimize differences.\n## Find minimum for error function using differentiation.\n## Final equations here instead of step by step explaining how to get there.\n\n## Just follow the math:\nmean_reg_x &lt;- mean(my_regression_x_set)\nmean_reg_y &lt;- mean(my_regression_y_set)\n\ndev_x &lt;- (my_regression_x_set - mean_reg_x)\ndev_y &lt;- (my_regression_y_set - mean_reg_y)\n\nbeta_est &lt;- sum(dev_x * dev_y) / sum(dev_x^2)\nalpha_est &lt;- mean_reg_y - (beta_est * mean_reg_x)\n\nplot(my_regression_y_set ~ my_regression_x_set)\n## alpha & beta can be confusing, I usually use Beta as multipliers of X...\nabline(a = alpha_est, b = beta_est, col=\"blue\")\n\n\n\n\n\n\n\n## Importantly, squared error is differentiable and has a global minimum\n## \"easily\". Related to \"Gradient Descent\"\nerr_x &lt;- my_regression_y_set - (alpha_est + my_regression_x_set * beta_est)\nerr_y &lt;- sapply(err_x, \\(x) x^2)\n\nplot(err_y ~ err_x)\n\n\n\n\n\n\n\n\nAnd that‚Äôs it! We‚Äôve used the math (skipping some steps), to show one can use it to do a linear regression (although nowadays all that stuff is available with one command, if not directly, then through packages).\nSo here we‚Äôve shown a downward slope: Let‚Äôs put it in context of a potential real-world application: Maybe our x axis represented the number of users connected to our system, and the y axis some number to represent the available resources (spare CPU or something) on a given server. If you know a linear relationship applies, you can estimate at which point you will reach 100% CPU‚Ä¶ (I could have started with that, sorry.)\nSo in the future, if you see a clear straight line between two things, well, you could use the above not-scary-at-all math approach to ‚Äúpredict‚Äù what would happen if you move along the x axis.\nLast note about the math: in this simple case, if you look at the code, we‚Äôve in fact used two statistics concepts to do the calculations: the means of the x and y axis, and the ‚Äúdeviations‚Äù of each point to said means.\nDoes it not make some intuitive sense that the line that best fits the points is related to the means and deviations of the points to said means? :)\n\n\n\nIn the above, we‚Äôve used deviations and means. These are basic concepts from statistics. And in the introduction, we have mentioned something about ‚Äúmodels‚Äù and ‚Äúcompression‚Äù. Which is what it is: Statistics are NOT the real world. And they can be misused. Here the 2 most common examples to illustrate how you can go astray with using statistics without looking at the data.\nLet‚Äôs illustrate just that, with a couple of typical (like, VERY typical) examples:\n\n\n\n\n\n\n\n\n\nAnd if it‚Äôs not clear enough, all these here below have the same mean and standard deviation:\n\n\n\n\n\n\n\n\n\n[1] \"Mean x Dino: 47.8322528169014\"\n\n\n[1] \"Mean x Bullseye: 47.8308231552178\"\n\n\n[1] \"SD x Dino: 16.7651420391168\"\n\n\n[1] \"SD x Bullseye: 16.7692394934267\"\n\n\nSee how they are basically the same numbers?\nStatistics are great, useful and very important today (and have been for quite a while, I‚Äôd argue). They‚Äôre just not always the right solution (neither is ML in a larger sense, mind you).\nAlways look at the data!\n\n\n\nWe‚Äôve gone from really REALLY simple stuff all the way to mentioning differentiation, gradient descent, statistics‚Ä¶\nAnd we‚Äôve skipped steps! (i.e.¬†solving systems of equations, which can be represented as matrices, which is relevant for completely different reasons :D)\nNot too shabby, after all‚Ä¶\n\nI‚Äôd like to go into Logistic Regression and Neural Networks a bit in a future entries. Even discuss a bit the history of ML, maybe. And that would conclude my preparation for my sessions about ‚ÄúML Background for Cybersecurity‚Äù. Surely there is more to it all, but if I manage to convey a few key concepts to my colleagues, I‚Äôll be very very happy. And this here is me trying.\nBut after that, I‚Äôll shift gears and focus on a personal project I have had in mind for some time and that is taking shape: It involves writing an R package, for a set of ML algorithms that for reasons I don‚Äôt quite understand have not received as much attention as I personally think they should. To be continued!\n\n\n\n\nThe Wikipedia for Simple Linear Regression does help.\nHere about the Gradient.\nThe math for the simple linear regression we‚Äôve used today.\nThen the more advanced discussion on Linear Regression hopefully can be followed."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#why-linear-regression-in-the-first-place",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#why-linear-regression-in-the-first-place",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "IF (and that‚Äôs a reasonably big if) you can find a linear relationship between two numerical vectors, then using a linear regression can help ‚Äúencode‚Äù the whole pair of vectors through ‚Äújust‚Äù two numbers: A slope, and an intercept.\nIn effect, you‚Äôre ‚Äúmodelling‚Äù a relationship - and in doing so, you‚Äôre actually compressing information.\nMore to the point, if you have an intuition that such relationship holds true for other points you were not given, then you can interpolate (if the new points are in the range of original vectors - otherwise it‚Äôs called ‚Äúextrapolation‚Äù) an expected output from a given input.\nIn other words: If you know (or expect) there is a linear (straight line) relationship between a webserver load and number of concurrently opened sessions, you could estimate a load (say % CPU) from knowing only the number of users currently connected (or the other way around, obviously). You‚Äôd expect the load to grow with the number of users.\nThe concept of regression (linear or otherwise) are important in ML. It‚Äôs a core part of most models. You doubt? Let me put it this way: Regression is a core part of how training a neural network works. There. Believe me now? (I guess I just justified myself in tagging this entry as Machine Learning, after all.)"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#two-points-interextrapolation",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#two-points-interextrapolation",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "We‚Äôll start off very easy today. Imagine to begin with you only have two points. So two pairs\\((x_1, y_1), (x_2, y_2)\\) of coordinates then. IF (again, big IF) you somehow expects a straight line could be used to estimate a third \\(y_3\\) from a third \\(x_3\\) coordinate in the range of the two original points‚Ä¶ How can you estimate said value of \\(y_3\\).\nWell, it‚Äôs basic (elementary? high school? Idk) math, you want to find \\(a, b\\) in the following:\n\\[\ny = ax + b\n\\]\nThere are two numbers to find (a slope, and an intercept), and we have two points. That‚Äôs a system of two equations. But instead of just doing the math, let‚Äôs think about this from a conceptual perspective.\nYou can probably skip this, as it‚Äôs really very basic, but I like to make sure I understand things, not just use the math I can find out there, and so I‚Äôm going to explain it nevertheless.\nWhat is a ‚Äúslope‚Äù?\nThink of a bicycle ride, and you‚Äôre faced with a 8% upwards (so ‚Äúpositive‚Äù) slope; what does that mean? That for every 100 meters you advance horizontally, you should expect to gain 8 meters in elevation. So if you gain 8 units in the y direction, you have moved 100 units in the x direction.\nWith two points: \\(y_2 - y_1 = 8, x_2 - x_1 = 100\\). And you get to the slope by dividing both numbers (which is why we expressed the slope in percentage). So you need then:\n\\[\na = slope = {(y_2 - y_1) \\over (x_2 - x_1)}\n\\]\nNot soooo misterious now, is it? If the slope is downwards, \\(y_2 &lt; y_1\\) and \\(a &lt; 0\\), that‚Äôs all.\nAnd what is the ‚Äúintercept‚Äù?\nIf you show lines on a two dimensional graph, you often will see the \\(y\\) and \\(x\\) axis. The ‚Äúintercept‚Äù is simply the point at which our line crosses (‚Äúintercepts‚Äù) the \\(y\\) axis. Which happens when \\(x = 0\\).\nAnd by now, if we take any of our points, we have the following bits \\(y_1, a, x_1\\) for our equation \\(y = ax + b\\). Can we get to \\(b\\)?\n\\[\nb = y_1 - a x_1\n\\]So that for \\(x = 0\\), we have \\(b\\) as the value of \\(y\\). Which is what we were looking for.\nThis was very very very basic stuff. But hopefully for those that don‚Äôt do math in their day-to-day life, or those that use formulas without thinking (or use Excel to estimate these things, if at all), or simply as a refresher‚Ä¶ I don‚Äôt know. Sorry if that was boring.\n\n## Suppose you have any two points:\nmy_x &lt;- c(1.5, 2.5)\nmy_y &lt;- c(5, 2)\n\n# Line goes through two points\n## By definition of \"slope\":\nmy_line_slope &lt;- (my_y[2] - my_y[1]) / (my_x[2]- my_x[1])\n## Then just replace with one point to get intercept:\nmy_intercept &lt;- my_y[1] - (my_line_slope * my_x[1])\n\ncalculate_resulting_y &lt;- function(x) {\n    my_line_slope * x + my_intercept\n}\n\n## Say then we have any value of x (\"independent variable\"):\nmy_interpolation_x_set &lt;- seq(0, 5, 0.1)\n## In a perfect World, we could then get any dependent value:\nmy_interpolation_y_set &lt;- sapply(my_interpolation_x_set, calculate_resulting_y)\n\nplot(my_interpolation_y_set ~ my_interpolation_x_set, col = \"blue\", type=\"l\")\npoints(my_y ~ my_x, col = \"red\")"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#multiple-points-with-a-linear-relationship",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#multiple-points-with-a-linear-relationship",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "In the above, we took a pretty big leap of faith: We just somehow knew/assumed that two points would represent a linear relationship.\nBut we‚Äôd probably feel more confident if we had more points to confirm our expectations first.\nToday we‚Äôll stay with the ‚ÄúSimple Linear Regression‚Äù, which is nicely explained in the Wikipedia.\nWe want to find \\(\\alpha, \\beta\\) that best describe not two points but instead estimate a line for several of them, so that we solve \\(y = \\alpha + \\beta x\\) that best estimate all \\(x_i, y_i\\) pairs we‚Äôre given (provided they look like a straight line).\nAnd I‚Äôve said ‚Äúestimate‚Äù twice above, as there will be some error, and that‚Äôs key here: We essentially try to minimize the error that our estimated \\(\\alpha, \\beta\\) introduce, for each point. Written ‚Äúfancily‚Äù:\n\\[\n\\widehat{\\epsilon}_i = y_i - (\\alpha + \\beta x_i), \\forall i \\in [1..n], n \\in \\mathbb{N}\n\\]\nWhere n is the number of your available points‚Ä¶\n\nMinimizing the error is an important concept in supervised machine learning.\n\nThen using the ‚ÄúSum of Square‚Äù of the residuals, we‚Äôre going to try to minimize that."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#sum-of-square-minimization",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#sum-of-square-minimization",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "I will take the time to dive into this a bit, as it is relevant to understand for other cases. Although the math is better explained here, that approach of minimizing sum of squares is useful in other contexts and so here goes.\nSo today we are trying to ‚Äúfit‚Äù a line to a set of points that are not necessarily exactly on said line (see ‚Äúthe result‚Äù below). But where do we start? How do we go about finding one line out of an infinite set of possible lines?\nMore or less, we find the parameters for our line equation that minimizes overall distance from all the points. So we try to minimize the sum of the error of our line from each point. It is a minimization exercise, but sometimes the points will be under the line, and others they‚Äôll be above. To correct for that, we actually minimize the sum of distances, each one squared! That way, all values are positive, and we really try to minimize something.\nThere is another important point hidden right here: The key to understand ‚ÄúGradient Descent‚Äù!\nAs we will see with the ‚Äúresults‚Äù below, the errors, when squared, are shaped in a sort of U. But what‚Äôs important is that the function behind that (parabolic) shape is that it‚Äôs differentiable.\nDifferentiating (that‚Äôs calculus) here will mean that we know in which direction (up, down, horizontal) said curve goes for a given point on the x axis. If we know that, we know that if the curve goes upwards, to minimize the thing, we want to go in the opposite direction.\nAnd that‚Äôs exactly what ‚Äúdescending the gradient‚Äù is all about (although in multivariate settings: A gradient in vector calculus is ‚Äúthe direction and the rate of fastest increase‚Äù, so to ‚Äúdescend a gradient‚Äù, you want to move in the opposite direction).\nFor linear regression, it‚Äôs cool because we can do partial derivatives (as shown on the referenced Wiki page above) to find our \\(\\alpha\\) and \\(\\beta\\) parameters that describe the line that ‚Äúbest fit the data‚Äù.\nIt‚Äôs quite clever, mind you: You minimize the error of your predicted value compared to the actual value. That‚Äôs how your machine can learn! But it‚Äôs also what people have been using to train Neural Networks!\nFor today, let‚Äôs just store the following information: The ‚ÄúLeast Square Minimization‚Äù can help find a minimum error through differentiation.\nAnd for a future conversation: Gradient Descent (hidden in today‚Äôs Sum of Square Minimization) is one of the two mathematical keys for back-propagation. (And back-propagation was the before and after of connectionism in Machine Learning theory).\nAlright, let‚Äôs move on and return to our very simple example."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#the-result",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#the-result",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "Remember how we said at some point that in ML, more data points is often better?\nWe‚Äôre going to find the line that best fits a set of points (I know a simple straight line will work here, because I‚Äôm the one generating said line‚Ä¶ :D)\nWe‚Äôre also going to show the square of errors, how it looks like a U shape, and how it‚Äôs clear there is a minimum to it‚Ä¶\n\n## Some points that can be used:\nmy_regression_x_set &lt;- seq(0, 5, 0.1) + rnorm(51,mean = 0, sd = 0.2)\nmy_regression_y_set &lt;- my_interpolation_y_set + rnorm(51, mean = 0, sd = 0.2)\n\n## SIMPLE linear regression. Using Least Squares distance.\n## Key idea: Minimize differences.\n## Find minimum for error function using differentiation.\n## Final equations here instead of step by step explaining how to get there.\n\n## Just follow the math:\nmean_reg_x &lt;- mean(my_regression_x_set)\nmean_reg_y &lt;- mean(my_regression_y_set)\n\ndev_x &lt;- (my_regression_x_set - mean_reg_x)\ndev_y &lt;- (my_regression_y_set - mean_reg_y)\n\nbeta_est &lt;- sum(dev_x * dev_y) / sum(dev_x^2)\nalpha_est &lt;- mean_reg_y - (beta_est * mean_reg_x)\n\nplot(my_regression_y_set ~ my_regression_x_set)\n## alpha & beta can be confusing, I usually use Beta as multipliers of X...\nabline(a = alpha_est, b = beta_est, col=\"blue\")\n\n\n\n\n\n\n\n## Importantly, squared error is differentiable and has a global minimum\n## \"easily\". Related to \"Gradient Descent\"\nerr_x &lt;- my_regression_y_set - (alpha_est + my_regression_x_set * beta_est)\nerr_y &lt;- sapply(err_x, \\(x) x^2)\n\nplot(err_y ~ err_x)\n\n\n\n\n\n\n\n\nAnd that‚Äôs it! We‚Äôve used the math (skipping some steps), to show one can use it to do a linear regression (although nowadays all that stuff is available with one command, if not directly, then through packages).\nSo here we‚Äôve shown a downward slope: Let‚Äôs put it in context of a potential real-world application: Maybe our x axis represented the number of users connected to our system, and the y axis some number to represent the available resources (spare CPU or something) on a given server. If you know a linear relationship applies, you can estimate at which point you will reach 100% CPU‚Ä¶ (I could have started with that, sorry.)\nSo in the future, if you see a clear straight line between two things, well, you could use the above not-scary-at-all math approach to ‚Äúpredict‚Äù what would happen if you move along the x axis.\nLast note about the math: in this simple case, if you look at the code, we‚Äôve in fact used two statistics concepts to do the calculations: the means of the x and y axis, and the ‚Äúdeviations‚Äù of each point to said means.\nDoes it not make some intuitive sense that the line that best fits the points is related to the means and deviations of the points to said means? :)"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#look-at-the-data",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#look-at-the-data",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "In the above, we‚Äôve used deviations and means. These are basic concepts from statistics. And in the introduction, we have mentioned something about ‚Äúmodels‚Äù and ‚Äúcompression‚Äù. Which is what it is: Statistics are NOT the real world. And they can be misused. Here the 2 most common examples to illustrate how you can go astray with using statistics without looking at the data.\nLet‚Äôs illustrate just that, with a couple of typical (like, VERY typical) examples:\n\n\n\n\n\n\n\n\n\nAnd if it‚Äôs not clear enough, all these here below have the same mean and standard deviation:\n\n\n\n\n\n\n\n\n\n[1] \"Mean x Dino: 47.8322528169014\"\n\n\n[1] \"Mean x Bullseye: 47.8308231552178\"\n\n\n[1] \"SD x Dino: 16.7651420391168\"\n\n\n[1] \"SD x Bullseye: 16.7692394934267\"\n\n\nSee how they are basically the same numbers?\nStatistics are great, useful and very important today (and have been for quite a while, I‚Äôd argue). They‚Äôre just not always the right solution (neither is ML in a larger sense, mind you).\nAlways look at the data!"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#conclusions",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#conclusions",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "We‚Äôve gone from really REALLY simple stuff all the way to mentioning differentiation, gradient descent, statistics‚Ä¶\nAnd we‚Äôve skipped steps! (i.e.¬†solving systems of equations, which can be represented as matrices, which is relevant for completely different reasons :D)\nNot too shabby, after all‚Ä¶\n\nI‚Äôd like to go into Logistic Regression and Neural Networks a bit in a future entries. Even discuss a bit the history of ML, maybe. And that would conclude my preparation for my sessions about ‚ÄúML Background for Cybersecurity‚Äù. Surely there is more to it all, but if I manage to convey a few key concepts to my colleagues, I‚Äôll be very very happy. And this here is me trying.\nBut after that, I‚Äôll shift gears and focus on a personal project I have had in mind for some time and that is taking shape: It involves writing an R package, for a set of ML algorithms that for reasons I don‚Äôt quite understand have not received as much attention as I personally think they should. To be continued!"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#references",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#references",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "The Wikipedia for Simple Linear Regression does help.\nHere about the Gradient.\nThe math for the simple linear regression we‚Äôve used today.\nThen the more advanced discussion on Linear Regression hopefully can be followed."
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html",
    "href": "posts/2024-12-08_A_new_project/index.html",
    "title": "A new project",
    "section": "",
    "text": "Ever since I read about the concept in M. Mitchell‚Äôs book ‚ÄúComplexity The emerging science at the edge of order and chaos‚Äù some time last year (I think around this time of the year‚Ä¶), I have been thinking about this, in the background of my head.\n\n\n\nWhat piqued me curiosity\n\n\nThe weird strings above, the 11##10### thing, might mean nothing to most right now (you have to look a bit further into it all, and I‚Äôll probably try and explain some of it in the future), but it was a revelation to me when I first read it.\nSo yes, I‚Äôve had other fish to fry for some time, but now it feels like I might just have the mental space to shift my focus a bit‚Ä¶\n\n\nSo here is my upcoming project, explained in one picture:\n\n\n\nMy new project in one picture\n\n\n(By the way, a personal note here: buying that left book in physical format from Springer was one ‚Äútough cookie‚Äù, thank goodness I‚Äôm motivated, it‚Äôs quite possibly the priciest book I ever bought from my own pocket, in ‚Ç¨-per-page‚Ä¶ Oh well: It looks it still was the right call for me :D)\n\n\n\nAs I said, I‚Äôve been a bit busy. Full-time job, MSc paper (success!), then summer, learning other stuff, (did I mention full-time job? :D)‚Ä¶ But it is time.\nI also have been thinking for quite some time I should try my hand eventually at creating my own R Package. This was all the more true after the last R conferences I attended recently, where it looks like ‚Äúeveryone is doing it‚Äù in the R (mostly academic) community‚Ä¶ So why not me?\nPlus, I have an idea or two. So what if I‚Äôm not the greatest coder out there? I still do have some tricks in my sleeves ;) I have proved (at least to myself) I can do highly distributed processing using (among other things) R and create my own thing. (My very own algorithm, that was.)\nPlus, I‚Äôm into ML just as much as the next person, but I‚Äôve also played quite a bit with Genetic Algorithms recently. And I believe I have all it takes to put that to good use here. (I‚Äôm just a bit sad this will not involve Network Graphs, the way I have it designed (yet, anyway :D).) I believe I understand how the LCS algorithms might be particularly useful in their own specific way. I can see in my head how they must work, what makes them different, all in spite of not having written one line of code yet.\nNow, it might take a while, indeed. Why, with the Holiday season coming, the day job and all. But I have a sort of a plan in mind, already an architecture (in my head, for now), examples of how I would want to validate the implementation (that last thing, one example with application to cybersecurity and logs classification, in particular, woke me up in the middle of the night a couple of weeks back).\nAnd R packages, from what I have read thus far, are not ‚Äúmagic‚Äù. It‚Äôs going to be work, for sure, but I don‚Äôt see anything I can‚Äôt manage. If anything, it‚Äôs less complicated than I thought. I‚Äôve already played with testthat, microbenchmark, I‚Äôve always separated my code in functions, separated R and C++ code, all that‚Ä¶ So yeah, I think I‚Äôm up to the task.\nAnd finally, a note about the ‚Äúwhy now‚Äù: I can‚Äôt find it done yet!! Which to me is quite incredible. I have found this niche, that nobody (in the R community) has deemed relevant enough thus far, and I am willing to put the work towards filling that niche!\n\n\n\nThis here today is me announcing, basically, my intention to create and (hopefully) get my future new RLCS package onto CRAN. I know, I know, I‚Äôm getting WAAAAY ahead of myself here.\nAnd I realize me publishing this entry might very well mean someone beats me to it. If that happens, well‚Ä¶ I‚Äôm already convinced my implementation will be ‚Äúcompetitive‚Äù, so‚Ä¶ let‚Äôem!\n(Plus, why would anyone decide to go for this now? It would just make no sense whatsoever - if not to bother me. If that‚Äôs you: Just give me a few months, and you‚Äôll save yourself the trouble :D)\n\n\n\nIt turns out, for some reason, I know I will. Obviously, I don‚Äôt ‚Äúknow know‚Äù, it‚Äôs more of a feeling.\nI am enthused, to begin with. I have the knowledge. Time is not an issue, although it will take me probably a few months (my best estimate right now, given my current known agenda, is 3-4 months for a functional package, plus whatever time it takes for submission to CRAN of a ‚Äúv001‚Äù).\nMoreover, I somehow needed a new, rather difficult, interesting long-term personal objective. I feel this will help me get in the ‚Äúflow‚Äù, which is a great feeling (but hard to come by).\nAlso, I have this crazy idea that, once the thing works, I can dive further into it and study it towards a bigger goal, maybe research, ideally even (who knows) a PhD‚Ä¶?\n\n\n\nI‚Äôve done my first (of two) presentation about ‚ÄúBackground of ML for Cybersecurity‚Äù. I actually had trouble with some of the demos during the first session :S But not because of the demos themselves, just, ‚Äúheat of the moment‚Äù, computer resources, demo effect, and so on. Nothing major.\nThankfully, I get a second chance, as there is a second session coming. And I have time to prepare some more examples, so that‚Äôs good too, the second session will be all-the-more interesting.\nBut because I have some time, I‚Äôm making a pause there and re-focusing on this new project of mine as a personal interest thing.\n\n\n\nI realize nobody cares, but heck: I do. I need to ‚Äúmigrate‚Äù, move everything (while reviewing each thing) from the old blog to this new setup, because.. Well, because this platform is more convenient, and ‚Äì let‚Äôs be honest ‚Äì cheaper‚Ä¶ After all, it‚Äôs one thing to write for one own‚Äôs pleasure, and quite another to spend money with no return year after year just to maintain it.\nSo yes, I need to organize myself to minimize future costs a bit (it just is the sensible thing to do, economically).\n\n\n\n**This is important: I still don‚Äôt really know why this family of algorithms have received sooo little attention**, beyond the fact that ‚ÄúConnectionism‚Äù has received a lot of said attention (deservingly, for sure).\nBut I‚Äôm curious about whether LCS can be made (ideally equally) valuable. And I have the personal conviction that I am in the right position to dig further into it.\nAnd so it is time, I‚Äôm in an ideal position, and I‚Äôm motivated.\nNow all I have to do is‚Ä¶ Start!\n(To start hear means also put thoughts to paper, design, code, test, document‚Ä¶ Don‚Äôt expect lots of news too soon, either! I‚Äôm not in a hurry, nobody should be: this thing has been known since the 70‚Äôs and it looks like few - aside from noted exceptions - have cared about it thus far‚Ä¶)\n\n\n\nThis is the best intro explaining the algorithm that I found thus far - and if anything it confirmed I was on the right path. It just makes a lot of sense to me!"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#the-new-project",
    "href": "posts/2024-12-08_A_new_project/index.html#the-new-project",
    "title": "A new project",
    "section": "",
    "text": "So here is my upcoming project, explained in one picture:\n\n\n\nMy new project in one picture\n\n\n(By the way, a personal note here: buying that left book in physical format from Springer was one ‚Äútough cookie‚Äù, thank goodness I‚Äôm motivated, it‚Äôs quite possibly the priciest book I ever bought from my own pocket, in ‚Ç¨-per-page‚Ä¶ Oh well: It looks it still was the right call for me :D)"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#why-now-why-me",
    "href": "posts/2024-12-08_A_new_project/index.html#why-now-why-me",
    "title": "A new project",
    "section": "",
    "text": "As I said, I‚Äôve been a bit busy. Full-time job, MSc paper (success!), then summer, learning other stuff, (did I mention full-time job? :D)‚Ä¶ But it is time.\nI also have been thinking for quite some time I should try my hand eventually at creating my own R Package. This was all the more true after the last R conferences I attended recently, where it looks like ‚Äúeveryone is doing it‚Äù in the R (mostly academic) community‚Ä¶ So why not me?\nPlus, I have an idea or two. So what if I‚Äôm not the greatest coder out there? I still do have some tricks in my sleeves ;) I have proved (at least to myself) I can do highly distributed processing using (among other things) R and create my own thing. (My very own algorithm, that was.)\nPlus, I‚Äôm into ML just as much as the next person, but I‚Äôve also played quite a bit with Genetic Algorithms recently. And I believe I have all it takes to put that to good use here. (I‚Äôm just a bit sad this will not involve Network Graphs, the way I have it designed (yet, anyway :D).) I believe I understand how the LCS algorithms might be particularly useful in their own specific way. I can see in my head how they must work, what makes them different, all in spite of not having written one line of code yet.\nNow, it might take a while, indeed. Why, with the Holiday season coming, the day job and all. But I have a sort of a plan in mind, already an architecture (in my head, for now), examples of how I would want to validate the implementation (that last thing, one example with application to cybersecurity and logs classification, in particular, woke me up in the middle of the night a couple of weeks back).\nAnd R packages, from what I have read thus far, are not ‚Äúmagic‚Äù. It‚Äôs going to be work, for sure, but I don‚Äôt see anything I can‚Äôt manage. If anything, it‚Äôs less complicated than I thought. I‚Äôve already played with testthat, microbenchmark, I‚Äôve always separated my code in functions, separated R and C++ code, all that‚Ä¶ So yeah, I think I‚Äôm up to the task.\nAnd finally, a note about the ‚Äúwhy now‚Äù: I can‚Äôt find it done yet!! Which to me is quite incredible. I have found this niche, that nobody (in the R community) has deemed relevant enough thus far, and I am willing to put the work towards filling that niche!"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#the-outcome",
    "href": "posts/2024-12-08_A_new_project/index.html#the-outcome",
    "title": "A new project",
    "section": "",
    "text": "This here today is me announcing, basically, my intention to create and (hopefully) get my future new RLCS package onto CRAN. I know, I know, I‚Äôm getting WAAAAY ahead of myself here.\nAnd I realize me publishing this entry might very well mean someone beats me to it. If that happens, well‚Ä¶ I‚Äôm already convinced my implementation will be ‚Äúcompetitive‚Äù, so‚Ä¶ let‚Äôem!\n(Plus, why would anyone decide to go for this now? It would just make no sense whatsoever - if not to bother me. If that‚Äôs you: Just give me a few months, and you‚Äôll save yourself the trouble :D)"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#will-i-manage",
    "href": "posts/2024-12-08_A_new_project/index.html#will-i-manage",
    "title": "A new project",
    "section": "",
    "text": "It turns out, for some reason, I know I will. Obviously, I don‚Äôt ‚Äúknow know‚Äù, it‚Äôs more of a feeling.\nI am enthused, to begin with. I have the knowledge. Time is not an issue, although it will take me probably a few months (my best estimate right now, given my current known agenda, is 3-4 months for a functional package, plus whatever time it takes for submission to CRAN of a ‚Äúv001‚Äù).\nMoreover, I somehow needed a new, rather difficult, interesting long-term personal objective. I feel this will help me get in the ‚Äúflow‚Äù, which is a great feeling (but hard to come by).\nAlso, I have this crazy idea that, once the thing works, I can dive further into it and study it towards a bigger goal, maybe research, ideally even (who knows) a PhD‚Ä¶?"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#what-about-the-sessions-on-ml",
    "href": "posts/2024-12-08_A_new_project/index.html#what-about-the-sessions-on-ml",
    "title": "A new project",
    "section": "",
    "text": "I‚Äôve done my first (of two) presentation about ‚ÄúBackground of ML for Cybersecurity‚Äù. I actually had trouble with some of the demos during the first session :S But not because of the demos themselves, just, ‚Äúheat of the moment‚Äù, computer resources, demo effect, and so on. Nothing major.\nThankfully, I get a second chance, as there is a second session coming. And I have time to prepare some more examples, so that‚Äôs good too, the second session will be all-the-more interesting.\nBut because I have some time, I‚Äôm making a pause there and re-focusing on this new project of mine as a personal interest thing."
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#and-what-about-the-migration-of-the-old-blog",
    "href": "posts/2024-12-08_A_new_project/index.html#and-what-about-the-migration-of-the-old-blog",
    "title": "A new project",
    "section": "",
    "text": "I realize nobody cares, but heck: I do. I need to ‚Äúmigrate‚Äù, move everything (while reviewing each thing) from the old blog to this new setup, because.. Well, because this platform is more convenient, and ‚Äì let‚Äôs be honest ‚Äì cheaper‚Ä¶ After all, it‚Äôs one thing to write for one own‚Äôs pleasure, and quite another to spend money with no return year after year just to maintain it.\nSo yes, I need to organize myself to minimize future costs a bit (it just is the sensible thing to do, economically)."
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#conclusions",
    "href": "posts/2024-12-08_A_new_project/index.html#conclusions",
    "title": "A new project",
    "section": "",
    "text": "**This is important: I still don‚Äôt really know why this family of algorithms have received sooo little attention**, beyond the fact that ‚ÄúConnectionism‚Äù has received a lot of said attention (deservingly, for sure).\nBut I‚Äôm curious about whether LCS can be made (ideally equally) valuable. And I have the personal conviction that I am in the right position to dig further into it.\nAnd so it is time, I‚Äôm in an ideal position, and I‚Äôm motivated.\nNow all I have to do is‚Ä¶ Start!\n(To start hear means also put thoughts to paper, design, code, test, document‚Ä¶ Don‚Äôt expect lots of news too soon, either! I‚Äôm not in a hurry, nobody should be: this thing has been known since the 70‚Äôs and it looks like few - aside from noted exceptions - have cared about it thus far‚Ä¶)"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#references",
    "href": "posts/2024-12-08_A_new_project/index.html#references",
    "title": "A new project",
    "section": "",
    "text": "This is the best intro explaining the algorithm that I found thus far - and if anything it confirmed I was on the right path. It just makes a lot of sense to me!"
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "At least the XCS version of LCS algorithm is a ‚Äúwell-known‚Äù (in the rather small sub-world of LCS enthusiasts, that is) approach to do Reinforcement Learning.\nThen again, all step-by-steps for RL in LCS seem to be hidden in more or less archane papers which are pay-per-access, and I don‚Äôt know yet if things make enough sense for me to pay for reading said papers. (Until recently, I had access with my University‚Äôs MSc Student account‚Ä¶ No more, it would seem. Fair enough.) We‚Äôll see how I go about that later‚Ä¶\nBut for now‚Ä¶\n\n\n\nWell, what I can do, is try to come up with my own personal version of an RL implementation with my RLCS ‚Äúfuture-package‚Äù, see if I can make it work, without worrying about past work too much for now.\nI do have a general understanding of ideas of LCS and RL, I just haven‚Äôt found a detailed step-by-step for me to copy from pseudo-code. I also have been reading (but I‚Äôm far from done) what I understand is the bible of Reinforcement Learning, i.e.¬†‚ÄúReinforcement Learning - An Introduction‚Äù by R. S. Sutton & A. G. Bardo. The general ideas of TD and Q-Learning I think I somewhat grasp, and I have my own (limited, sure) experience with Monte-Carlo.\nMixing these concepts with what I have implemented thus far of an LCS, adapting my code with concepts of Action Sets instead of Correct Sets, detect/action/effect/reward with an environment, and a split between exploit and explore, I came up with this ‚Äúdesign‚Äù this morning (Saturday and Sunday mornings are often a bit quieter than the rest of the week):\n\n\n\nthinking, thinking‚Ä¶\n\n\nThis is all on my small home-office wall right now, not at all implemented, but it seems it should be feasible, and I am curious as to whether it could work (or be a complete bust).\n\n\n\nWell, truth being told‚Ä¶ I don‚Äôt know.\nIt ‚Äúsounds‚Äù like a cool thing to be able to do. In the above drawing, I suggest I could, using an LCS, implement an ‚Äúagent‚Äù (yes, one of those, but please, I‚Äôm not talking about LLMs) that would learn on its own how to navigate a simple grid-world and look for food.\nI will have to code the whole logic and rules of that world, all aspects of movements in it, rewards, etc. It‚Äôs going to be a bit of work (somehow I think I‚Äôll manage).\nBut yeah, I don‚Äôt know a few things at this point: whether my ‚Äúdesign‚Äù will work (with whatever adjustments I can come up with), or why even program an RL algorithm.\nI just think it would be cool to have this future package be able to do all of data mining, supervised learning and reinforcement learning, with examples of each.\nAs to when I could use it, I know the answer for data mining and SL ‚Äì not so much for RL, for now.\n\n\n\nJust another fun exercise, really. It will take a few weeks, probably.\nBut if (big if at this stage) I manage to make it work, that would mean I actually understand enough of LCS as a concept to implement it from scratch and do data mining, Supervised Learning and Reinforcement learning. In turn that would also imply I know a bit more of what Reinforcement Learning entails.\nWhich in itself ‚Äì for me ‚Äì would already be pretty cool.\n\n\n\nTo be fair, there are a few papers out there I was able to access for free. In no special order and particularly valuable to inspire my future exercise I found these:\nhttps://arxiv.org/abs/2305.09945\nhttps://dl.acm.org/doi/pdf/10.1145/206944.206955\nhttps://www2.cs.uh.edu/~ceick/6367/bull-lcs.pdf"
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#although-i-know-in-theory",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#although-i-know-in-theory",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "At least the XCS version of LCS algorithm is a ‚Äúwell-known‚Äù (in the rather small sub-world of LCS enthusiasts, that is) approach to do Reinforcement Learning.\nThen again, all step-by-steps for RL in LCS seem to be hidden in more or less archane papers which are pay-per-access, and I don‚Äôt know yet if things make enough sense for me to pay for reading said papers. (Until recently, I had access with my University‚Äôs MSc Student account‚Ä¶ No more, it would seem. Fair enough.) We‚Äôll see how I go about that later‚Ä¶\nBut for now‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#general-understanding-of-rl-and-lcs-my-own-mix",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#general-understanding-of-rl-and-lcs-my-own-mix",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "Well, what I can do, is try to come up with my own personal version of an RL implementation with my RLCS ‚Äúfuture-package‚Äù, see if I can make it work, without worrying about past work too much for now.\nI do have a general understanding of ideas of LCS and RL, I just haven‚Äôt found a detailed step-by-step for me to copy from pseudo-code. I also have been reading (but I‚Äôm far from done) what I understand is the bible of Reinforcement Learning, i.e.¬†‚ÄúReinforcement Learning - An Introduction‚Äù by R. S. Sutton & A. G. Bardo. The general ideas of TD and Q-Learning I think I somewhat grasp, and I have my own (limited, sure) experience with Monte-Carlo.\nMixing these concepts with what I have implemented thus far of an LCS, adapting my code with concepts of Action Sets instead of Correct Sets, detect/action/effect/reward with an environment, and a split between exploit and explore, I came up with this ‚Äúdesign‚Äù this morning (Saturday and Sunday mornings are often a bit quieter than the rest of the week):\n\n\n\nthinking, thinking‚Ä¶\n\n\nThis is all on my small home-office wall right now, not at all implemented, but it seems it should be feasible, and I am curious as to whether it could work (or be a complete bust)."
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#why-even-consider-rl",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#why-even-consider-rl",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "Well, truth being told‚Ä¶ I don‚Äôt know.\nIt ‚Äúsounds‚Äù like a cool thing to be able to do. In the above drawing, I suggest I could, using an LCS, implement an ‚Äúagent‚Äù (yes, one of those, but please, I‚Äôm not talking about LLMs) that would learn on its own how to navigate a simple grid-world and look for food.\nI will have to code the whole logic and rules of that world, all aspects of movements in it, rewards, etc. It‚Äôs going to be a bit of work (somehow I think I‚Äôll manage).\nBut yeah, I don‚Äôt know a few things at this point: whether my ‚Äúdesign‚Äù will work (with whatever adjustments I can come up with), or why even program an RL algorithm.\nI just think it would be cool to have this future package be able to do all of data mining, supervised learning and reinforcement learning, with examples of each.\nAs to when I could use it, I know the answer for data mining and SL ‚Äì not so much for RL, for now."
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#conclusions",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#conclusions",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "Just another fun exercise, really. It will take a few weeks, probably.\nBut if (big if at this stage) I manage to make it work, that would mean I actually understand enough of LCS as a concept to implement it from scratch and do data mining, Supervised Learning and Reinforcement learning. In turn that would also imply I know a bit more of what Reinforcement Learning entails.\nWhich in itself ‚Äì for me ‚Äì would already be pretty cool."
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#references",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#references",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "To be fair, there are a few papers out there I was able to access for free. In no special order and particularly valuable to inspire my future exercise I found these:\nhttps://arxiv.org/abs/2305.09945\nhttps://dl.acm.org/doi/pdf/10.1145/206944.206955\nhttps://www2.cs.uh.edu/~ceick/6367/bull-lcs.pdf"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "I have yet another day off (that‚Äôs the thing with Holiday season), and I really wanted to keep working on this.\nSo I added the Genetic Algorithm part to my Supervised Learning basic Michigan-style LCS.\nThat means, after covering (done last Sunday) I am done with the (basic) implementation of the Rule Discovery features of the LCS algorithm.\nAnd, heck, it works.\n\n\n\nSo we had covering, whereby new rules were created when none in the classifiers matched the a new-coming environment instance.\nBut this is only part of the discovery of new rules. The second part in the Michigan-style LCS is that of a Genetic Algorithm.\nI implemented now a simple version, with tournament parents selection. That is, from the Correct set (see past entries) ‚Äì but taking numerosity of classifiers into account ‚Äì I choose N (a parameter) parents randomly. Then I take the best 2 parents by fitness. Which is ellitist.\nThen I run a one-point cross-over, to generate 2 children.\nI apply mutation to the children genes in turn, with a mutation pressure set as yet another parameter (default is 5% in my implementation).\nAnd finally I add the children to the population.\nNote that children mutations ensure only valid rules are created (i.e.¬†condition is compatible with the training instance). Mutations either generalize (add a wildcard) or specify (replaces a wildcard by a 0 or 1, depending on training instance).\nFully general (‚Äúall wildcard‚Äù) individuals are of course discarded. If a child actually matches by condition an individual in the correct set, it is discarded and the repeated classifier instead get a numerosity increase.\nAnd that‚Äôs IT!\n\n\n\nIn my current dummy example, whereby the goal is for the LCS to discover the rule ‚ÄúClass is NOT(bit 4 of input state)‚Äù, well‚Ä¶ In manual runs (several of them), I often (not always, mind you) find the two best rules are perfect rules:\n\n\n\nPerfect Rules are often discovered!\n\n\n\n\n\nThe first two rules (ordered by accuracy and numerosity, both decreasing) are the exact rules that perfectly will always predict the correct class/action, and are perfectly balanced in this case between general and specific.\nThese are the kind of things that an LCS provides that a Neural Network won‚Äôt:\nIt tells you WHY it makes a decision.\nHere, it is saying:\n\nRule 1: If bit 4 is 0, then class is 1. I am 100% accurate, whenever that happens.\nRule 2: If bit 4 is 1, then class is 0. I am 100% accurate, whenever that happens.\n\nWhich is exactly what this dummy training data set was trying to say in the first place, with 70% of the training possibilities shown to train the algorithm.\nObviously this is‚Ä¶ A somewhat silly example. But the thing is, I did NOT tell it how to get there, and it got the IF THEN ELSE correctly on its own.\n\n\n\nEvery now and then (after a certain number of Epochs) I run a Deletion process, whereby rules that have Numerosity 1 are discarded, thereby containing the size of the set of classifiers.\nThis is important for interpretability and performance.\n\n\n\nI am quite happy that after only a couple of days of coding, I have a basic, perfectly functional LCS!\nI do miss two components: Subsumption and Compaction.\nPlus of course the Prediction, that comes after training, on a test subset.\nIt‚Äôs all on my to-do, but this thing is already promising!\nGranted, it‚Äôs not real fast. And the code is far from perfect, or duly organized. But it works!"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#intro",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#intro",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "I have yet another day off (that‚Äôs the thing with Holiday season), and I really wanted to keep working on this.\nSo I added the Genetic Algorithm part to my Supervised Learning basic Michigan-style LCS.\nThat means, after covering (done last Sunday) I am done with the (basic) implementation of the Rule Discovery features of the LCS algorithm.\nAnd, heck, it works."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#what-the-ga-does",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#what-the-ga-does",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "So we had covering, whereby new rules were created when none in the classifiers matched the a new-coming environment instance.\nBut this is only part of the discovery of new rules. The second part in the Michigan-style LCS is that of a Genetic Algorithm.\nI implemented now a simple version, with tournament parents selection. That is, from the Correct set (see past entries) ‚Äì but taking numerosity of classifiers into account ‚Äì I choose N (a parameter) parents randomly. Then I take the best 2 parents by fitness. Which is ellitist.\nThen I run a one-point cross-over, to generate 2 children.\nI apply mutation to the children genes in turn, with a mutation pressure set as yet another parameter (default is 5% in my implementation).\nAnd finally I add the children to the population.\nNote that children mutations ensure only valid rules are created (i.e.¬†condition is compatible with the training instance). Mutations either generalize (add a wildcard) or specify (replaces a wildcard by a 0 or 1, depending on training instance).\nFully general (‚Äúall wildcard‚Äù) individuals are of course discarded. If a child actually matches by condition an individual in the correct set, it is discarded and the repeated classifier instead get a numerosity increase.\nAnd that‚Äôs IT!"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#results-with-fully-functional-rule-discovery",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#results-with-fully-functional-rule-discovery",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "In my current dummy example, whereby the goal is for the LCS to discover the rule ‚ÄúClass is NOT(bit 4 of input state)‚Äù, well‚Ä¶ In manual runs (several of them), I often (not always, mind you) find the two best rules are perfect rules:\n\n\n\nPerfect Rules are often discovered!"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#important-what-the-rules-set-tells-us",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#important-what-the-rules-set-tells-us",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "The first two rules (ordered by accuracy and numerosity, both decreasing) are the exact rules that perfectly will always predict the correct class/action, and are perfectly balanced in this case between general and specific.\nThese are the kind of things that an LCS provides that a Neural Network won‚Äôt:\nIt tells you WHY it makes a decision.\nHere, it is saying:\n\nRule 1: If bit 4 is 0, then class is 1. I am 100% accurate, whenever that happens.\nRule 2: If bit 4 is 1, then class is 0. I am 100% accurate, whenever that happens.\n\nWhich is exactly what this dummy training data set was trying to say in the first place, with 70% of the training possibilities shown to train the algorithm.\nObviously this is‚Ä¶ A somewhat silly example. But the thing is, I did NOT tell it how to get there, and it got the IF THEN ELSE correctly on its own."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#deletion-is-also-implemented",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#deletion-is-also-implemented",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "Every now and then (after a certain number of Epochs) I run a Deletion process, whereby rules that have Numerosity 1 are discarded, thereby containing the size of the set of classifiers.\nThis is important for interpretability and performance."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#conclusion",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#conclusion",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "I am quite happy that after only a couple of days of coding, I have a basic, perfectly functional LCS!\nI do miss two components: Subsumption and Compaction.\nPlus of course the Prediction, that comes after training, on a test subset.\nIt‚Äôs all on my to-do, but this thing is already promising!\nGranted, it‚Äôs not real fast. And the code is far from perfect, or duly organized. But it works!"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I‚Äôve already tried to parallelize a bit the training of a supervised-learning LCS. It kinda‚Äô works‚Ä¶ But the more I think about it, the more I‚Äôm convinced it‚Äôs not as easy as I thought.\nI initially thought of one way of parallel processing: Break the training samples in N (as many as CPU cores you wanna use) subsets, train N LCS.\nThen ‚Äúmerge‚Äù, subsume, remove duplicates, and‚Ä¶ Repeat.\nIdeally you would bootstrap the thing with an initial epoch on all the training set, just to have relevant coverage to start with.\nBut there is an issue: What if your dataset is ‚Äúnot perfect‚Äù? Let‚Äôs suppose there isn‚Äôt a clear-cut separation between classes to be found, but rather a fuzzy separation. So there aren‚Äôt perfect rules to be discovered. Instead, in each subset, some rules work, but THEN when these are exposed to the complete dataset, the discovered classes in one subset is correct in that subset, but NOT in the overall training data?\nYou end up with classifiers that‚Ä¶ Don‚Äôt work over test sets.\nAt least that‚Äôs my current conclusion after trying it with some messier-than-perfect datasets.\n\n\n\nSo if breaking the dataset in sub-sets does not work‚Ä¶ What about breaking the data POINTS in different parts?\nIn my earlier example, I have tested (quite successfully, in my view) my RLCS on a simplified MNIST exercise.\nThere, I was training to classify images. The original images where too big, in their equivalent bit-strings representation, for my laptop to work on in acceptable runtimes (for my taste). I ended up compressing said images into 49 bits strings per image. It was then manageable, and it worked quite good, in little over 20 seconds of training (with the correct hyper-parameters).\nBut could I instead break each images in say 7x7 lines, and train on each line separately (and in parallel), and then train one more LCS on the result of that (so 7 outputs would become the input states for a new classifier system, another 7-bits train set, so to speak)?\nI would then in effect, in terms of overall processing time, train approximately twice on 7 bits strings, instead of once on 49 bits strings.\nWould that even work? More on that later.\nBut first‚Ä¶\n\n\n\nEpistasis!\nWell, at least that‚Äôs how I understand it right now: Two (or more) different locations of the ‚Äúgenome‚Äù are required to define the phenotype. In my simple RLCS, in that case two or more bits in a state interact in a more or less complex manner so that the class is decided by said interaction. THEN, the second approach would break the assumptions!\nThink of the ‚ÄúMux6‚Äù example explained in this Blog a few entries ago. Two bits pointed to a position, which value was the class to be predicted. In that example, the first two bits pointed to a position in the last 4 bits.\nIF I were to break the 6 bits states in say two sub-strings of 3 bits each, thinking I can learn from each sub-string and then recompose (see ‚Äúidea version 2‚Äù above)‚Ä¶ I would fail miserably: you need the complete 6-bit strings in that example for the LCS to come up with the relevant rules!\nThat said‚Ä¶\n\n\n\nI‚Äôve shown ad-nauseam by now the ‚Äúnot-bit-4‚Äù example. In the example so far, I have used 5 bits strings. So the ‚Äúsearch space‚Äù was that of 5 bits. And the solution was simple in that one bit would suffice to predict the whole class.\nBut that was arbitrary, and meant to be simple.\nWhat if we were to be faced with the exact same rule to be found in 10-bits strings? The solution would be just as simple (two rules would suffice), but the search space would be comparatively‚Ä¶ Well, way bigger.\nIn 5-bits, the not-bit-4 is fully expressed with 32 states.\nIn 10-bits, the SAME not-bit-4 classifier works with‚Ä¶ 1024 states (total). For a perfect data mining exercise, you need to find a ruleset that perfectly matches all 1024 states, and you need to try combinations of 3 possible values per-bit (our ternary alphabet: {‚Äò#‚Äô, ‚Äò0‚Äô, ‚Äò1‚Äô}). If I‚Äôm correct, that could be up to about 3^10*2 (including class) 59049-1 possible unique classifiers to potentially check (coverage ensures here that not-bit-4 is respected, so that‚Äôs only half the overall 10 bits strings + class, as half are illegal, making it a 3^10 matter‚Ä¶ And the ‚Äò##########‚Äô option is not valid for us‚Ä¶). Well, approximately anyway.\nAnd let‚Äôs remember, an LCS is expected to output a combination subset of these ~59K classifiers as an output System, which could be a very large search space indeed‚Ä¶ (In a real world setup, that is.)\nHowever! Well, let‚Äôs try to apply the logic of my ‚Äúapproach number 2‚Äù above to break down the exercise and then see what would happen in such a simple case‚Ä¶\nImagine I break down the exercise into 3 ‚Äútrainings‚Äù:\n\nThe first 5 bits (which in fact are more than enough) per state, creating \\(RLCS_1\\)\nThe last 5 bits per state (completely irrelevant, but we don‚Äôt know that upfront), creating \\(RLCS_2\\)\nThen the outputs of the above to train a third LCS, \\(RLCS_{combined}\\)\n\nThe hope is that we can train the first two in parallel, and use the predictions of these two in combination to then train a third classifier (which incidentally here would be exposed to 2-bits states). So I‚Äôd expect to reduce the search space quite a bit. And hence the training times.\nWell, with no further ado, here come the initial results, and heck!\nParallel runtime using the approach described here to break the problem in just two 5-bit string sub-problems, less than 5 seconds:\n\n\n\nbreaking original problem in two makes it fast!\n\n\nCompared to original sequential time on the 10-bit string, more than 4 minutes:\n\n\n\nThat worked, just‚Ä¶ Very slow, comparatively\n\n\nThe result however is slightly more difficult to interpret (that is, in the current version of this exercise, which I just wanted to see working, but it is not clean‚Ä¶). In particular, each subset I have to keep (for now) better than chance results (anything &gt; 0.5), but that‚Äôs just because it‚Äôs a dirty example, it can be made cleaner for sure:\n\n\n\nFast results, but requires a bit more work to interpret\n\n\nStill, from ‚Äú&gt; 4 minutes‚Äù to ‚Äú&lt; 5 seconds‚Äù! But then again, this will only works for such a problem where the solution did exist in a subset of original bit-strings.\n\n\n\nWell, for one, I am NOT in fact browsing in parallel through 1024 states!\nSee, each sub-string of 5 bits and its class, as a subset can be de-duplicated! There are a lot of repeated instances, say in the first subset, whereby all entries that say ‚Äú00000 -&gt; class 1‚Äù appear 32 times (one for each of the continuing 5-bit strings for the rest of the original state).\nThat kind of deduplication can reduce processing efforts indeed :)\n\n\n\nI haven‚Äôt tested it yet, but I would wager this approach should work just fine on images too, as described above, breaking images in lines and then combining classifiers‚Ä¶\nParallelizing training is an obvious idea (to me) in concept: For some examples, training is slow, and so running it in N parallel processes must be faster (for, you know, complicated examples).\nBut as explained in some detail today, it‚Äôs not that simple! It might work in some cases, as explained, but in the ‚Äúreal world‚Äù, we wouldn‚Äôt know upfront whether it would in fact work, would we? And so we might have to do some trial-and-error‚Ä¶\nBut even just running tests on new datasets, as explained above, might allow us to:\n\nEither see how we can run fast in parallel on a given dataset and still work out a solution\nOr find out that the dataset cannot be broken the way we have initially chosen, indicating possible ‚Äúepistasis‚Äù, which in itself would be interesting and would have just supposed a few seconds lost here.\nOr ‚Äì and that could happen ‚Äì that the chosen dataset cannot be broken down as we would like it to, for an LCS to come up with a proposed classification solution as there is not a very clear-cut solution (think Yahoo finance historical data‚Ä¶ Yes, I‚Äôve had a quick look. Suffice to say, I won‚Äôt beat the markets just yet :D:D:D But that‚Äôs not relevant‚Ä¶ )\n\nI‚Äôll work on that parallelizing stuff some more‚Ä¶ Even if that‚Äôs not part of the package itself, it‚Äôs more about data preparation and how to use the package, I am aware. But demonstrating how the future RLCS package can hopefully be used and competitive compared to a Neural Network (for instance) is part of my overall objective, I guess.\nFun stuff (to me) all of it :)"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I‚Äôve already tried to parallelize a bit the training of a supervised-learning LCS. It kinda‚Äô works‚Ä¶ But the more I think about it, the more I‚Äôm convinced it‚Äôs not as easy as I thought.\nI initially thought of one way of parallel processing: Break the training samples in N (as many as CPU cores you wanna use) subsets, train N LCS.\nThen ‚Äúmerge‚Äù, subsume, remove duplicates, and‚Ä¶ Repeat.\nIdeally you would bootstrap the thing with an initial epoch on all the training set, just to have relevant coverage to start with.\nBut there is an issue: What if your dataset is ‚Äúnot perfect‚Äù? Let‚Äôs suppose there isn‚Äôt a clear-cut separation between classes to be found, but rather a fuzzy separation. So there aren‚Äôt perfect rules to be discovered. Instead, in each subset, some rules work, but THEN when these are exposed to the complete dataset, the discovered classes in one subset is correct in that subset, but NOT in the overall training data?\nYou end up with classifiers that‚Ä¶ Don‚Äôt work over test sets.\nAt least that‚Äôs my current conclusion after trying it with some messier-than-perfect datasets."
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea-version-2",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea-version-2",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "So if breaking the dataset in sub-sets does not work‚Ä¶ What about breaking the data POINTS in different parts?\nIn my earlier example, I have tested (quite successfully, in my view) my RLCS on a simplified MNIST exercise.\nThere, I was training to classify images. The original images where too big, in their equivalent bit-strings representation, for my laptop to work on in acceptable runtimes (for my taste). I ended up compressing said images into 49 bits strings per image. It was then manageable, and it worked quite good, in little over 20 seconds of training (with the correct hyper-parameters).\nBut could I instead break each images in say 7x7 lines, and train on each line separately (and in parallel), and then train one more LCS on the result of that (so 7 outputs would become the input states for a new classifier system, another 7-bits train set, so to speak)?\nI would then in effect, in terms of overall processing time, train approximately twice on 7 bits strings, instead of once on 49 bits strings.\nWould that even work? More on that later.\nBut first‚Ä¶"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-caveat",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-caveat",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "Epistasis!\nWell, at least that‚Äôs how I understand it right now: Two (or more) different locations of the ‚Äúgenome‚Äù are required to define the phenotype. In my simple RLCS, in that case two or more bits in a state interact in a more or less complex manner so that the class is decided by said interaction. THEN, the second approach would break the assumptions!\nThink of the ‚ÄúMux6‚Äù example explained in this Blog a few entries ago. Two bits pointed to a position, which value was the class to be predicted. In that example, the first two bits pointed to a position in the last 4 bits.\nIF I were to break the 6 bits states in say two sub-strings of 3 bits each, thinking I can learn from each sub-string and then recompose (see ‚Äúidea version 2‚Äù above)‚Ä¶ I would fail miserably: you need the complete 6-bit strings in that example for the LCS to come up with the relevant rules!\nThat said‚Ä¶"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#counter-caveat-not-bit-4-in-10-bits-states",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#counter-caveat-not-bit-4-in-10-bits-states",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I‚Äôve shown ad-nauseam by now the ‚Äúnot-bit-4‚Äù example. In the example so far, I have used 5 bits strings. So the ‚Äúsearch space‚Äù was that of 5 bits. And the solution was simple in that one bit would suffice to predict the whole class.\nBut that was arbitrary, and meant to be simple.\nWhat if we were to be faced with the exact same rule to be found in 10-bits strings? The solution would be just as simple (two rules would suffice), but the search space would be comparatively‚Ä¶ Well, way bigger.\nIn 5-bits, the not-bit-4 is fully expressed with 32 states.\nIn 10-bits, the SAME not-bit-4 classifier works with‚Ä¶ 1024 states (total). For a perfect data mining exercise, you need to find a ruleset that perfectly matches all 1024 states, and you need to try combinations of 3 possible values per-bit (our ternary alphabet: {‚Äò#‚Äô, ‚Äò0‚Äô, ‚Äò1‚Äô}). If I‚Äôm correct, that could be up to about 3^10*2 (including class) 59049-1 possible unique classifiers to potentially check (coverage ensures here that not-bit-4 is respected, so that‚Äôs only half the overall 10 bits strings + class, as half are illegal, making it a 3^10 matter‚Ä¶ And the ‚Äò##########‚Äô option is not valid for us‚Ä¶). Well, approximately anyway.\nAnd let‚Äôs remember, an LCS is expected to output a combination subset of these ~59K classifiers as an output System, which could be a very large search space indeed‚Ä¶ (In a real world setup, that is.)\nHowever! Well, let‚Äôs try to apply the logic of my ‚Äúapproach number 2‚Äù above to break down the exercise and then see what would happen in such a simple case‚Ä¶\nImagine I break down the exercise into 3 ‚Äútrainings‚Äù:\n\nThe first 5 bits (which in fact are more than enough) per state, creating \\(RLCS_1\\)\nThe last 5 bits per state (completely irrelevant, but we don‚Äôt know that upfront), creating \\(RLCS_2\\)\nThen the outputs of the above to train a third LCS, \\(RLCS_{combined}\\)\n\nThe hope is that we can train the first two in parallel, and use the predictions of these two in combination to then train a third classifier (which incidentally here would be exposed to 2-bits states). So I‚Äôd expect to reduce the search space quite a bit. And hence the training times.\nWell, with no further ado, here come the initial results, and heck!\nParallel runtime using the approach described here to break the problem in just two 5-bit string sub-problems, less than 5 seconds:\n\n\n\nbreaking original problem in two makes it fast!\n\n\nCompared to original sequential time on the 10-bit string, more than 4 minutes:\n\n\n\nThat worked, just‚Ä¶ Very slow, comparatively\n\n\nThe result however is slightly more difficult to interpret (that is, in the current version of this exercise, which I just wanted to see working, but it is not clean‚Ä¶). In particular, each subset I have to keep (for now) better than chance results (anything &gt; 0.5), but that‚Äôs just because it‚Äôs a dirty example, it can be made cleaner for sure:\n\n\n\nFast results, but requires a bit more work to interpret\n\n\nStill, from ‚Äú&gt; 4 minutes‚Äù to ‚Äú&lt; 5 seconds‚Äù! But then again, this will only works for such a problem where the solution did exist in a subset of original bit-strings."
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#trick-why-so-fast",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#trick-why-so-fast",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "Well, for one, I am NOT in fact browsing in parallel through 1024 states!\nSee, each sub-string of 5 bits and its class, as a subset can be de-duplicated! There are a lot of repeated instances, say in the first subset, whereby all entries that say ‚Äú00000 -&gt; class 1‚Äù appear 32 times (one for each of the continuing 5-bit strings for the rest of the original state).\nThat kind of deduplication can reduce processing efforts indeed :)"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#conclusion",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#conclusion",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I haven‚Äôt tested it yet, but I would wager this approach should work just fine on images too, as described above, breaking images in lines and then combining classifiers‚Ä¶\nParallelizing training is an obvious idea (to me) in concept: For some examples, training is slow, and so running it in N parallel processes must be faster (for, you know, complicated examples).\nBut as explained in some detail today, it‚Äôs not that simple! It might work in some cases, as explained, but in the ‚Äúreal world‚Äù, we wouldn‚Äôt know upfront whether it would in fact work, would we? And so we might have to do some trial-and-error‚Ä¶\nBut even just running tests on new datasets, as explained above, might allow us to:\n\nEither see how we can run fast in parallel on a given dataset and still work out a solution\nOr find out that the dataset cannot be broken the way we have initially chosen, indicating possible ‚Äúepistasis‚Äù, which in itself would be interesting and would have just supposed a few seconds lost here.\nOr ‚Äì and that could happen ‚Äì that the chosen dataset cannot be broken down as we would like it to, for an LCS to come up with a proposed classification solution as there is not a very clear-cut solution (think Yahoo finance historical data‚Ä¶ Yes, I‚Äôve had a quick look. Suffice to say, I won‚Äôt beat the markets just yet :D:D:D But that‚Äôs not relevant‚Ä¶ )\n\nI‚Äôll work on that parallelizing stuff some more‚Ä¶ Even if that‚Äôs not part of the package itself, it‚Äôs more about data preparation and how to use the package, I am aware. But demonstrating how the future RLCS package can hopefully be used and competitive compared to a Neural Network (for instance) is part of my overall objective, I guess.\nFun stuff (to me) all of it :)"
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html",
    "href": "posts/2024-10-20_Interpolation_Example/index.html",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "",
    "text": "As I want to make sure I keep posting here, I keep looking for things to write about in my own personal ‚Äúlibrary‚Äù (book shelves).\nOne topic I want to make sure I don‚Äôt forget about is ‚ÄúNumerical Methods‚Äù. (Another one is graph theory, another one is anomaly detection‚Ä¶ There are a few topics like that :D).\nAnyhow. Today: Interpolation.\nMore specifically, Lagrange polynomials. Which is just one of many possible approaches (and has its own drawbacks)."
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#intro",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#intro",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "",
    "text": "As I want to make sure I keep posting here, I keep looking for things to write about in my own personal ‚Äúlibrary‚Äù (book shelves).\nOne topic I want to make sure I don‚Äôt forget about is ‚ÄúNumerical Methods‚Äù. (Another one is graph theory, another one is anomaly detection‚Ä¶ There are a few topics like that :D).\nAnyhow. Today: Interpolation.\nMore specifically, Lagrange polynomials. Which is just one of many possible approaches (and has its own drawbacks)."
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#concept-of-interpolation",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#concept-of-interpolation",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Concept of Interpolation",
    "text": "Concept of Interpolation\nSo first, what is ‚ÄúInterpolation‚Äù. Most people interested in Statistics and Machine Learning (mostly wrt time series and predicting the future of stock markets, say‚Ä¶) will have heard about Extrapolation. And that‚Äôs mostly it: Take a distribution or timeline and ‚Äúpredict‚Äù what the future of it should look like. (To keep things simple, that is).\nInterpolation is about finding a distribution in between known points. So not about the ‚Äúfuture‚Äù (in the above comparison) but instead about the ‚Äúpresent‚Äù, if you will. It‚Äôs about ‚Äúfilling the gaps‚Äù. Or trying to üôÇ"
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#lagrange-polynomials",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#lagrange-polynomials",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Lagrange Polynomials",
    "text": "Lagrange Polynomials\n‚ÄúFor a given set of N+1 data points, we want to find the coefficients of an Nth-degree polynomial function to match them [‚Ä¶]‚Äù\n\nLagrange polynomials are somewhat intuitive because each term x_m can rather easily be shown to correspond exactly to y_m (by definition of L_{N, m} above).\nAlright, and the math above leads me to the following R code (my own, but not saying ‚Äúperfect‚Äù, of course :D):\n\n(The source code can be found here)\nAnd to validate that, I take the same example points as that of the reference book, and we can plot the results:\n\nSo we see a POSSIBLE approach here, which ‚Äúlooks‚Äù sensible (but MIGHT VERY WELL be wrong)."
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#conclusions",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#conclusions",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Conclusions",
    "text": "Conclusions\nToday was about one of many approaches, a short introduction to the topic of Interpolation (as ‚Äúopposed‚Äù, though not really, to extrapolation).\nThere are several alternatives, namely Newton Polynomials, for which improvements can be obtained by better choosing sample points (Chebyshev nodes). Or the well appreciated Cubic Splines.\nNewton Polynomials for instance work with recursion, and so might come in handy if one expects to ADD new reference data points in the future, because with Lagrange Polynomials, all calculations need to be redone from scratch.\nAll of which might make for a nice few future entries of this blog üôÇ"
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#resources",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#resources",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Resources",
    "text": "Resources\n‚ÄúApplied Numerical Methods Using MatLab, 2nd Edition‚Äù, Ed. Wiley, by W. Y. Yang, W. Cao, J. Kim & al."
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html",
    "title": "While on the Train: Cellular Automata",
    "section": "",
    "text": "I decidedly LOVE riding on a train: For whatever reason, it‚Äôs one particular moment where I somehow focus a bit more than usual. It‚Äôs usually just a few hours, reasonably quiet environment, a bit more spacious than say airplanes (airplanes are just not the same, even with earplugs), no TV to distract my attention, the excuse of limited/imperfect mobile coverage‚Ä¶ You name it. I just don‚Äôt know, but it works, I feel motivated to code & run experiments while on a train‚Ä¶"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#intro",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#intro",
    "title": "While on the Train: Cellular Automata",
    "section": "",
    "text": "I decidedly LOVE riding on a train: For whatever reason, it‚Äôs one particular moment where I somehow focus a bit more than usual. It‚Äôs usually just a few hours, reasonably quiet environment, a bit more spacious than say airplanes (airplanes are just not the same, even with earplugs), no TV to distract my attention, the excuse of limited/imperfect mobile coverage‚Ä¶ You name it. I just don‚Äôt know, but it works, I feel motivated to code & run experiments while on a train‚Ä¶"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#introducing-the-cellular-automata",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#introducing-the-cellular-automata",
    "title": "While on the Train: Cellular Automata",
    "section": "Introducing the Cellular Automata",
    "text": "Introducing the Cellular Automata\nOne thing that I liked about the book I recommended a couple of days back (see last entry before this one) is that it provides nice and easy examples (but in Python), and then reasonable exercises. I loved to read through most of them (and some of the math sections), but reading is not that fun in this case, I wanted to try it out (of course!).\nThe following is the result of a simple R implementation of ‚ÄúCellular Automata‚Äù for simulation of a ‚Äúfire spread‚Äù in a theoretical forest setting. The identification of the ‚Äúneighbour‚Äù trees in fire follows the Von Neumann definition of ‚Äúneighbours‚Äù in this 2D configuration.\nHopefully the video is quite self-explanatory. You start with ONE burning tree, and then let time pass‚Ä¶ üôÇ"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#visualization-trick",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#visualization-trick",
    "title": "While on the Train: Cellular Automata",
    "section": "Visualization trick",
    "text": "Visualization trick\nOne thing I learnt today is how to quickly draw a matrix into a picture. I hereby recommend you look into the ‚ÄúMBCbook‚Äù R package, for its function ‚Äúimshow()‚Äù. Which incidentally I found while looking for alternatives to the Python function‚Ä¶ imshow! (So yeah, it was a fast search that one‚Ä¶)"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#conclusions",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#conclusions",
    "title": "While on the Train: Cellular Automata",
    "section": "Conclusions",
    "text": "Conclusions\nI hope you enjoyed it (I could definitely make it faster, and more or less dense a forest, and change the neighbours identification, and what not‚Ä¶ I‚Äôve tested this a few times with a few parameters).\nAt least to me, this was for no-good-reason quite‚Ä¶ Satisfying üôÇ\nAnd I‚Äôm not sharing the code (not yet anyway) just because this was a first, horrible pasted-together step by step implementation, full of slow and nested ‚Äúfor loops‚Äù, to-be-improved matrix indexing, not enough functions‚Ä¶ And well, just not quite ‚Äúpresentable‚Äù. Find the code linked below. But it does work just fine though üôÇ\nCode on my GitHub account"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "",
    "text": "So as I keep working on an upcoming presentation I shall give in a few weeks, I thought I‚Äôd prepare something a bit more ‚Äúlive‚Äù than just a PPT with Memes (although who doesn‚Äôt like a few Memes in a PPT?).\nMy current idea is to talk about some ‚ÄúMachine Learning Key Concepts‚Äù, and then bring it back to Cybersecurity applications.\nSo part of it could be about two key things:\n\nSupervised vs Unsupervised Learning\nSymbolic vs Sub-Symbolic ‚ÄúAI‚Äù\n\nWhich I feel help frame a bit what we‚Äôre talking about when we talk about Machine Learning.\nToday, we‚Äôll do some review, and then talk about Clustering. Here an output of one such algorithm, in 3D:"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#section",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#section",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "",
    "text": "So as I keep working on an upcoming presentation I shall give in a few weeks, I thought I‚Äôd prepare something a bit more ‚Äúlive‚Äù than just a PPT with Memes (although who doesn‚Äôt like a few Memes in a PPT?).\nMy current idea is to talk about some ‚ÄúMachine Learning Key Concepts‚Äù, and then bring it back to Cybersecurity applications.\nSo part of it could be about two key things:\n\nSupervised vs Unsupervised Learning\nSymbolic vs Sub-Symbolic ‚ÄúAI‚Äù\n\nWhich I feel help frame a bit what we‚Äôre talking about when we talk about Machine Learning.\nToday, we‚Äôll do some review, and then talk about Clustering. Here an output of one such algorithm, in 3D:"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#simplified-definition-of-machine-learning",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#simplified-definition-of-machine-learning",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Simplified definition of Machine Learning",
    "text": "Simplified definition of Machine Learning\nNow I‚Äôm probably NOT the right source for you to learn this, so I really suggest you read about that somewhere else. The wiki puts it a bit like so:\n‚ÄúThe study of statistical algorithms¬†that can learn from¬†data¬†and¬†generalize¬†to unseen data, and thus perform¬†tasks¬†without explicit¬†instructions.‚Äù\nThere are quite a FEW THINGS in that sentence right there. But for today:\n\nIn traditional programming, a person WRITES A PROGRAM, that receives INPUT (say a picture), and generates an OUTPUT (say‚Ä¶ ‚ÄúDog‚Äù or ‚ÄúCat‚Äù)\nIn a Machine Learning approach, a person PROVIDES (LOTS OF) INPUTS AND CORRESPONDING OUTPUTS, and the COMPUTER CREATES THE PROGRAM (usually then called a ‚Äúmodel‚Äù).\n\nThe above is specifically applicable to ‚ÄúSupervised‚Äù learning, but nevermind that, the key here is: The computer CREATES the MODEL that we (humans) can then apply to new data."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-unsupervised-learning",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-unsupervised-learning",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Why Unsupervised Learning?",
    "text": "Why Unsupervised Learning?\nIn Cybersecurity, a relevant part of the job for ML can be about detecting anomalies.\nOften times, you don‚Äôt get pre-trained neural networks applicable to your scenario. Or more simple: you don‚Äôt have access to relevant ‚Äúbig data‚Äù (which would help with training your models, indeed), i.e.¬†people (companies) rarely share detailed data (network packets, logs, configurations, etc.) of their breaches. Understandable.\nAnd so ‚Äúunsupervised‚Äù learning might make sense in that scenario. Unsupervised Learning is about discovering structure in your data, that you might not have known about upfront."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Warning",
    "text": "Warning\nAlso there are lots of potential issues with leveraging ML, but two possibly relevant ones would be:\n\nImbalance between classes (hopefully you have little data as examples of real attacks on your network, and a LOT of ‚Äúnormal traffic‚Äù data, for instance),\nBase-rate fallacy (a SOC analyst might, in certain settings, work most of their time on false-positives)\n\nThat said, let‚Äôs keep it simple for today, we will keep it clean, no complications (yet, anyway)."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#symbolic-vs-sub-symbolic",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#symbolic-vs-sub-symbolic",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Symbolic vs Sub-symbolic",
    "text": "Symbolic vs Sub-symbolic\nSimplifying A LOT, let‚Äôs just say for today that ‚ÄúSymbolic‚Äù can be read by a Human, and so it could look like sets of rules of the type:\nIF (A & NOT B) THEN (ACTION X)\nWhere a person could read A (‚Äúnumber of Errors in 1‚Ä≤ log file &gt; 100‚Äù), B (‚Äúless than 10 errors are of type ‚Äòlogin failed‚Äô‚Äù), X (‚ÄúBlock originating IP on Firewall‚Äù). (Note the negation of B in the expression above ;))\nPutting together many of these rules a person COULD setup a reactive security configuration for a firewall based on monitoring logs. I mean, conceptually, why not?! That would be an ‚ÄúExpert System‚Äù, as they called them in the 80‚Äôs.\nOh: And you COULD have ‚ÄúMachine Learning‚Äù on top of Rule-based systems. For example one interesting field (to me) that somehow has received little attention so far is that of the ‚ÄúLearning Classifier Systems‚Äù (LCS)‚Ä¶ But that‚Äôs for another day.\nWhen you enter the realm of Neural Networks, Dimensionality Reduction (say PCA on TF-IDF), BackPropagation, non-linearity, differentiable functions, etc., you quickly leave the realm of ‚Äúhuman readable‚Äù, and you enter the world of vectors, matrices, tensors‚Ä¶¬†In these settings, you use numbers, linear algebra, and the concept of distance.\nFor instance, distances between the ‚Äúcalculated class‚Äù and ‚Äúreal class‚Äù for a set of entries (say, images of cats and dogs, or log files, or‚Ä¶), trying to reduce these distances would mean trying to reduce the prediction error. Said like that, it is probably a bit confusing. But to be perfectly clear: That last sentence is a BIG part of what supervised machine learning with Neural Networks is all about! (More exactly in this case the goal is to minimize the difference between predicted and real class, or predicted and real value)\nLet‚Äôs just make a note at this point, then: Sub-symbolic is the domain of neural networks, a world of algebra and calculus, weights and thresholds, which often are hard to translate into ‚Äúhuman-readable rules‚Äù. And more specifically in the case of the current trend with deep neural networks (which are truly an impressive thing!), it‚Äôs a big issue, because there is a problem with how we can UNDERSTAND what the algorithm does. And that introduces things like lack of trust, issues with responsibility, and not being able to explain why something works (or, usually more to the point, why something suddenly DOES NOT work).\nBut the goal here and today is not to explain the details (‚Äúwhy backpropagation expects differentiable activation functions, for gradient descent, and the Chain Rule‚Äù ‚Äì or ‚Äúwhy ReLU works so good for training a NNet, but it‚Äôs not a differentiable function, and so people use approximations like leaky ReLU‚Äù‚Ä¶ All that might be a bit much ‚Äì Me at least, I still often need to come back to my books each time I want to explain these things correctly‚Ä¶ So some other time :D).\nToday I‚Äôll focus on the concept of distance between points, and leverage that to identify ‚Äúclusters‚Äù of points (and we‚Äôll mention multi-dimensional spaces real quick)."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#clustering",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#clustering",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Clustering",
    "text": "Clustering\nOne type of ‚ÄúUnsupervised Learning‚Äù is what is called clustering. The main idea is to look at data and to try and create ‚Äúgroups of similar data points‚Äù. That‚Äôs it. That‚Äôs what Clustering is all about.\nRight, but‚Ä¶ How?\nSo let‚Äôs see:\nIf a = 20, b = 21, c = 99 and d = 100‚Ä¶ Would you agree you could possibly say ‚Äúa is nearer from b than from d‚Äù. And iterating, you might conclude:\n\nGroup 1: a, b\nGroup 2: c, d\n\nDoes that make sense? Hopefully YES üôÇ\n\n\n\n1d and 3 groups of points\n\n\nLet‚Äôs move on to two dimensions. You get a set of points (imagine, for Cybersecurity, I don‚Äôt know: for each point representing a machine on a network, the x coordinate represents the number of TCP Packets sent by the machine from its TCP Port 80 in the last minute, and the y coordinate represents the number of TCP Packets sent by the same machine from its port TCP 443 in the last minute).\nSo now you might have two sets of points that ‚Äúcluster together‚Äù, some with very little activity on both axis, that is: (x, y) = 0, and others (maybe only a few), that have a range of numbers but overall have maybe lots of activity as per both axis, so say for example (x &gt; 1000, y &gt; 1000).\nLet‚Äôs apply the same logic as above. There will probably be two clusters, one of which might have many points but overlapping, and the other a cloud of points on the top right‚Ä¶ Representing Web Servers.\nThat‚Äôs probably a very dumb example, but it serves a purpose: You COULD identify groups of machines in these two dimensions. Distances here would probably use Euclidean Distance, but if you understand it visually, good enough for today.\n\n\n\n2d and 2 groups of points\n\n\nAnd with more (very similar) dimensions, you might be able to discover groups of machines that are similar to one another, but a bit different from those of another group‚Ä¶\nHere, I just gave you an algorithm to group machines and separate Windows from Linux, or Clients from Servers, or Web from Mail from LDAP from NTP from DNS servers‚Ä¶ Obviously, the above categories are a bit‚Ä¶ not great, well because most of the time you will KNOW what the machines are to begin with. But what if all you have to work from is a tcpdump file?\nLet‚Äôs visualize this, shall we?"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#dbscan",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#dbscan",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "DBScan",
    "text": "DBScan\nOne (of MANY) algorithms out there to do clustering is DBScan. Its very name says most of it: ‚ÄúDensity based Spatial Clustering‚Äù.\nI‚Äôm not going to reinvent the wheel today, and we‚Äôll go right ahead and leverage the dbscan R package and its documentation. The code is here.\nSo first, we‚Äôll generate a set of seemingly almost-random points in a 2-dimensional space.\n\nVisually, a person can already tell there seems to be some structure in there, some groups. How many might be a bit of a judgement call, but still.\nLeveraging a number of neighbours (say, 4 nearest points to identify a group)¬† to identify an ‚Äúelbow‚Äù of the separation of the groups, we can set a ‚Äúnoise threshold‚Äù to the above whereby if a node is too removed from a group, it could be considered as SEPARATE.\nLet‚Äôs see:\n\n\n\nidentifying noise in clustering\n\n\nIn the above, there is a ‚Äúclear‚Äù change in trend in the line that finds said distances, at 0.85 approximately (red line) that identifies regions of LOW DENSITY of points, that the DBScan algorithm would then propose as a limit to separate OUTLIERS from the rest of clustered points.\nIt‚Äôs a bit of a mess to write down, but hopefully the results are self explanatory:\n\nHere we color the groups of points by cluster, or what the algorithm has proposed as such. Again, the only concept in use was the distance to other points. A detailed look in the last picture would show that maybe something is amiss, at least one point had x &lt; 0 before, and it doesn‚Äôt show up here.\nThat‚Äôs an identified outlier.\n\nLet‚Äôs take a minute here: We‚Äôve identified stuff that goes together, so ‚Äúclusters‚Äù.\nBut one key aspect (value) of DBScan over some alternative algorithms for clustering is, it can help with ANOMALY DETECTION. Indeed, that‚Äôs why I have chosen this algorithm today (the typical intro to clustering would have probably focused on KMeans first :D).\nSo now, we have an ‚Äúautomated ML algorithm‚Äù that receives coordinates of points, and is capable of identifying groups of points, AND points that seem to not quite fit anywhere.\nRemember earlier when we mentioned ‚Äúimbalance‚Äù of prevalence of ‚Äúnormal traffic‚Äù vs ‚Äúattack traffic‚Äù on a corporate network? Well, this is why I mentioned it. With a little luck, what a DBScan output tells us are ‚Äúoutliers‚Äù is something that is UNUSUAL, and HOPEFULLY that‚Äôs actually identifying attack-related data for us!\nOK, back to the demos."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-bother",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-bother",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Why bother?",
    "text": "Why bother?\nOK, so beyond finding things out about your data, the data you have‚Ä¶ What if you could get information from new data (of the same kind, that is)?\nAfter all, you‚Äôve identified groups. And that‚Äôs cool, and maybe you‚Äôve learned that somehow ten computers seem to behave similarly, and quite differently from another set of 50 computers, on the same network. And maybe that leads you to do some digging and conclude that all 10 of the first group were DB servers, and the other were front-end stuff (Idk, Apache). All from network dump files. Not too bad.\nBut now you receive a new dump file, which you‚Äôre told contains network traces from other computers. Wouldn‚Äôt it be cool to then just feed that to your ‚Äútrained‚Äù model (which, remember, was actually unsupervised to begin with), and get it to tell you ‚Äì like a supervised algorithm would: That new machine is a ‚ÄúGroup 1‚Äù machine (and so you can deduce it‚Äôs a DB server).\nI know, I know. Just look at ports, and you would know, fair enough. Plus, it‚Äôs not clear the example above would even work (there are MANY considerations in there). Anyhow, let‚Äôs take your ‚Äúpre-trained‚Äù model from above, and see what would happen with say 12 new points:\n That‚Äôs it: New data, and without you having to look at it, the machine will tell you to which group each entry belongs. That‚Äôs the cool thing about Machine Learning üôÇ"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#going-3d",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#going-3d",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Going 3D",
    "text": "Going 3D\nAn almost identical exercise, but in 3 Dimensions. I just want to show it so that we can all agree on one thing: We human can conceptualize up to three dimensions. But with this next visualization, I hope to show one important idea: There is nothing precluding an algorithm from going and work into ‚Äúhigher dimensions‚Äù. We can easily visualize groups in 1-D, 2-D, now 3-D (maybe, on a screen, with the help of some animation). But 4-D, or 1000-D, is NOT a problem for a computer!\nOK, so in 3D, same algorithm, similarly random-generated data points. What DBScan can do is shown at the top of this Blog post üôÇ\n(I just know people are more impressed by 3-D animations than 2D visuals for some reason, and so I put it at the top to keep you interested :D)"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#its-not-magic",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#its-not-magic",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "It‚Äôs NOT magic",
    "text": "It‚Äôs NOT magic\nLet‚Äôs see a very classical example, and how DBScan kinda‚Äô fails. Not really, but still.\nWhen applied to the ‚ÄúIris‚Äù dataset (if you‚Äôve ever studied a bit of data science, you know what it is), DBScan identifies two clusters, while we all know there are 3 types of flowers represented in the dataset.\nThat does NOT mean that DBScan FAILED. It just means that the information it can tell us about that dataset is that one group of flowers is clearly different from the rest. And that‚Äôs OK, although we know it‚Äôs insufficient. BUT YOU NEED TO KNOW IT‚ÄôS NOT MAGIC. From a few data points / coordinates, it‚Äôs still only working with so much information‚Ä¶\nReal groups: 3\n\n\n\nreal iris groups\n\n\nDBScan (with selected parameters)groups: 2 (and a few outlier points)\n\n\n\ndbscan identified iris groups"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning-2",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning-2",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Warning 2",
    "text": "Warning 2\nNOT ALL numbers are ordinals/cardinals. Although 20, 21, 22, 23 might seem nearer from one another than say from 80, 123, or 443‚Ä¶ That doesn‚Äôt mean you can use THAT ‚Äúdistance‚Äù.\nIn Cybersecurity (but in any other field), PLEASE remember DOMAIN KNOWLEDGE can ‚Äúmake or break‚Äù a data scientist. And not knowing why things don‚Äôt work as you expect then is a bad thing. And it‚Äôs not always the algorithm fault.\nWeb is different from NTP, while HTTPS uses cryptography and so does SSH, but‚Ä¶ In context, port TCP 80 is NOT nearer TCP port 22 than it is from TCP Port 443.\nYou‚Äôve been warned."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#conclusions",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#conclusions",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Conclusions",
    "text": "Conclusions\nUnsupervised Machine Learning has potential for applications to Cybersecurity data. Maybe used on network traffic captures or logs, one can identify structure and propose groupings of machines, users (by their activity, accesses, hours, who knows‚Ä¶).\nAlthough we‚Äôve seen one algorithm and how to visualize its decisions with 2- and 3-dimensional data, the ‚Äúsky is the limit‚Äù, and if one can come up with 100 (or 1000) such dimensions (that‚Äôs the concept behind ‚Äúfeature engineering‚Äù), there is nothing precluding our machines to work with that and propose groupings for us (although not all algorithms deal nicely with ‚Äúcurse of dimensionality‚Äù, but that‚Äôs a different topic). In ML, more (quality) data is often a good thing. Also, if one of the 100 dimensions helps us separate perfectly some groups, some ML algorithm will find that and use it for us. Would you visualize manually and study 100 dimensions? 1000?\nAnd that‚Äôs where it‚Äôs powerful: A person might have a hard time grouping hundreds of machines or users while considering several aspects at once, much less when the number of groups or ‚Äúkinds of groups‚Äù to be found are not known upfront‚Ä¶ But that‚Äôd be no issue for a computer üôÇ\nHopefully I can walk some of my colleagues through the above concepts (organised in a PPT) and show them (in R :P) how all the concepts ‚Äúwork‚Äù, and then translate into ‚Äúreal world applications‚Äù.\nMaybe next week I‚Äôll move on to making text into multi-dimensional data points. And then, we‚Äôd be set to apply all the above to text data. Which is quite prevalent in Cybersecurity (CVE descriptions, logs, code‚Ä¶ it‚Äôs all text :D).\n\nResources\nWikipedia as linked above, and dbscan R Package documentation, mostly.\nAlso this about making a video from 3D scatter plot with RGL"
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "The exact same code to do supervised learning can be used for data mining. In fact, I‚Äôve already shown that, with the ‚Äúnot-bit-4‚Äù example.\nThe approach is simple: Instead of training (and then testing with new data), suppose what you want to understand something about your data. Then you can show it all to the LCS for it to come up with rules. If you‚Äôre lucky, your data has hidden rules that you didn‚Äôt know about.\n\n\n\nSo we‚Äôve seen it already, the ‚Äúnot-bit-4‚Äù example. Well, the LCS can try to find out for you:\n\n\n\nit looks easier like this, doesn‚Äôt it?\n\n\nBut what if what you have is this? Not so easy to find the rule manually, right?\n\n\n\nWhat if your data looks like so?\n\n\nMaybe too easy? But it‚Äôs the concept that matters! Let‚Äôs try a different example. See if you can spot it:\n\n\n\nThis one is just slightly trickier‚Ä¶\n\n\nThe LCS only takes a few second‚Ä¶\n\n\n\nprocessing for simple cases is rather fast\n\n\nAnd finds the rules (4 in this case):\n\n\n\nBit 4 XOR bit 5\n\n\nThat‚Äôs how the LCS here expresses ‚Äúbit 4 XOR bit 5‚Äù. And yes, that was it.\nEnough with the examples. But conceptually, the ‚ÄúMux6‚Äù example for the last entry? Same-same.\nAnd more to the point, I had 6 bits inputs here, but what if there are 10 bits? 20 bits? Would you be able to manually find similar rules? It would take me some time :D\nSure, the algorithm is slower with 20 bits instead of 6 (can you blame it?). But it‚Äôs machine time. :)\n\n\n\nI actually have a use-case at work I want to test this on. As soon as I upload a first version of my project to GitHub, I‚Äôll be able to test it there. I have inventories and some things about them I want to know whether I can explain with short, readable rules. I think the LCS might help. Although‚Ä¶ Maybe there is no simple rule to be found, but that‚Äôs worth trying.\nThe trick there will be: How to encode the data into binary strings, as this is what my code accepts‚Ä¶? But I actually know how to go about that.\n\n\n\nWell, I just like the idea that the same Supervised Learning algorithm can be used to do Data mining. And provide interpretable information about some datasets (if there is something to be found, that is).\nTake that, Neural Networks! :D\nAlright, I guess next week I‚Äôll return to working on the package itself."
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#lcs-for-data-mining",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#lcs-for-data-mining",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "The exact same code to do supervised learning can be used for data mining. In fact, I‚Äôve already shown that, with the ‚Äúnot-bit-4‚Äù example.\nThe approach is simple: Instead of training (and then testing with new data), suppose what you want to understand something about your data. Then you can show it all to the LCS for it to come up with rules. If you‚Äôre lucky, your data has hidden rules that you didn‚Äôt know about."
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#demonstration",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#demonstration",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "So we‚Äôve seen it already, the ‚Äúnot-bit-4‚Äù example. Well, the LCS can try to find out for you:\n\n\n\nit looks easier like this, doesn‚Äôt it?\n\n\nBut what if what you have is this? Not so easy to find the rule manually, right?\n\n\n\nWhat if your data looks like so?\n\n\nMaybe too easy? But it‚Äôs the concept that matters! Let‚Äôs try a different example. See if you can spot it:\n\n\n\nThis one is just slightly trickier‚Ä¶\n\n\nThe LCS only takes a few second‚Ä¶\n\n\n\nprocessing for simple cases is rather fast\n\n\nAnd finds the rules (4 in this case):\n\n\n\nBit 4 XOR bit 5\n\n\nThat‚Äôs how the LCS here expresses ‚Äúbit 4 XOR bit 5‚Äù. And yes, that was it.\nEnough with the examples. But conceptually, the ‚ÄúMux6‚Äù example for the last entry? Same-same.\nAnd more to the point, I had 6 bits inputs here, but what if there are 10 bits? 20 bits? Would you be able to manually find similar rules? It would take me some time :D\nSure, the algorithm is slower with 20 bits instead of 6 (can you blame it?). But it‚Äôs machine time. :)"
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#so-what",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#so-what",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "I actually have a use-case at work I want to test this on. As soon as I upload a first version of my project to GitHub, I‚Äôll be able to test it there. I have inventories and some things about them I want to know whether I can explain with short, readable rules. I think the LCS might help. Although‚Ä¶ Maybe there is no simple rule to be found, but that‚Äôs worth trying.\nThe trick there will be: How to encode the data into binary strings, as this is what my code accepts‚Ä¶? But I actually know how to go about that."
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#conclusion",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#conclusion",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "Well, I just like the idea that the same Supervised Learning algorithm can be used to do Data mining. And provide interpretable information about some datasets (if there is something to be found, that is).\nTake that, Neural Networks! :D\nAlright, I guess next week I‚Äôll return to working on the package itself."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html",
    "title": "Classifying URLs",
    "section": "",
    "text": "I‚Äôm still working my way through a ‚Äúpractical demos‚Äù session on ML background for Cybersecurity.\nSee I have a ‚Äúplan‚Äù in my head about how to introduce Machine Learning, including demystifying a bit the math behind it. The ‚Äúplan‚Äù goes a bit like this:\n\nWe‚Äôve seen recently a bit of what unsupervised learning can be about. For Cybersecurity, anomaly detection is a clear use-case. We‚Äôve used the concept of distance between points. But there is more to this category (clustering per-se, but also dimensionality reduction is quite useful, for instance).\nML can be about making a line go through points (nicely). That‚Äôs regression. With sufficient data, you‚Äôre compressing information of relationships between dimensions to predict an outcome. In the simplest form, given x, you can estimate y. And the math is quite simple:\n\\[\ny = \\beta_0 + \\beta_1x\n\\]\nSuch a linear regression is then really of the realm of statistics. As such, there is little of practical application of that thing for us in this simplest form, as in Cybersecurity, many things are expected to be black or white. But maybe it‚Äôs not completely crazy to maybe estimate that the more time a machine is connected to the Internet, the higher the risk of infection (all other things being equal, that is). That could be a growing line. Sometimes things can be approximated with the appropriate transformations on the dimensions (say ‚Äúindependent variables‚Äù) and then used in a linear regression (so the ‚Äúmodel is linear on its parameters‚Äù).\n\n\nI‚Äôll make an entry about linear regression soon(-ish), when I have time.\n\n\nML can also be about making a line separate groups of things nicely, i.e.¬†classifying stuff. In its simplest form, we will have input data, and will want to know in which one of two categories a given input should belong too. That‚Äôs our ‚Äúrun of the mill‚Äù Spam classifier.\n\n\nBoth regression (linear or not) and classification are usually considered to be the two most common supervised learning use-cases.\n\n\nThen you have Reinforcement Learning (aka RL). So far, I have yet to personally come across a real-world cybersecurity application of that, although it is pretty cool a concept! (Actually a friend has demonstrated something with Honeypots and such‚Ä¶) But I‚Äôll admit, I have played little with RL myself for now, and it‚Äôs a pending task of mine. It does sound appealing, and there are some papers out there to use RL for optimizing pentesting processes and such.\nAnd then you have Self-Supervised Learning. The first time I read that I was like ‚ÄúWhaaaat?‚Äù. But that‚Äôs LLMs, as noted at the end of one of my most recent posts. You predict next word in a text, and check what actually comes in the‚Ä¶ Rest of the text. So you only need‚Ä¶ The text. Nifty. But I‚Äôm not too fond of it and have yet to think it through enough to come up with something practical beyond LLMs‚Ä¶\n\nThere is plenty more, but this is one way of putting categories to what ML is about.\n\n\n\nAlright, so I‚Äôll skip some of the above, and today jump right onto this:\nHow does Supervised Machine Learning work, as applied to one cybersecurity-relevant classification use-case?\nSomehow I feel it‚Äôs more fun to frame things a little into practical use-cases, instead of pure math (which I suppose is boring/threatening to quite a few).\nEven then, be warned: This entry is pretty long. But hopefully it contains some important things and my wish is, it‚Äôs also understandable."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#i-have-a-plan",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#i-have-a-plan",
    "title": "Classifying URLs",
    "section": "",
    "text": "I‚Äôm still working my way through a ‚Äúpractical demos‚Äù session on ML background for Cybersecurity.\nSee I have a ‚Äúplan‚Äù in my head about how to introduce Machine Learning, including demystifying a bit the math behind it. The ‚Äúplan‚Äù goes a bit like this:\n\nWe‚Äôve seen recently a bit of what unsupervised learning can be about. For Cybersecurity, anomaly detection is a clear use-case. We‚Äôve used the concept of distance between points. But there is more to this category (clustering per-se, but also dimensionality reduction is quite useful, for instance).\nML can be about making a line go through points (nicely). That‚Äôs regression. With sufficient data, you‚Äôre compressing information of relationships between dimensions to predict an outcome. In the simplest form, given x, you can estimate y. And the math is quite simple:\n\\[\ny = \\beta_0 + \\beta_1x\n\\]\nSuch a linear regression is then really of the realm of statistics. As such, there is little of practical application of that thing for us in this simplest form, as in Cybersecurity, many things are expected to be black or white. But maybe it‚Äôs not completely crazy to maybe estimate that the more time a machine is connected to the Internet, the higher the risk of infection (all other things being equal, that is). That could be a growing line. Sometimes things can be approximated with the appropriate transformations on the dimensions (say ‚Äúindependent variables‚Äù) and then used in a linear regression (so the ‚Äúmodel is linear on its parameters‚Äù).\n\n\nI‚Äôll make an entry about linear regression soon(-ish), when I have time.\n\n\nML can also be about making a line separate groups of things nicely, i.e.¬†classifying stuff. In its simplest form, we will have input data, and will want to know in which one of two categories a given input should belong too. That‚Äôs our ‚Äúrun of the mill‚Äù Spam classifier.\n\n\nBoth regression (linear or not) and classification are usually considered to be the two most common supervised learning use-cases.\n\n\nThen you have Reinforcement Learning (aka RL). So far, I have yet to personally come across a real-world cybersecurity application of that, although it is pretty cool a concept! (Actually a friend has demonstrated something with Honeypots and such‚Ä¶) But I‚Äôll admit, I have played little with RL myself for now, and it‚Äôs a pending task of mine. It does sound appealing, and there are some papers out there to use RL for optimizing pentesting processes and such.\nAnd then you have Self-Supervised Learning. The first time I read that I was like ‚ÄúWhaaaat?‚Äù. But that‚Äôs LLMs, as noted at the end of one of my most recent posts. You predict next word in a text, and check what actually comes in the‚Ä¶ Rest of the text. So you only need‚Ä¶ The text. Nifty. But I‚Äôm not too fond of it and have yet to think it through enough to come up with something practical beyond LLMs‚Ä¶\n\nThere is plenty more, but this is one way of putting categories to what ML is about."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#todays-intro",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#todays-intro",
    "title": "Classifying URLs",
    "section": "",
    "text": "Alright, so I‚Äôll skip some of the above, and today jump right onto this:\nHow does Supervised Machine Learning work, as applied to one cybersecurity-relevant classification use-case?\nSomehow I feel it‚Äôs more fun to frame things a little into practical use-cases, instead of pure math (which I suppose is boring/threatening to quite a few).\nEven then, be warned: This entry is pretty long. But hopefully it contains some important things and my wish is, it‚Äôs also understandable."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#classifying-urls-into-two-categories",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#classifying-urls-into-two-categories",
    "title": "Classifying URLs",
    "section": "Classifying URLs into TWO categories",
    "text": "Classifying URLs into TWO categories\n\nThe data\nIn Cybersecurity sometimes getting to interesting datasets can be a bit challenging. After all, certain things are usually done behind closed doors. You can probably understand why. Which is why I like for instance this resource, secrepo.com.\nToday, we‚Äôre gathering a seemingly simple dataset: A list of web URLs, very simply tagged as either good or bad. Nothing else. But, mind you, 420 thousand of‚Äôem.\n\nPoint number one: to do ML, it can help to have lots of data. (It‚Äôs not always necessary, but it‚Äôs usually a good idea.)\n\n\n\nThe objective\nToday is about trying to distinguish (really just trying!) to classify URLs (‚Äúwebpages‚Äù, for most) in two categories: Good or Bad. Why? Applications are for your protection, and can be used to recommend you to avoid certain websites, which in turn can be maybe used as a supplementary control for other security measures, such as detecting Phishing emails.\nCan we make our computer tell us if a given URL is good or bad?\nThat‚Äôs it. That‚Äôs our goal for today. Using Machine Learning, of course. So we‚Äôre aiming to implement one (or more) model(s) to classify URLs. Based on data we already have. That‚Äôs supervised learning, more precisely a classifier.\n\nJust to be very clear: This is all part of a preparation to an introduction on ML background for Cybersecurity. ‚ÄúIntroduction‚Äù is the key here: I‚Äôm not aiming for complete, perfect, not even good, as long as I can convey certain concepts that I believe are relevant to grasp an idea at best of how ML works.\nEven the code I put together is‚Ä¶ Well, lacking. It‚Äôs not meant to be production grade.\n\nThere is nothing in the way of test-driven anything\nSome of the regular expressions are simplistic\nSome stuff will be badly filtered\nThe trained models are not good\nThe data is what it is, and I‚Äôm not trying to complement it\n‚Ä¶\n\nPlease don‚Äôt come saying ‚Äúthis is not great‚Äù. I know. I only have so much spare time. This post is only my way to support with contents an interactive session I plan to give soon. There is a lot of good training on ML, Cybersecurity & al.¬†out there. Go find it, if you want formal and/or good, detailed training.\nIf you‚Äôre fine with simply trying to wrap our heads around concepts, do keep reading.\n\n\n\nThe code\nThe code will be on my Github eventually. But for now, as usual, a few blocks of it:\n\nurls_df &lt;- read.csv(\"https://raw.githubusercontent.com/faizann24/Using-machine-learning-to-detect-malicious-URLs/refs/heads/master/data/data.csv\")\n\nurls_df$url &lt;- tolower(urls_df$url)\n## Let's have a quick look, a.k.a. \"EDA\"\ndim(urls_df)\n&gt; [1] 420464      2\ntable(urls_df$label) ## Imbalance, we might want e.g. to under-sample \"good\"\n&gt;  bad   good \n 75643 344821 \n\n\nPoint number two: Imbalance is often bad. Here we have 4.5 times more good entries than bad entries. Now, why could that be bad? Here we‚Äôre going to try to learn from our data. If we keep the imbalance in the dataset, to make things simple, our model could learn that there is more good than bad. And maybe that‚Äôs what we want, but then that imbalance could affect the outcome of our model. Unless we want to use that imbalance for some reason, it‚Äôs probably best to manage it upfront.\n\nHow to remove imbalance? Well, one way (of surely many out there, only I only know a few), is to ‚Äúunder-sample‚Äù some of the over-represented class. Today we‚Äôre going to take proportionally less entries from good to train our model, making then sure that we have roughly half and half, of each class.\nAs per the class, it‚Äôs a binary choice, good or bad. We‚Äôll create a variable to encode that as 0 or 1 (or the other way around, it‚Äôs irrelevant). That‚Äôs just to make things compatible with certain models, as most will expect numerical data.\nurls_df$label_num &lt;- ifelse(urls_df$label == \"good\", 0, 1)\nurls_df$url_length &lt;- nchar(urls_df$url)\n\n## A bit of domain knowledge helps:\nnrow(urls_df[urls_df$url_length &gt; 300,]) / nrow(urls_df)\n[1] 0.001203432\n\nPoint number 3: Domain Knowledge is important. We‚Äôre going to leverage that today quite a bit. To begin with, we have 0.1% of the entries with URL length superior to 300 characters, and to make things cleaner, we‚Äôll assume today these are‚Ä¶ Irrelevant. So we remove them. Our classifier will hence not be trained with such data. And maybe that‚Äôs a bad idea, depending on your goals. For today, everything is fair game, we want to keep things simple.\n\n\n\nFeature Engineering\nHeck. We only have URLs. And a class. How is a machine suppose to go from there?\nLet‚Äôs try to extract something akin‚Äô to a signal out of that. So we‚Äôve got already the length of each URL. And maybe that‚Äôs helpful. Are longer URLs more often bad than good? Well, for real long URLs, maybe a bit. But it‚Äôs not really definitive, is it?\n\n\n\nComparing densities of URL lengths per class\n\n\n\nPoint number 4: Always look at the data. Don‚Äôt just run into the modelling, it‚Äôs not a good idea. Get a feeling of the data you want to work with. I can‚Äôt stress this enough.\n\nLet‚Äôs keep going then. Again, domain knowledge is key. The good news is, most of us have seen thousands of URLs in our lifetime, so maybe we have a little understanding of what we could look for.\nToo many slashes ‚Äú/‚Äù? Too many ‚Äúdots‚Äù? Maybe. So those could be two new ‚Äúdimensions‚Äù. Although maybe these two are already somewhat expressed through the length of the URL? In other words, it might make sense that the longer the URL, the more dots and slashes.\n\nPoint number 5: That‚Äôs a correlation right there, and depending on how much two variables are correlated, maybe you‚Äôre better off with fewer variables. There is a lot of background statistics on this topic. And for ML algorithms, sometimes too many variables is a bad thing, more so if they don‚Äôt add any useful information.\n\nFor today, we‚Äôll keep it. After all, we have for now only what, 3 variables to work with? We need more. I‚Äôm going to save you the pain of going through it all one by one, and propose my own few variables I thought we might consider for training our model, ALL extracted from the URLs themselves.\n\nIP as host: Humans use ‚ÄúDomain Names‚Äù that are readable. You need a DNS entry for that, and you need to register things as the owner for the DNS entry, for legal reasons. So if you skip the DNS step, you can still have an IP address, but it will look like‚Ä¶ An IP. It‚Äôs a bit far-fetched, but I‚Äôd argue if a URL reflects a Public IP, it‚Äôs either known good (ours or provided by some trusted third party), or - more often than not - it‚Äôs a bad indicator.\nURL readability: So it‚Äôs not direct. A URL can of course contain stuff that‚Äôs purely technical. But we usually make an effort to make things readable: variable names, folder names, etc. Bad actors might want to obfuscate stuff or generate random folder or what-not. And so if a URL is mostly unreadable gibberish, I‚Äôd guess it‚Äôs a bad sign. Which we can ‚Äúencode‚Äù as: How many vowels has the URL relative to its length? Does the URL contain things with 4 consecutive consonants (not usual in written english, although not good an indicator in some other languages‚Ä¶)? Again, both things are probably somewhat related, but not necessarily/completely. So I take both.\nIs a port expressly identified in the URL? After the host, a column and number is usually not required for a normal website, it‚Äôs usually a default (443 or 80). So if you see ‚Äúsomehost.somedomain.com:12345‚Äù, something exotic is going on. Exotic for normal web browsing is weird (well, it‚Äôs exotic :D), and so not expected for good stuff.\nWe can keep going: Domain extension, file extension (a URL ending in .exe is a red flag, for sure :D), or more simply how common is either of these, is probably helpful too.\n\nIt‚Äôs not exhaustive (not in the least) but hopefully it makes some sense. From a URL, we‚Äôve put together 14 different variables that way. All chosen from experience, from ‚Äúdomain knowledge‚Äù. (See point number 3 above if it wasn‚Äôt clear before.)\n\n\n\nWe should keep looking at our data‚Ä¶\n\n\nFrom no variables (except considering the URL itself‚Ä¶) to 14. Not too shabby.\n&gt; names(urls_df)\n [1] \"url\"                       \"label\"                     \"url_length\"               \n [4] \"label_num\"                 \"slashes_count\"             \"dots_count\"               \n [7] \"host_is_ip\"                \"vowels_prev\"               \"ends_in_slash\"            \n[10] \"contains_port\"             \"n_params\"                  \"domain_ext\"               \n[13] \"file_ext\"                  \"is_common_domain_ext\"      \"is_uncommon_domain_ext\"   \n[16] \"is_uncommon_file_ext\"      \"has_4_consonants_straight\"\nThere is sooo much more to consider.\nFor instance if you check out the code (if/when I make it available on my GitHub), you‚Äôll see at one point I ‚Äúscale‚Äù the data. That is, I try to put all the variable in comparable orders of magnitude. This is to avoid one variable overshadowing all the others. Something that varies from 0.5 to 0.6 might otherwise be considered less important than something that varies from 3 to 4000. Which is not always true.\nI also make a BAD thing: I transform extensions to ‚Äúfactors‚Äù, and then I encode the levels of the factors as numerical data. This is not great, I know :D\nNamely, factors are not ordered, while two numbers could be, providing ordinal value at least, and distances could be considered, when here there is clearly no such thing. BAD! BAD Nico!\nLook, this is no excuse, but hopefully, if you order things upfront, and then encode to numerical value, say bad entries as factors first, then good, you end up with ordered levels where by lower ones are for bad, and higher for good (or vice-versa). It will turn out wrong for today. This is tricky and let me insist, NOT good practice. As it turns out, I have so many possible extensions (values) in there, that a better approach - such as one-hot-encoding - makes my dataset explode in size and not fit my RAM memory‚Ä¶ And I am just too lazy to work through this for what was meant to be a simple demo. So‚Ä¶ My apologies, I know, it hurts the eyes to see this. Moving on.\n\n\nTraining Models\nOne last concept, and we‚Äôll dive in actual ‚ÄúLearning‚Äù.\n\nPoint number 6: Save some entries for testing you trained model. So say we have 10K entries, of which 5000 are good and 5000 are bad entries. How do you know your trained model ‚Äúgeneralizes‚Äù correctly? If you were to try and evaluate your model on data you used to train it, you couldn‚Äôt know whether it just learnt exactly that case, or if it would work on future data. To verify how it would work on future data, you‚Ä¶ Validate using data not seen during training. There is more to that, too, but that‚Äôll be enough for conceptual understanding today.\n\nOK. At last. As today has been dense (I know, sorry), I‚Äôll train just ONE model on our dataset.\ngood_urls &lt;- urls_df[urls_df$label == \"good\",]\nbad_urls &lt;- urls_df[urls_df$label == \"bad\",]\n## Undersampling \"good\" vs \"bad\"\nsample_urls_df &lt;- rbind(bad_urls[sample(1:nrow(bad_urls), size = 10000,replace = FALSE),],\n                        good_urls[sample(1:nrow(good_urls), size = 10000,replace = FALSE),])\n\n## ...\n\nseparate_sets &lt;- sample(c(TRUE, FALSE), nrow(sample_urls_df), replace=TRUE, prob=c(0.7,0.3))\nt_train &lt;- sample_urls_df[separate_sets, ]\nt_test &lt;- sample_urls_df[!separate_sets, ] # i.e. Not train set...\n\n\nPartitioning Tree, train and test\nHere is how you train a Partitioning Tree in R:\n## A Partitioning tree but WITHOUT the bad trick of extensions encoding\n## And low depth:\ntree_model &lt;- rpart(label ~ url_length + slashes_count + dots_count +\n                        host_is_ip + vowels_prev + ends_in_slash + contains_port +\n                        n_params + is_common_domain_ext + is_uncommon_domain_ext +\n                        is_uncommon_file_ext + has_4_consonants_straight,\n                    data = t_train,\n                    method = \"class\",\n                    control = rpart.control(cp = 0.05))\nAnd here how you visualize, and ‚Äútest‚Äù it:\n&gt; tree_model ; plot(tree_model); text(tree_model)\nn= 13929 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 13929 6933 good (0.4977385 0.5022615)  \n  2) dots_count&gt;=0.3877992 2846  729 bad (0.7438510 0.2561490) *\n  3) dots_count&lt; 0.3877992 11083 4816 good (0.4345394 0.5654606)  \n    6) vowels_prev&lt; -0.6992319 1877  658 bad (0.6494406 0.3505594) *\n    7) vowels_prev&gt;=-0.6992319 9206 3597 good (0.3907234 0.6092766) *\n&gt; t_test$predicted &lt;- predict(tree_model, t_test, type=\"class\")\n&gt; table(t_test[, c(\"label\", \"predicted\")])\n      predicted\nlabel   bad good\n  bad  1431 1636\n  good  611 2393\nNow to the important part: We‚Äôve tested on 30% of the data our model trained on the other 70% of the data. In the above, we‚Äôve also excluded the factor-level-encoded variables because they‚Äôre a bad thing (but as we‚Äôll see in a second, they contain useful information, unfortunately). And we got some results, as such:\nBased on the data, we have trained a partitioning tree that makes mistakes about 37% of the time. As we have balanced our dataset, we know that randomly choosing one class of the other would have led us to 50% error, approximately. Still, not great.\nLet‚Äôs have a look at this ‚Äútree‚Äù:\n\n\n\nA simplistic partitioning tree\n\n\nLow depth, and still, with only two choices, we get a 63% correct classification on unseen data.\n\nOne thing to note, I‚Äôm not sure that this particular implementation of the model in fact uses Shannon‚Äôs information entropy to select nodes (it could use Gini impurity, typically). But suffice to say it could, and that‚Äôs one way a Partitioning Tree could decide which variable to choose first to make a separation in two branches, and then iterate. And I only mention it because that was the topic of last week‚Äôs entry.\n\nIt does look like the number of ‚Äúdots‚Äù in the URL, and our prevalence of vowels (which I explained a bit earlier) are important to help classify our URLs. Take note! Actually, this is a fair point, Trees are nice because they‚Äôre readable by a human. That is, the decisions of this algorithm are explainable, and that‚Äôs a good thing.\nNow without further ado, what better models I have managed to produce, just increasing depth and/or adding the (badly encoded) extension variables:\n\nWith just more decisions (more branches in the tree, i.e.¬†more depth), I got my classification to a 75% correct classification rate.\nAdding the (incorrectly) encoded extension variables, I go up to 80%.\n\n\n\n\nA somewhat better tree, albeit using bad practices"
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#conclusions",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#conclusions",
    "title": "Classifying URLs",
    "section": "Conclusions",
    "text": "Conclusions\nLots of theory covered. And only a bit of practical outcome, as today we just have a (first) model that is ‚Äúbetter than chance‚Äù, although well, far from perfect.\nIn a future post, we‚Äôll probably circle back to this exercise, to see potentially things related to other classification algorithms such as logistic regression, random forests, neural nets, and maybe SVM. Now that most of the theory is covered, it should be shorter, more to the point. (I have them all working already, I just don‚Äôt want to add content for today, it‚Äôs already too much‚Ä¶)\n\nNote: If I have time, I‚Äôll make a Shiny Application, so that you can test whether or not you can beat this simple (bad) model. Fair warning: I don‚Äôt know how the URLs were originally tagged; but I‚Äôm not much better than my very own simple partitioning tree model :D"
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#references",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#references",
    "title": "Classifying URLs",
    "section": "References",
    "text": "References\nFor today, only the recommended list of potential datasets for Cybersecurity.\nThe rest is of my own doing. Of course, the Internet, Stack Overflow, Wikipedia, etc. as usual."
  },
  {
    "objectID": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html",
    "href": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html",
    "title": "RLCS Day 1 - Part II: Basic Covering",
    "section": "",
    "text": "So I had a bit of time. And I chose to use it to keep going a bit. I moved a couple of things, created a few more supporting functions, modified them somewhat, and added the covering part.\nCovering needs to happen when for a given training instance (state, class), there is no match in the correct set.\nCorrect set is the subset of the Matching set for which, on top of the condition matching, the predicted class/action is also correct.\nAnd well, it‚Äôs working:\n\n\n\ntesting basic covering functionality\n\n\n\n\n\nNext up is ‚ÄúRule Discovery‚Äù, whereby, in Michigan-style LCS, a GA-like process will take place to create offspring from two parents taken in the Correct set.\nThat will include cross-over (in one way or another) and mutation (with a preset pressure/probability). With these, on top of coverage, the LCS will hence create new valid rules, that will become part of the population, to be evaluated on further training instances.\nOther parts of the algorithm ‚Äì meant to contain the population size ‚Äì will come later. :)"
  },
  {
    "objectID": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#its-sunday",
    "href": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#its-sunday",
    "title": "RLCS Day 1 - Part II: Basic Covering",
    "section": "",
    "text": "So I had a bit of time. And I chose to use it to keep going a bit. I moved a couple of things, created a few more supporting functions, modified them somewhat, and added the covering part.\nCovering needs to happen when for a given training instance (state, class), there is no match in the correct set.\nCorrect set is the subset of the Matching set for which, on top of the condition matching, the predicted class/action is also correct.\nAnd well, it‚Äôs working:\n\n\n\ntesting basic covering functionality"
  },
  {
    "objectID": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#conclusion",
    "href": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#conclusion",
    "title": "RLCS Day 1 - Part II: Basic Covering",
    "section": "",
    "text": "Next up is ‚ÄúRule Discovery‚Äù, whereby, in Michigan-style LCS, a GA-like process will take place to create offspring from two parents taken in the Correct set.\nThat will include cross-over (in one way or another) and mutation (with a preset pressure/probability). With these, on top of coverage, the LCS will hence create new valid rules, that will become part of the population, to be evaluated on further training instances.\nOther parts of the algorithm ‚Äì meant to contain the population size ‚Äì will come later. :)"
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html",
    "href": "posts/2024-11-10_entropy_of_zip/index.html",
    "title": "Entropy - Identifying compressed files",
    "section": "",
    "text": "About Shannon‚Äôs Information Entropy, applied to potentially detecting ciphered or compressed text compared to plain text.\n(First entry of the new platform, let‚Äôs see how it goes.)"
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#shannons-information-entropy",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#shannons-information-entropy",
    "title": "Entropy - Identifying compressed files",
    "section": "Shannon‚Äôs Information Entropy",
    "text": "Shannon‚Äôs Information Entropy\n\nWhy try to understand that?\nLong story short, Information Entropy is useful in quite a few machine learning algorithms, and to name only a few, the following two use it directly:\n\nPartitioning Trees (for nodes selection)\nLogistic Regression (through log loss)\n\nDoesn‚Äôt seem like much, said like that, but the Logistic Regression in turn can be used for‚Ä¶ Neural Networks :)\n\n\nHow it is defined?\nThe best way I personally managed to try and understand information entropy is through the concept of compression and surprise.\nA few helpful descriptions:\n‚Äú[‚Ä¶] the expected amount of information needed to describe the state of the variable [‚Ä¶]‚Äù\n‚ÄúEntropy is the measure of uncertainty of a variable. The more uncertain it is, the higher the entropy is.‚Äù\nHere is the mathematical expression of it:\n\\[\nH(X) = - \\sum_{x \\in X} p(x) log(p(x))\n\\]\nFrom the Wikipedia (I mean, why not?), this is the part that somehow can make sense for an intuitive understanding of the concept:\n‚ÄúThe information content, also called the surprisal or self-information, of an event \\(E\\) is a function which increases as the probability \\(p(E)\\) of an event decreases. When \\(p(E)\\) is close to 1, the surprisal of the event is low, but if \\(p(E)\\) is close to 0, the surprisal of the event is high. This relationship is described by the function\n\\[\nlog({1 \\over p(E)})\n\\]\nwhere \\(log()\\) is the logarithm, which gives 0 surprise when the probability of the event is 1. In fact, log is the only function that satisfies –∞ specific set of conditions [‚Ä¶]‚Äú"
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#application-detecting-cipherzip-on-data-streams",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#application-detecting-cipherzip-on-data-streams",
    "title": "Entropy - Identifying compressed files",
    "section": "Application: Detecting cipher/zip on data streams",
    "text": "Application: Detecting cipher/zip on data streams\nWe‚Äôre aiming for this today:\n\n\n\nCharacters distribution in Plain vs Zip text for a few Wiki entries\n\n\n\nThe code\nThe code will be on my Github soon enough (if not already).\nBut for now, a few blocks of it:\n\nmake_freq_df &lt;- function(filename) {\n    test1 &lt;- file(filename, open=\"rb\", raw = TRUE)\n    t1_bytes &lt;- t1_chars &lt;- c()\n    while(TRUE) {\n        temp &lt;- readBin(test1, what = \"raw\")\n        if(length(temp) == 0) break;\n        t1_bytes &lt;- c(t1_bytes, temp)\n        t1_chars &lt;- c(t1_chars, rawToChar(temp))\n    }\n    close(test1)\n    t1_df &lt;- data.frame(sort(table(as.character.hexmode(t1_bytes)), decreasing = TRUE))\n    t1_df$char &lt;- names(sort(table(t1_chars), decreasing = TRUE))\n    names(t1_df) &lt;- c(\"x\", \"probs\", \"char\")\n    # Instead of counts (table output), make it probability:\n    t1_df$probs &lt;- t1_df$probs/sum(t1_df$probs)\n    # Alternative could have been:\n    #t1_df$probs &lt;- as.numeric(prop.table(sort(table(t1_chars), decreasing = TRUE)))\n    \n    t1_df\n}\n\nThe above function is a (bad, but functional) way of taking a file, reading it in ‚Äúraw‚Äù format, and output byte-by-byte into a dataframe.\n\nThe first output column will be the ‚Äúraw byte‚Äù (for text, the ASCII code, say ‚Äú20‚Äù for space character).\nThe second column contains the Probability of appearance of a character, compared to the whole text being analysed (so, the frequency of it‚Äôs appearance).\nThe third column is for reference only, to ‚Äúsee‚Äù what the character would look like in plain text. Note that ‚Äù ‚Äù (space) and null would look similar‚Ä¶ And so would other encoded bytes, but that‚Äôs not to worry for today.\n\nWith the above in mind, here is an output of plain and zip‚Äôed text, along with the Shannon‚Äôs Entropy of it, correspondingly:\n&gt; firewall_wiki &lt;- compare_clear_zip(1, wiki_pages_df)\nupdating: posts/2024-11-10_entropy_of_zip/firewall_wiki.txt (deflated 63%)\n   x      probs char\n1 20 0.14766670     \n2 65 0.09267745    e\n3 69 0.07790143    i\n4 74 0.06658562    t\n5 6e 0.06621154    n\n6 61 0.06050687    a\n   x       probs char\n1  0 0.012244898     \n2 39 0.006722689    9\n3 72 0.006722689    r\n4 5f 0.006482593    _\n5 34 0.006242497    4\n6 e4 0.006242497 \\xe4\n[1] \"Entropy Plain text: 4.29839806234458\"\n[1] \"Entropy Zip text: 7.94701914818039\"\n\nIn Plain text, the space character appears quite a bit. So do the letters e, i, t, n, a... (That‚Äôs for English, and remember these are small sample texts extracted from some Wikipedia pages‚Ä¶). Plain text has repetition on some characters (higher probability of appearance), with varying distributions (and uses fewer different bytes).\nIn Zip, the probabilities are each MUCH lower, and more even across all possible bytes. And that‚Äôs our KEY concept for today. Zip is compression, so all its characters have as few repetition as possible (i.e.¬†low probability).\nInterestingly, with the above approach, ciphered data would look like zip data.\n\nOK, so let‚Äôs go back to our definitions of the first part:\n‚Äú[‚Ä¶] When \\(p(E)\\) is close to 1, the surprisal of the event is low, but if \\(p(E)\\) is close to 0, the surprisal of the event is high[‚Ä¶]‚Äú\nHopefully we‚Äôre getting somewhere with understanding the concept. Uncommon characters will have higher ‚Äúsurprisal‚Äù, and lower probability of appearing.\nOh: And we should not be afraid of the math, it wasn‚Äôt that bad. Here is what Shannon‚Äôs Entropy actually looks like for varying values of probability of appearance of a given character:\n\n\n\nShannon Entropy across possible probabilities\n\n\n\n\nWhat does it mean in practice?\nWell, it means that if you sample some bytes sniffed on a network, if you see seemingly random characters and no particular prevalence of any given one over the rest, you know it‚Äôs not clear-text.\nAnd yes, if you have the file extension, maybe this is all useless. So why you would care?\nFirst, this is pretty cool. If you sample data (from a network stream, or bytes on a disk‚Ä¶), you can distinguish ‚Äúautomagically‚Äù what‚Äôs plain text and what‚Äôs ciphered/zip.\nSecond: Maybe you can use that to detect covert channels out of packet capture? Or maybe let your computer on its own decide to use one set of algorithm to analyse things when there is plain text, and use another set of characters when there is ciphered/compressed text (or images, etc.)."
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#conclusions",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#conclusions",
    "title": "Entropy - Identifying compressed files",
    "section": "Conclusions",
    "text": "Conclusions\nAll this took me quite a while to really understand it. Or think I do, anyway :D\nToday we‚Äôve tried to explain the concept of information entropy through a simple application. If at this point my readers have gotten somewhat of an intuition about the concept, I‚Äôll be very happy.\nAnd the concept is quite relevant for Machine Learning, as we shall see in future posts."
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#references",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#references",
    "title": "Entropy - Identifying compressed files",
    "section": "References",
    "text": "References\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)\nThe original idea about this post I read a few years back in ‚ÄúNetwork Security Through Data Analysis‚Äù, 2nd Ed, by M. Collins (O`Reilly)"
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "I‚Äôve had my own kaizen-r.com blog for some time, and I relaunched on github.io in November last year.\nOne issue I‚Äôm facing is I need to migrate all the old contents, which were not on the same platform‚Ä¶ And so I need to manually migrate the contents of the older blog.\n\n\n\nSo one thing that happens is I have made many publications about my last MSc project. As the project is published already, it makes little sense for me to re-produce all those entries.\nSome, I might, but the majority will disappear.\nAnd some more of the old contents is probably‚Ä¶ Well, I might be better off if it disappears. Saves me the effort.\n\n\n\nText is just copy-paste, almost.\nThe issue is to re-organize the multimedia (mostly images‚Ä¶ And a few videos). That, unfortunately, will not be fast‚Ä¶\n\n\n\nIn short, I‚Äôll need to slowly move contents from the old blog, with some criteria hopefully."
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#migrating-more-of-the-old-contents",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#migrating-more-of-the-old-contents",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "I‚Äôve had my own kaizen-r.com blog for some time, and I relaunched on github.io in November last year.\nOne issue I‚Äôm facing is I need to migrate all the old contents, which were not on the same platform‚Ä¶ And so I need to manually migrate the contents of the older blog."
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#not-everything-goes",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#not-everything-goes",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "So one thing that happens is I have made many publications about my last MSc project. As the project is published already, it makes little sense for me to re-produce all those entries.\nSome, I might, but the majority will disappear.\nAnd some more of the old contents is probably‚Ä¶ Well, I might be better off if it disappears. Saves me the effort."
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#easy-and-slow",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#easy-and-slow",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "Text is just copy-paste, almost.\nThe issue is to re-organize the multimedia (mostly images‚Ä¶ And a few videos). That, unfortunately, will not be fast‚Ä¶"
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#conclusions",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#conclusions",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "In short, I‚Äôll need to slowly move contents from the old blog, with some criteria hopefully."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Version ‚Äú0‚Äù is done and working. And I will say this, it is already showing some of its power. Let me show you.\n\n\n\nAlright so this is what I have so far:\n\nA basic, fully functional, Michigan-style LCS for Supervised Learning\nA wrapper makes it easier to call the thing for training\nGA (tournament-based parents selection, one-point cross over reproduction, simple mutation) and Covering for rules discovery\nDeletion was somewhat fixed\nAccuracy and other stats work\nPrinting support functions\nNew: It includes subsumption. But calling subsumption affects performance ‚Äì both ways ‚Äì depending on the when/where/how, a known fact. But when used ‚Äì and that‚Äôs an important running decision, by the way ‚Äì, it condensates the relevant information encoded in the system‚Ä¶ Making it, when applicable, much better!\n\n\n\nsubsumption and deletion are costly, but effective to contain the size of the system\n\n\nNew too: Testing can be called now too, which is very helpful to validate the value of the LCS\nThis LCS for SL accepts an environment that, importantly, currently supports only:\n\nBinary classes/actions (so 0 or 1)\nBinary strings as input/states (no more no less) (and I mean, strings)\nAll states must be of equal length (whatever that is)\n\n\nThat‚Äôs on the plus side. On the not-so-great, well:\n\nLittle-to-no error control in there yet\nNext-to-zero documentation so far\nAnd I‚Äôll make a note on performance later in this post.\nImperfect/still-a-bit-disorganized code is probably an understatement.\n\nBut that‚Äôs just what needs to happen next, I guess. Now on to what you can do with this thing.\n\n\n\nSo I‚Äôve already shown that one, but it wasn‚Äôt ‚Äúperfect‚Äù. Here the new version and a few tricks allow me to show how the LCS can be used for data mining.\nSo I give it 70% of the sample space, and it comes up with the following ‚ÄúClassifiers Set‚Äù:\n\n\n\nUsing LCS to discover the hidden rules in the dataset\n\n\nNotice how this is quite exactly what the LCS was supposed to try and discover. And also how it indeed tells us what that particular ‚Äúenvironment‚Äù (dataset and its classes) is all about.\n(Oh, and in this particular exercise, I really did not need to run so many epochs, it‚Äôs just my default hyperparameter setting, but after a hundred or so epochs, it was already done.)\nLet‚Äôs move to a more interesting example.\n\n\n\nThe ‚ÄúIntroduction to Learning Classifier Systems‚Äù book ‚Äì referenced in my first post on LCS ‚Äì explains the algorithm with (among much more explanations) one particular example, called a ‚Äú6-bit multiplexer‚Äù.\nThe rules of that particular example are like so:\n\nyou get a string of 6 bits as input\nthe first two bits point to an address in the resting 4 bits\nthe ‚Äúclass‚Äù of your classifier should be whatever the bit is at the designated address.\n\nA couple of examples:\n\n‚Äú010000‚Äù says ‚ÄúBit 2 of the last 4 bits is the class, so 0‚Äù\n‚Äú110001‚Äù says ‚ÄúBit 4 of the last 4 bits is the class, so 1‚Äù\n‚Äú101101‚Äù says ‚ÄúBit 3 of the last 4 bits is the class, so 0‚Äù\n‚Ä¶\n\nYou get the idea. One aspect of this is meant to show that the LCS SL can somehow recognize things that are interrelated in the data, here how two different parts interact. They call this ‚Äúepistasis‚Äù. For now, suffice to say, it‚Äôs a bit more interesting use-case and we hope our LCS can discover relevant rules.\nAgain, with 70% of the whole dataset (64 entries total, for 6 bits), we train our LCS and hope to identify important rules (ideally, up to 8 perfect rules in this case ‚Äì but that‚Äôs not guaranteed). Here I show one such trained Classifier Set (aka ‚ÄúLCS‚Äù or ‚ÄúLearning Classifier System‚Äù, where the S stands for ‚ÄúSystem‚Äù, implying more than one classifier working together‚Ä¶ but I digress) that gets it almost perfect:\n\n\n\nAlmost perfect MUX6 LCS\n\n\nYou have to look somewhat carefully but you should find 7 of the 8 ‚Äúperfect‚Äù classifiers in there. And I haven‚Äôt checked for this particular example, but if the training set (randomly selected) did contain all the needed cases (probably did), this LCS should also almost cover for the missing classifier (11###0 0). Hint, the following would be the only one that would match for that missing classifier, so they would participate to recommend the action/class, and see how they all recommend class 0 (‚Ä¶):\n\n1####0 0\n#1#0#0 0\n##00#0 0\n##0#10 0\n\nSo in spite of missing one perfect rule (and that‚Äôs also depending on the random environment and non-deterministic nature of the LCS training‚Ä¶), it would work in all testing cases.\nLet‚Äôs move on to a (much) harder use-case.\n\n\n\nNow this was a stretch. the MNIST dataset is nothing like the dummy tests above, for sure. First, I don‚Äôt know how to manually write a perfect program manually for it (the above cases were trivial, comparatively).\nSecond, many trained neural nets on the MNIST dataset don‚Äôt get perfect record (but that‚Äôs understandable, if you spend enough time with that dataset).\nThird, and quite important, the ‚Äústates‚Äù here are numerical strings representing the shade (black-white) of 784 pixels (28*28) integers between 0 and 255. With a simple binary transformation I could end up with my states represented each as 6272 binary strings (8 bits per pixel).\nAnd there are a few thousand entries (samples) per represented number.\nI knew upfront, doing thousands of exposures per sample to get anywhere would take forever.\nFair enough, and so I simplified the problem first. Here is what I did:\n\nCompress the data, from shades to 0-1 values per pixel. I used a threshold to ‚Äúsharpen‚Äù the dataset, so only if the value was &gt; 100 (out of 255) I would set my new pixel to 1, otherwise I set it to 0.\nCompress further, from 28*28 to 7*7 pixels. Each original 4-pixels-square is encoded as only 1 pixel, with value 1 if any of the 4*4 subsquare was one, 0 otherwise.\nThe above gives me 7*7 binary strings, so my states are now 49 bits long.\n\n\n\nI ‚Äúcompress‚Äù the MNIST data before training my LCS\n\n\nClass stays the same, but I also will do a simpler exercise. My LCS should distinguish between classes 0 and 1 only. (This is not an issue, it‚Äôs a choice. I just choose to keep things simple, that‚Äôs all, but the value of the approach is the same, think of it as a one-versus-rest approach if you want). This is in part to make things faster, and in (big) part because the code so far only allows two-classes-output only.\n\nLet me skip directly to the results here:\n\n\n\nShowing a couple of runs of an LCS SL on MNIST simplified dataset\n\n\n(BTW, I meant to write ‚Äúconfusion matrix‚Äù, not ‚Äúcontention table‚Äù above, obviously‚Ä¶ Apologies, I hadn‚Äôt slept much yesterday‚Ä¶)\nI just want to make this clear:\n\nTraining on a total 500 random (sampled) training inputs only (that‚Äôs the size of my environment here), so roughly 250 examples of pictures of Zero and 250 of One\nTraining on these 500 examples takes me between 7.4 and 9.3 minutes (in the above example)\nIt generalizes very well: I end up with an LCS that correctly classified 96 to 97+% of the 3799 sample tests.\n\n\n\nJust FYI, I worked a bit more (31/12) on this and reduced the running times for better results in just above 20 seconds for the mono-thread version:\n\n\n\nBetter speed AND accuracy after slight code update\n\n\nThis gives us in fact 98.6+% of correctly classified test samples after 20s training (still on only 500 training samples)!\n\n\n\n\nOne thing that has worried me the past few days, something I actually knew was a concern with LCS, was performance. I mean speed, here. (SEE EDIT ABOVE before you continue)\nTraining requires exposing the LCS to sufficient training instances to ‚Äúrepresent‚Äù the overall data. Here I believe, in LCS in particular, too many training instances would hinder training because of the speed of processing power needed. I truly believe better chosen training instances are more important than more training instances. Kinda contrary of what Deep Learning would have you think. Although, this belief is, right now, just that: A belief.\nLCS on the other hand, for each training instance, require a lot of ‚Äúepochs‚Äù, so seeing the same data quite a few times. That is because after covering, the GA kicks in and that happens with choosing what best rules are available, which are evolved over time and where ‚Äúbest‚Äù depends on how many matches, how general but specific enough a rule is, whether a rule not only matches but also returns the correct action‚Ä¶ The point is, ideally, you want to train your LCS on a dataset, but circle over that dataset many times. I‚Äôm talking, thousands of times ideally - for each entry.\nThe other aspect of it is, of course, choosing R as a programming language. As much as I am comfortable with it, it is a slow language, if you consider compiled alternatives and such.\nI wanted to dive a bit into the details of what in my current (basic) implementation I could do, and so I ran the thing through ‚Äúprofvis‚Äù quite a few times, separate functions at separate times, then overall, with larger problems, etc.\nOne thing I do a lot, because I‚Äôm using lists for everything right now, is use ‚Äúlapply()‚Äù (and al.). I have tried as much as I could to not use dependencies if I could avoid it. So much so, I believe the only actual dependency I have is the plyr package for the rbind() function, and that‚Äôs for printing stuff‚Ä¶\nNot having too many dependencies is partly motivated by my end goal of producing a package, which I‚Äôd like to make ‚Äúlight‚Äù as much as possible.\nAnyhow, so I use lapply() a lot, but I have also used %in% quite a few times. And in some cases, I mix both and check quite a few instances, quite a few times!\nMatching, which happens a lot, for instance, can have an impact:\n\n\n\nProfiling run-times of mono-thread R interpreted code with LOTS of iterations in there‚Ä¶\n\n\n\n\n\nYou might know this by now: I like to parallelize stuff over my CPU cores/threads. It‚Äôs just a thing I think about when I see my processing times are‚Ä¶ Long. Even if long is a few seconds. And here, we‚Äôre talking many iterations, which on bigger datasets/longer states means minutes.\n(Actually, before I decided to simplify to 49 bits the states of the MNIST exercise, I was working with 784 bits states, and well, it would NOT converge after several minutes‚Ä¶)\nWell, I have to keep working on this, but the cool thing is, an LCS can be pre-trained, and then re-trained.\nSo the LCS (in supervised learning setting for now) is nothing but a set of classifiers, which are rules learnt from examples. You can improve it by exposing it, simply, to more examples. Or let it run more iterations over the same examples!\nEither are valid approaches.\nNow‚Ä¶ What if I ‚Äúsharded‚Äù my data? Say, if I have 7 CPU cores‚Ä¶\nI could do one shorter run over all the data to generate a first ruleset. I could then use at the same time that pretrained LCS, and what if I trained 7 sub-LCS on 7 subsets of the data for more iterations (quite a few more epochs now, but in parallel, each on smaller data samples)?\nWhat if then I loop over that, with some sort of ‚Äúcompaction‚Äù of the 7 newly re-trained sub-LCS and generate a new single base LCS, to then re-distribute to 7 sub processes.\nWhat if I did that a few times?\nI still have to think (seriously, and hard) through all the implications of the simple ‚Äúalgorithm‚Äù I just proposed above. I came up with it quite independently, mind you. And maybe it‚Äôs a very bad idea. (not showing all the training examples hinders the selection and then generalization of classifiers, for one‚Ä¶ So maybe it‚Äôs a stupid idea‚Ä¶ I just haven‚Äôt thought it through yet. For now I do think maybe it does in fact kinda make sense.)\nBUT, I have done it already! One version of my code allows for parallel processing like just described.\nAnd well, in (quite a few) tests with the MNIST example above, for roughly the same overall runtimes (and ‚Äúcompaction‚Äù is costly!) I get a gain of 1-2% of correctly classified samples.\nFrom 96% to 97+%. (It might sound marginal, and indeed I‚Äôm talking about a LOT more processing, but the same runtimes with marginal gains in such near-100% ranges, sounds like potentially valuable).\nWell, indeed, I‚Äôve done that too:\n\n\n\nCPU-based parallel training of my LCS\n\n\n\n\n\nSo I‚Äôve got all the pieces of the puzzle. And it works. And I have validated some marvellous (to me) results. All in a few days.\nWell, not a ‚Äúfew days‚Äù. See, I‚Äôve coded for only a few hours maybe, but I‚Äôve been thinking about this for a long time. And I also think it‚Äôd be unfair to say I was fast: Coding time per-se doesn‚Äôt account for waking up in the middle of the night coming up with my own design to approach the MNIST example. Waking up in the middle of the night, when I have a project like this, somehow happens more often. I have a bit of insomnia, but these projects make it worse, although they also make me more‚Ä¶ Productive.\nI guess what I‚Äôm saying is: coding time might not have been too many hours. But thinking time, or ‚Äúobsessing‚Äù (rather) did account for much more than 4 or 5 days (in 4 to 8h/day). I mean, I have designed everything in between the days of coding, and that started long before.\nAnyhow, to the next step: Moving from a few functions and examples to an actual Package!\nThat‚Äôs what I‚Äôll be up to when I next have time to dedicate to this.\n\n\n\nWell, that is, indeed, when I next have the time. I‚Äôll be disconnecting for a couple of weeks during January. I simply have other personal priorities (all great).\nSo I‚Äôll take this where I left it maybe a bit later. No matter. I have gone through half the book of packages, and I actually am more convinced than ever that this is the perfect opportunity to create a package for me.\n\n\n\nWell, I am just so‚Ä¶ Proud of this thing already!\nSee I had only ever read about the idea until‚Ä¶ Well, basically a week ago. Thanks to the holiday season, and long week-end hours, I can now say I have a working LCS in R.\nNot only that:\n\nI know it‚Äôs already not too bad in terms of what you could expect about performance. I know (from profiling) where I could try to go RCpp. I also have ideas to run it in more ‚Äúhighly‚Äù parallel setups. With more cores on a machine, or‚Ä¶ More machines! (plumbeR, here I come‚Ä¶ Well, later though, that‚Äôs not urgent)\nBut I also know, if I can ‚Äúencode‚Äù a problem into a binary string for binary classification, it turns out LCS are a very valid idea to consider! Just as good as a Neural Network! Heck, in some cases, better!\nI have done some testthat() already, I have several separate functions, very few (almost none) dependencies, the thing is somewhat organized‚Ä¶ So the next step comes rather naturally.\n\nAnyhow, overall, I am happy to report, I do believe this (future) package can prove very valuable to the community!\nAnd at this stage, I already hope I can explain that in future forums - I will convince people, I‚Äôm sure of it.\nSo this is turning out to be as great a project as I had (unreasonably) imagined already a long time ago. Which, let me tell you, is very satisfying at a very personal level.\n\n\n\nAgain, if some keywords above are confusing, the how of the algorithm is neatly explained in this 12‚Äô video, which I highly recommend."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#intro-imperfect-but-very-cool",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#intro-imperfect-but-very-cool",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Version ‚Äú0‚Äù is done and working. And I will say this, it is already showing some of its power. Let me show you."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#whats-in-it",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#whats-in-it",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Alright so this is what I have so far:\n\nA basic, fully functional, Michigan-style LCS for Supervised Learning\nA wrapper makes it easier to call the thing for training\nGA (tournament-based parents selection, one-point cross over reproduction, simple mutation) and Covering for rules discovery\nDeletion was somewhat fixed\nAccuracy and other stats work\nPrinting support functions\nNew: It includes subsumption. But calling subsumption affects performance ‚Äì both ways ‚Äì depending on the when/where/how, a known fact. But when used ‚Äì and that‚Äôs an important running decision, by the way ‚Äì, it condensates the relevant information encoded in the system‚Ä¶ Making it, when applicable, much better!\n\n\n\nsubsumption and deletion are costly, but effective to contain the size of the system\n\n\nNew too: Testing can be called now too, which is very helpful to validate the value of the LCS\nThis LCS for SL accepts an environment that, importantly, currently supports only:\n\nBinary classes/actions (so 0 or 1)\nBinary strings as input/states (no more no less) (and I mean, strings)\nAll states must be of equal length (whatever that is)\n\n\nThat‚Äôs on the plus side. On the not-so-great, well:\n\nLittle-to-no error control in there yet\nNext-to-zero documentation so far\nAnd I‚Äôll make a note on performance later in this post.\nImperfect/still-a-bit-disorganized code is probably an understatement.\n\nBut that‚Äôs just what needs to happen next, I guess. Now on to what you can do with this thing."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-1-not-bit-4",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-1-not-bit-4",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "So I‚Äôve already shown that one, but it wasn‚Äôt ‚Äúperfect‚Äù. Here the new version and a few tricks allow me to show how the LCS can be used for data mining.\nSo I give it 70% of the sample space, and it comes up with the following ‚ÄúClassifiers Set‚Äù:\n\n\n\nUsing LCS to discover the hidden rules in the dataset\n\n\nNotice how this is quite exactly what the LCS was supposed to try and discover. And also how it indeed tells us what that particular ‚Äúenvironment‚Äù (dataset and its classes) is all about.\n(Oh, and in this particular exercise, I really did not need to run so many epochs, it‚Äôs just my default hyperparameter setting, but after a hundred or so epochs, it was already done.)\nLet‚Äôs move to a more interesting example."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-2-mux6",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-2-mux6",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "The ‚ÄúIntroduction to Learning Classifier Systems‚Äù book ‚Äì referenced in my first post on LCS ‚Äì explains the algorithm with (among much more explanations) one particular example, called a ‚Äú6-bit multiplexer‚Äù.\nThe rules of that particular example are like so:\n\nyou get a string of 6 bits as input\nthe first two bits point to an address in the resting 4 bits\nthe ‚Äúclass‚Äù of your classifier should be whatever the bit is at the designated address.\n\nA couple of examples:\n\n‚Äú010000‚Äù says ‚ÄúBit 2 of the last 4 bits is the class, so 0‚Äù\n‚Äú110001‚Äù says ‚ÄúBit 4 of the last 4 bits is the class, so 1‚Äù\n‚Äú101101‚Äù says ‚ÄúBit 3 of the last 4 bits is the class, so 0‚Äù\n‚Ä¶\n\nYou get the idea. One aspect of this is meant to show that the LCS SL can somehow recognize things that are interrelated in the data, here how two different parts interact. They call this ‚Äúepistasis‚Äù. For now, suffice to say, it‚Äôs a bit more interesting use-case and we hope our LCS can discover relevant rules.\nAgain, with 70% of the whole dataset (64 entries total, for 6 bits), we train our LCS and hope to identify important rules (ideally, up to 8 perfect rules in this case ‚Äì but that‚Äôs not guaranteed). Here I show one such trained Classifier Set (aka ‚ÄúLCS‚Äù or ‚ÄúLearning Classifier System‚Äù, where the S stands for ‚ÄúSystem‚Äù, implying more than one classifier working together‚Ä¶ but I digress) that gets it almost perfect:\n\n\n\nAlmost perfect MUX6 LCS\n\n\nYou have to look somewhat carefully but you should find 7 of the 8 ‚Äúperfect‚Äù classifiers in there. And I haven‚Äôt checked for this particular example, but if the training set (randomly selected) did contain all the needed cases (probably did), this LCS should also almost cover for the missing classifier (11###0 0). Hint, the following would be the only one that would match for that missing classifier, so they would participate to recommend the action/class, and see how they all recommend class 0 (‚Ä¶):\n\n1####0 0\n#1#0#0 0\n##00#0 0\n##0#10 0\n\nSo in spite of missing one perfect rule (and that‚Äôs also depending on the random environment and non-deterministic nature of the LCS training‚Ä¶), it would work in all testing cases.\nLet‚Äôs move on to a (much) harder use-case."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-3-mnist-with-tricks",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-3-mnist-with-tricks",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Now this was a stretch. the MNIST dataset is nothing like the dummy tests above, for sure. First, I don‚Äôt know how to manually write a perfect program manually for it (the above cases were trivial, comparatively).\nSecond, many trained neural nets on the MNIST dataset don‚Äôt get perfect record (but that‚Äôs understandable, if you spend enough time with that dataset).\nThird, and quite important, the ‚Äústates‚Äù here are numerical strings representing the shade (black-white) of 784 pixels (28*28) integers between 0 and 255. With a simple binary transformation I could end up with my states represented each as 6272 binary strings (8 bits per pixel).\nAnd there are a few thousand entries (samples) per represented number.\nI knew upfront, doing thousands of exposures per sample to get anywhere would take forever.\nFair enough, and so I simplified the problem first. Here is what I did:\n\nCompress the data, from shades to 0-1 values per pixel. I used a threshold to ‚Äúsharpen‚Äù the dataset, so only if the value was &gt; 100 (out of 255) I would set my new pixel to 1, otherwise I set it to 0.\nCompress further, from 28*28 to 7*7 pixels. Each original 4-pixels-square is encoded as only 1 pixel, with value 1 if any of the 4*4 subsquare was one, 0 otherwise.\nThe above gives me 7*7 binary strings, so my states are now 49 bits long.\n\n\n\nI ‚Äúcompress‚Äù the MNIST data before training my LCS\n\n\nClass stays the same, but I also will do a simpler exercise. My LCS should distinguish between classes 0 and 1 only. (This is not an issue, it‚Äôs a choice. I just choose to keep things simple, that‚Äôs all, but the value of the approach is the same, think of it as a one-versus-rest approach if you want). This is in part to make things faster, and in (big) part because the code so far only allows two-classes-output only.\n\nLet me skip directly to the results here:\n\n\n\nShowing a couple of runs of an LCS SL on MNIST simplified dataset\n\n\n(BTW, I meant to write ‚Äúconfusion matrix‚Äù, not ‚Äúcontention table‚Äù above, obviously‚Ä¶ Apologies, I hadn‚Äôt slept much yesterday‚Ä¶)\nI just want to make this clear:\n\nTraining on a total 500 random (sampled) training inputs only (that‚Äôs the size of my environment here), so roughly 250 examples of pictures of Zero and 250 of One\nTraining on these 500 examples takes me between 7.4 and 9.3 minutes (in the above example)\nIt generalizes very well: I end up with an LCS that correctly classified 96 to 97+% of the 3799 sample tests.\n\n\n\nJust FYI, I worked a bit more (31/12) on this and reduced the running times for better results in just above 20 seconds for the mono-thread version:\n\n\n\nBetter speed AND accuracy after slight code update\n\n\nThis gives us in fact 98.6+% of correctly classified test samples after 20s training (still on only 500 training samples)!"
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#performance",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#performance",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "One thing that has worried me the past few days, something I actually knew was a concern with LCS, was performance. I mean speed, here. (SEE EDIT ABOVE before you continue)\nTraining requires exposing the LCS to sufficient training instances to ‚Äúrepresent‚Äù the overall data. Here I believe, in LCS in particular, too many training instances would hinder training because of the speed of processing power needed. I truly believe better chosen training instances are more important than more training instances. Kinda contrary of what Deep Learning would have you think. Although, this belief is, right now, just that: A belief.\nLCS on the other hand, for each training instance, require a lot of ‚Äúepochs‚Äù, so seeing the same data quite a few times. That is because after covering, the GA kicks in and that happens with choosing what best rules are available, which are evolved over time and where ‚Äúbest‚Äù depends on how many matches, how general but specific enough a rule is, whether a rule not only matches but also returns the correct action‚Ä¶ The point is, ideally, you want to train your LCS on a dataset, but circle over that dataset many times. I‚Äôm talking, thousands of times ideally - for each entry.\nThe other aspect of it is, of course, choosing R as a programming language. As much as I am comfortable with it, it is a slow language, if you consider compiled alternatives and such.\nI wanted to dive a bit into the details of what in my current (basic) implementation I could do, and so I ran the thing through ‚Äúprofvis‚Äù quite a few times, separate functions at separate times, then overall, with larger problems, etc.\nOne thing I do a lot, because I‚Äôm using lists for everything right now, is use ‚Äúlapply()‚Äù (and al.). I have tried as much as I could to not use dependencies if I could avoid it. So much so, I believe the only actual dependency I have is the plyr package for the rbind() function, and that‚Äôs for printing stuff‚Ä¶\nNot having too many dependencies is partly motivated by my end goal of producing a package, which I‚Äôd like to make ‚Äúlight‚Äù as much as possible.\nAnyhow, so I use lapply() a lot, but I have also used %in% quite a few times. And in some cases, I mix both and check quite a few instances, quite a few times!\nMatching, which happens a lot, for instance, can have an impact:\n\n\n\nProfiling run-times of mono-thread R interpreted code with LOTS of iterations in there‚Ä¶"
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#nicos-trick-parallelizing",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#nicos-trick-parallelizing",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "You might know this by now: I like to parallelize stuff over my CPU cores/threads. It‚Äôs just a thing I think about when I see my processing times are‚Ä¶ Long. Even if long is a few seconds. And here, we‚Äôre talking many iterations, which on bigger datasets/longer states means minutes.\n(Actually, before I decided to simplify to 49 bits the states of the MNIST exercise, I was working with 784 bits states, and well, it would NOT converge after several minutes‚Ä¶)\nWell, I have to keep working on this, but the cool thing is, an LCS can be pre-trained, and then re-trained.\nSo the LCS (in supervised learning setting for now) is nothing but a set of classifiers, which are rules learnt from examples. You can improve it by exposing it, simply, to more examples. Or let it run more iterations over the same examples!\nEither are valid approaches.\nNow‚Ä¶ What if I ‚Äúsharded‚Äù my data? Say, if I have 7 CPU cores‚Ä¶\nI could do one shorter run over all the data to generate a first ruleset. I could then use at the same time that pretrained LCS, and what if I trained 7 sub-LCS on 7 subsets of the data for more iterations (quite a few more epochs now, but in parallel, each on smaller data samples)?\nWhat if then I loop over that, with some sort of ‚Äúcompaction‚Äù of the 7 newly re-trained sub-LCS and generate a new single base LCS, to then re-distribute to 7 sub processes.\nWhat if I did that a few times?\nI still have to think (seriously, and hard) through all the implications of the simple ‚Äúalgorithm‚Äù I just proposed above. I came up with it quite independently, mind you. And maybe it‚Äôs a very bad idea. (not showing all the training examples hinders the selection and then generalization of classifiers, for one‚Ä¶ So maybe it‚Äôs a stupid idea‚Ä¶ I just haven‚Äôt thought it through yet. For now I do think maybe it does in fact kinda make sense.)\nBUT, I have done it already! One version of my code allows for parallel processing like just described.\nAnd well, in (quite a few) tests with the MNIST example above, for roughly the same overall runtimes (and ‚Äúcompaction‚Äù is costly!) I get a gain of 1-2% of correctly classified samples.\nFrom 96% to 97+%. (It might sound marginal, and indeed I‚Äôm talking about a LOT more processing, but the same runtimes with marginal gains in such near-100% ranges, sounds like potentially valuable).\nWell, indeed, I‚Äôve done that too:\n\n\n\nCPU-based parallel training of my LCS"
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#next-steps-packaging",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#next-steps-packaging",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "So I‚Äôve got all the pieces of the puzzle. And it works. And I have validated some marvellous (to me) results. All in a few days.\nWell, not a ‚Äúfew days‚Äù. See, I‚Äôve coded for only a few hours maybe, but I‚Äôve been thinking about this for a long time. And I also think it‚Äôd be unfair to say I was fast: Coding time per-se doesn‚Äôt account for waking up in the middle of the night coming up with my own design to approach the MNIST example. Waking up in the middle of the night, when I have a project like this, somehow happens more often. I have a bit of insomnia, but these projects make it worse, although they also make me more‚Ä¶ Productive.\nI guess what I‚Äôm saying is: coding time might not have been too many hours. But thinking time, or ‚Äúobsessing‚Äù (rather) did account for much more than 4 or 5 days (in 4 to 8h/day). I mean, I have designed everything in between the days of coding, and that started long before.\nAnyhow, to the next step: Moving from a few functions and examples to an actual Package!\nThat‚Äôs what I‚Äôll be up to when I next have time to dedicate to this."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#a-few-days-off",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#a-few-days-off",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Well, that is, indeed, when I next have the time. I‚Äôll be disconnecting for a couple of weeks during January. I simply have other personal priorities (all great).\nSo I‚Äôll take this where I left it maybe a bit later. No matter. I have gone through half the book of packages, and I actually am more convinced than ever that this is the perfect opportunity to create a package for me."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#conclusion",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#conclusion",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Well, I am just so‚Ä¶ Proud of this thing already!\nSee I had only ever read about the idea until‚Ä¶ Well, basically a week ago. Thanks to the holiday season, and long week-end hours, I can now say I have a working LCS in R.\nNot only that:\n\nI know it‚Äôs already not too bad in terms of what you could expect about performance. I know (from profiling) where I could try to go RCpp. I also have ideas to run it in more ‚Äúhighly‚Äù parallel setups. With more cores on a machine, or‚Ä¶ More machines! (plumbeR, here I come‚Ä¶ Well, later though, that‚Äôs not urgent)\nBut I also know, if I can ‚Äúencode‚Äù a problem into a binary string for binary classification, it turns out LCS are a very valid idea to consider! Just as good as a Neural Network! Heck, in some cases, better!\nI have done some testthat() already, I have several separate functions, very few (almost none) dependencies, the thing is somewhat organized‚Ä¶ So the next step comes rather naturally.\n\nAnyhow, overall, I am happy to report, I do believe this (future) package can prove very valuable to the community!\nAnd at this stage, I already hope I can explain that in future forums - I will convince people, I‚Äôm sure of it.\nSo this is turning out to be as great a project as I had (unreasonably) imagined already a long time ago. Which, let me tell you, is very satisfying at a very personal level."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#references",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#references",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Again, if some keywords above are confusing, the how of the algorithm is neatly explained in this 12‚Äô video, which I highly recommend."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "Recently, I had ‚Äúbroken‚Äù my Reinforcement Learning implementation, in the sense that it would learn at first, and then it would get worse with more experience, instead of better.\nWith so many hyper-parameters to consider, I knew I had touched too many levers while trying to improve‚Ä¶\nOne key thing was my exploration prevalence. But I didn‚Äôt know that. I start training with high exploration (1 step in each 2). That‚Äôs meant to expose to the maximum possible situations at first. In the smaller worlds, that should cover more environment states, and so reward of different actions would be learnt early on. That was the idea behind my choice, anyway. It kind of made sense‚Ä¶\nBut then I knew of course that too much exploration would mean‚Ä¶ Worse rewards.\nThat was correct, but a few more things were happening‚Ä¶ The short list here:\n\nmy reward update function per classifier of the LCS was not great\nand my choosing of best classifiers (influencing the algorithm step of going from match-set to action-set) was in fact all wrong too ü§¶‚Äç‚ôÇÔ∏è\n\nTheory to the rescue!\n\n\n\nI bought the ‚Äúbible of RL‚Äù (see Resources below) and recently received it at home. And it‚Äôs already making a difference! (Actually, having a physical copy was a great choice for me.)\nI have now corrected the above issues, but inspired by only the first two chapters (the second one really, as I had glanced at the first one earlier‚Ä¶) I was able to implement 2 different reward-update functions that (along with exploration pressure increase) made all the difference.\nPlus I now have initialized my rewards with a high positive value to foment more exploration at the earlier stages.\n\n\n\nTo emit a judgement on the quality of my implementation, along with the right mix of hyper-parameters (then again, I‚Äôm not saying they‚Äôre optimal, I‚Äôm saying they‚Äôre OK)‚Ä¶\nI had to sit down and choose what would demonstrate ‚Äúlearning‚Äù in my small scenario.\nSo I kept things very simple:\n\n1 agent\n4 actions corresponding to the 4 directions (horizontal and vertical only)\nAt all steps, the agent can see empty cells, wall cells, food cells, enemy cells and where it came from (allocentric perception forced me to do that), 1 cell in any direction (including diagonals).\n\nIn a simple scenario, 5x5 world, there are at any point: 16 wall cells, 5 empty cells, 1 food, 1 enemy, 1 walk-back.\n\n\n\na simple world to study learning quality\n\n\nGiven that, I think this demonstrates how good the agent is after some training:\n\n\n\nlearning with sample average reward update\n\n\nThe above is using a ‚Äúsample average‚Äù reward update function, whereby the reward contribution gets smaller and smaller with time.\nThis learns the best exploitation policy in certain scenarios where there is a unique perfect policy. However, in our case, given the limited agent perception, one could consider that it would be best to keep learning over time.\nThat is because the ‚Äúfood‚Äù and the ‚Äúenemy‚Äù positions change once the agent collides with them. Although in the longer run that is probably incorrect, there are many possible perceived states even in this very small scenario (and in spite of the LCS and its GA pressuring to learn of each scenario only the relevant part of the states‚Ä¶), we could think that with limited learning steps, an option that ensures the agent keeps learning in a non-stationary environment will be better off.\nWell, so I implemented the equations with fixed alpha (set to 0.1).\nNote that in all the above, I keep starting training with one exploratory step in each 2, then reduce exploration every 2000 time-steps, until I get to 1 in 6. That forces exploration too, with more pressure at the beginning.\nWith the new rules this might be in fact unnecessary, I am aware, but I still haven‚Äôt tested with fixed exploration pressure (say always 1 in 6 in my example). It should work!\nAnyhow, the results with fixed alpha are almost identical in my simple scenario:\n\n\n\nlearning with fixed alpha\n\n\n\n\n\nWell, we‚Äôve seen there are:\n\nMore wall cells (16) than empty cells (5)\nmore empty cells (5) than enemy, food and ‚Äúwalk-back‚Äù cell (of which there is one)\n\nAnd yet, after some training, the agent averages, over 1000 most recent steps taken:\n\n275 food eaten (most rewarding)\n3 enemy encounters (most penalized)\nless than 100 wall bumps (heavily penalized)\nabout 150 walk-backs (slightly penalized)\nand well, quite a bit of walking around in empty cells\n\nI haven‚Äôt done the math, but in some of the perceived states:\n\nCorner: 5 walls/8 total cells. There are 4 corners. 62.5%\nSide: 3 walls/8. 37.5%\nThe above account for 8 of 9 possible agent positions (exceptuating center cell)\n\nAnd yet, only 10% of the time the agent bumps into a wall on average (and that‚Äôs with a 17% chance of exploratory move!).\nAs the agent has very limited state perception, I believe this is proof right there that it has indeed learnt something.\nI‚Äôd have to run many more numbers (4 moves possible, 9 locations, 5 different possible cells-status per cell, in different combinations‚Ä¶ I‚Äôm not doing that until someone asks me to do a PhD on the topic üòÅ)\nSimilar numbers could demonstrate that there are lots of food-related actions compared to white cell prevalence‚Ä¶\nAnd indeed, supposing the identical probability of encountering food or enemy at any stage, the fact that our agent has chosen the right action when faced with both possibilities, choosing food 275 times for only 3 times the enemy (again, exploration step 1 in 6 in all described statistics above), well‚Ä¶\nI‚Äôm sold.\n\n\n\nI‚Äôm very happy about it all. The Reinforcement Learning example I chose is working nicely! And it makes sense, based on the little theory I have studied thus far.\n(Note: I shall actually post a short entry on real-world application of data mining with my LCS implementation. Any day now, it has happened already‚Ä¶).\n\n\n\n‚ÄúReinforcement Learning, An introduction.‚Äù, 2nd Ed. Bradford Books, by R. S. Sutton & A. G. Bardo."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#using-theory-is-proving-useful-indeed",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#using-theory-is-proving-useful-indeed",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "Recently, I had ‚Äúbroken‚Äù my Reinforcement Learning implementation, in the sense that it would learn at first, and then it would get worse with more experience, instead of better.\nWith so many hyper-parameters to consider, I knew I had touched too many levers while trying to improve‚Ä¶\nOne key thing was my exploration prevalence. But I didn‚Äôt know that. I start training with high exploration (1 step in each 2). That‚Äôs meant to expose to the maximum possible situations at first. In the smaller worlds, that should cover more environment states, and so reward of different actions would be learnt early on. That was the idea behind my choice, anyway. It kind of made sense‚Ä¶\nBut then I knew of course that too much exploration would mean‚Ä¶ Worse rewards.\nThat was correct, but a few more things were happening‚Ä¶ The short list here:\n\nmy reward update function per classifier of the LCS was not great\nand my choosing of best classifiers (influencing the algorithm step of going from match-set to action-set) was in fact all wrong too ü§¶‚Äç‚ôÇÔ∏è\n\nTheory to the rescue!"
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#i-only-have-gone-through-chapter-1-2-and-already",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#i-only-have-gone-through-chapter-1-2-and-already",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "I bought the ‚Äúbible of RL‚Äù (see Resources below) and recently received it at home. And it‚Äôs already making a difference! (Actually, having a physical copy was a great choice for me.)\nI have now corrected the above issues, but inspired by only the first two chapters (the second one really, as I had glanced at the first one earlier‚Ä¶) I was able to implement 2 different reward-update functions that (along with exploration pressure increase) made all the difference.\nPlus I now have initialized my rewards with a high positive value to foment more exploration at the earlier stages."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#results",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#results",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "To emit a judgement on the quality of my implementation, along with the right mix of hyper-parameters (then again, I‚Äôm not saying they‚Äôre optimal, I‚Äôm saying they‚Äôre OK)‚Ä¶\nI had to sit down and choose what would demonstrate ‚Äúlearning‚Äù in my small scenario.\nSo I kept things very simple:\n\n1 agent\n4 actions corresponding to the 4 directions (horizontal and vertical only)\nAt all steps, the agent can see empty cells, wall cells, food cells, enemy cells and where it came from (allocentric perception forced me to do that), 1 cell in any direction (including diagonals).\n\nIn a simple scenario, 5x5 world, there are at any point: 16 wall cells, 5 empty cells, 1 food, 1 enemy, 1 walk-back.\n\n\n\na simple world to study learning quality\n\n\nGiven that, I think this demonstrates how good the agent is after some training:\n\n\n\nlearning with sample average reward update\n\n\nThe above is using a ‚Äúsample average‚Äù reward update function, whereby the reward contribution gets smaller and smaller with time.\nThis learns the best exploitation policy in certain scenarios where there is a unique perfect policy. However, in our case, given the limited agent perception, one could consider that it would be best to keep learning over time.\nThat is because the ‚Äúfood‚Äù and the ‚Äúenemy‚Äù positions change once the agent collides with them. Although in the longer run that is probably incorrect, there are many possible perceived states even in this very small scenario (and in spite of the LCS and its GA pressuring to learn of each scenario only the relevant part of the states‚Ä¶), we could think that with limited learning steps, an option that ensures the agent keeps learning in a non-stationary environment will be better off.\nWell, so I implemented the equations with fixed alpha (set to 0.1).\nNote that in all the above, I keep starting training with one exploratory step in each 2, then reduce exploration every 2000 time-steps, until I get to 1 in 6. That forces exploration too, with more pressure at the beginning.\nWith the new rules this might be in fact unnecessary, I am aware, but I still haven‚Äôt tested with fixed exploration pressure (say always 1 in 6 in my example). It should work!\nAnyhow, the results with fixed alpha are almost identical in my simple scenario:\n\n\n\nlearning with fixed alpha"
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#what-do-the-above-screenshot-say",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#what-do-the-above-screenshot-say",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "Well, we‚Äôve seen there are:\n\nMore wall cells (16) than empty cells (5)\nmore empty cells (5) than enemy, food and ‚Äúwalk-back‚Äù cell (of which there is one)\n\nAnd yet, after some training, the agent averages, over 1000 most recent steps taken:\n\n275 food eaten (most rewarding)\n3 enemy encounters (most penalized)\nless than 100 wall bumps (heavily penalized)\nabout 150 walk-backs (slightly penalized)\nand well, quite a bit of walking around in empty cells\n\nI haven‚Äôt done the math, but in some of the perceived states:\n\nCorner: 5 walls/8 total cells. There are 4 corners. 62.5%\nSide: 3 walls/8. 37.5%\nThe above account for 8 of 9 possible agent positions (exceptuating center cell)\n\nAnd yet, only 10% of the time the agent bumps into a wall on average (and that‚Äôs with a 17% chance of exploratory move!).\nAs the agent has very limited state perception, I believe this is proof right there that it has indeed learnt something.\nI‚Äôd have to run many more numbers (4 moves possible, 9 locations, 5 different possible cells-status per cell, in different combinations‚Ä¶ I‚Äôm not doing that until someone asks me to do a PhD on the topic üòÅ)\nSimilar numbers could demonstrate that there are lots of food-related actions compared to white cell prevalence‚Ä¶\nAnd indeed, supposing the identical probability of encountering food or enemy at any stage, the fact that our agent has chosen the right action when faced with both possibilities, choosing food 275 times for only 3 times the enemy (again, exploration step 1 in 6 in all described statistics above), well‚Ä¶\nI‚Äôm sold."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#conclusions",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#conclusions",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "I‚Äôm very happy about it all. The Reinforcement Learning example I chose is working nicely! And it makes sense, based on the little theory I have studied thus far.\n(Note: I shall actually post a short entry on real-world application of data mining with my LCS implementation. Any day now, it has happened already‚Ä¶)."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#resources",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#resources",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "‚ÄúReinforcement Learning, An introduction.‚Äù, 2nd Ed. Bradford Books, by R. S. Sutton & A. G. Bardo."
  },
  {
    "objectID": "posts/2024-11-08_welcome/index.html",
    "href": "posts/2024-11-08_welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in this future Blog. Currently testing all kinds of things, will updated hopefully shortly!\nSeems like it will nicely manage some defaults.\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2024-11-08_welcome/index.html#references-for-future-use",
    "href": "posts/2024-11-08_welcome/index.html#references-for-future-use",
    "title": "Welcome To My Blog",
    "section": "References for future use",
    "text": "References for future use\nFor now, I need to keep references of what allowed me to get here:\n\nManage GitHub access\nhttps://usethis.r-lib.org/articles/git-credentials.html\n\n\nSet things up\nhttps://sites.northwestern.edu/researchcomputing/2022/05/11/git-with-rstudio-order-matters/\nhttps://sites.northwestern.edu/researchcomputing/resources/using-git-and-github-with-r-rstudio/\nhttps://ucsb-meds.github.io/creating-quarto-websites/"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html",
    "href": "posts/2025-03-19_ExplainableAI/index.html",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "I have been open about some of my worries with GenAI (and in fact Deep Learning with Neural Networks).\nOne of these worries had to do with the fact that DeepLearning outputs, while very precise in the immense majority of cases, are never easily interpreted. In other words, one cannot easily understand why a (deep) neural network (in the supervised learning case, for instance) makes the choices it makes.\nThey say ‚ÄúIf you talk the talk‚Ä¶ Walk the walk‚Äù. Well, here goes my own personal attempt at just that.\n\n\n\nI‚Äôve shown in the past, how for data mining the rules themselves are showing how a classifier (separately, each of the classifier systems) actually ‚Äúshows‚Äù where to put our attention (see https://kaizen-r.github.io/posts/2025-01-25_RLCS_4_DataMining/ for instance).\nWell, today we‚Äôll see how using LCS outcome can be used itself directly to help interpret the algorithm‚Äôs own choices.\nAnd how that can be important.\nAnd to be perfectly clear, this was one of the key reasons why I got interested in LCS as a family of algorithms in the first place!\n\n\n\nIf you use a neural network trained to classify numbers, say on MNIST dataset, it will work. Most assuredly!\nAnd training it can be faster than with the LCS (although, that is‚Ä¶ not that clear. More on that some other time).\nBut more importantly: although the neural network output will work, you will have a hard time understanding why it chose one class or another. (At least, without SHAP, LIME, etc.)\nWell, I present to you, interpreting LCS output as images classifier:\n\n\n\nLCS are expressive in their choices!\n\n\nA few things happen here: First, I take MNIST, but as explained in the past, to contain the processing needs, I compress the input image first (that‚Äôs not part of the LCS algorithm üòÅ). Once I‚Äôm down to a 7x7 binary image, I use that as my state and I train the LCS with an environment of several such ‚Äústates‚Äù (i.e.¬†compressed pictures).\nDifferent from a Neural Network, I do not need to train on that many examples (also already clarified in the past). So I train on only 500 samples, chosen randomly, and here I train for quite some time (almost 7 minutes, but single-thread, mind you) on said 500 samples.\nAs an output, I get a set of a few thousand ‚Äúclassifiers‚Äù (&lt; 2500 in today‚Äôs example), all of which I have presented in the past also and won‚Äôt explain here again. Together, they form the System of classifiers.\nBut just for reference, I got a 99% correctly classified testing samples (out of 3799 of them, none of which had been seen for training).\n\n\n\nQuality of generated classifier\n\n\nNow on to the explainable part!\n\n\n\nNow tell me: Does the following set of visuals help in actually interpreting the LCS recommendation? (hint: I think it does!)\nI‚Äôve been trying to visualize why the LCS choices are indeed explainable. Of course, matching several rules makes it more confusing‚Ä¶ But each rule that matches is a vote. If you can visualize the ‚Äúensemble‚Äù of rules/classifiers, all of which must have matched the sample, in our case hopefully you can see what the LCS is using as subset of reference for a given image.\nHere an example, whereby a Zero image is correctly classified, among other reasons because it needs to have ‚Äúlines around‚Äù and one ‚Äúempty pixel‚Äù in the center! That‚Äôs enough for the LCS to decide that was a zero, and probably very few of the 156 rules that matched that test sample (all of which agreed on the correct class) would have been sufficient to explain the choice!\nIn other words, I try to show the ensemble and its choice in such a way that you see why the LCS decided on class 0 for that sample.\nGood luck doing that with a Neural network!\n\n\n\nClass 0 - Visualized\n\n\n\n\n\nExplainable AI is important for many reasons.\nBecause of the very nature of ML (in that sense different from other approaches), sometimes (as rarely as it might be, 1% of the cases in today‚Äôs exercise) it makes a classification mistake. There are several possible reasons for that, but often it has to do with problems with generalization (overfitting).\nRemember we used 500 samples for training, and have tested with 99% correct results on 3800 test samples!\n(Note that part of the trick here is that my compressing the original images has removed more subtle differences that might have otherwise required better generalization‚Ä¶ Fair enough!)\nBut I do hope in the exercise for today, the visualization of the LCS choices is helpful and indeed explainable.\nAt the very least, I feel it‚Äôs important to know, when an error is detected.\n\n\n\nResistance to Adversarial attacks!\nOne important issue with some CNNs for instance for images classification is that they are sensible to a type of attack whereby someone can add noise to a picture, invisible to the human eye, and force the classifier to err the classification.\nThis will not happen with LCS, as I hope the visuals I introduced today clearly show. And that‚Äôs pretty cool, too.\n\n\n\nFor our exercise today, a few (1%) of the examples were wrongly classified, and in all cases the confidence of the proposed classification was below 65%.\nAs the LCS output is a ‚Äúsystem of classifiers‚Äù, whereby a new sample matches several ‚Äúrules‚Äù, and given in our exercise the high level of compression with loss of information, it was no surprise that some examples were difficult based on the compressed image (I wouldn‚Äôt have been able to do any better myself, without the original image).\n\n\n\nLook at the compressed sample used, you‚Äôll understand the confusion of the LCS\n\n\nIn summary, they matched several classifiers, some of which were voting for one class, while the rest for the other‚Ä¶ Which is reflected in the low confidence of the outcome!\nAnd overall, my code for LCS did a great job, given what they see as (compressed) input.\n\n\n\n\n\n\nClass 0 - other example\n\n\nIt doesn‚Äôt always happen, but here again a central empty cell is quite self-explanatory. Moreover, one can tell the line needs to spread horizontally for the set of classifiers to agree on a Zero class, here. (That happens more often indeed)\n\n\n\nClass 1 - an example\n\n\nYou can see in the above, how the width of the line is constrained by the ‚Äúempty cells‚Äù recommendations (‚ÄúPixel=0‚Äù). Clearly the line for the one is apparent (‚ÄúPixel=1‚Äù). And the resulting visual shows (green) empty cells surrounding (maroon) full cells, indeed.\n\n\n\nLast example for today\n\n\n\n\n\nLook, I‚Äôm not kidding myself. I will not magically shift the focus of the whole AI community towards the LCS algorithm, overnight.\nPlus, it‚Äôs not a perfect algorithm. Many hyper-parameters to consider, not particularly fast (although‚Ä¶ to be discussed)‚Ä¶\nBut it does work, requires (at least in today‚Äôs example) relatively few (ideally well chosen) samples for training, and is clear in why it makes the decisions it makes, which as explained is (at least from my perspective) very important.\nI do hope, however, that maybe it shows there are alternatives to neural networks, and if nothing else, I would expect this exercise might be considered useful, as a complementary approach: Keep using your neural networks, deep learning and what-not, but maybe you can use an LCS on the side, to confirm (with a level of confidence included, and visuals that express the why) the neural net choices‚Ä¶\nAnd although I still have a lot to do, I hope when it‚Äôs ready, my RLCS package will participate in making that a reality :)"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#foreword",
    "href": "posts/2025-03-19_ExplainableAI/index.html#foreword",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "I have been open about some of my worries with GenAI (and in fact Deep Learning with Neural Networks).\nOne of these worries had to do with the fact that DeepLearning outputs, while very precise in the immense majority of cases, are never easily interpreted. In other words, one cannot easily understand why a (deep) neural network (in the supervised learning case, for instance) makes the choices it makes.\nThey say ‚ÄúIf you talk the talk‚Ä¶ Walk the walk‚Äù. Well, here goes my own personal attempt at just that."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#lcs-outputs-can-be-interpreted",
    "href": "posts/2025-03-19_ExplainableAI/index.html#lcs-outputs-can-be-interpreted",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "I‚Äôve shown in the past, how for data mining the rules themselves are showing how a classifier (separately, each of the classifier systems) actually ‚Äúshows‚Äù where to put our attention (see https://kaizen-r.github.io/posts/2025-01-25_RLCS_4_DataMining/ for instance).\nWell, today we‚Äôll see how using LCS outcome can be used itself directly to help interpret the algorithm‚Äôs own choices.\nAnd how that can be important.\nAnd to be perfectly clear, this was one of the key reasons why I got interested in LCS as a family of algorithms in the first place!"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#explainable-ai",
    "href": "posts/2025-03-19_ExplainableAI/index.html#explainable-ai",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "If you use a neural network trained to classify numbers, say on MNIST dataset, it will work. Most assuredly!\nAnd training it can be faster than with the LCS (although, that is‚Ä¶ not that clear. More on that some other time).\nBut more importantly: although the neural network output will work, you will have a hard time understanding why it chose one class or another. (At least, without SHAP, LIME, etc.)\nWell, I present to you, interpreting LCS output as images classifier:\n\n\n\nLCS are expressive in their choices!\n\n\nA few things happen here: First, I take MNIST, but as explained in the past, to contain the processing needs, I compress the input image first (that‚Äôs not part of the LCS algorithm üòÅ). Once I‚Äôm down to a 7x7 binary image, I use that as my state and I train the LCS with an environment of several such ‚Äústates‚Äù (i.e.¬†compressed pictures).\nDifferent from a Neural Network, I do not need to train on that many examples (also already clarified in the past). So I train on only 500 samples, chosen randomly, and here I train for quite some time (almost 7 minutes, but single-thread, mind you) on said 500 samples.\nAs an output, I get a set of a few thousand ‚Äúclassifiers‚Äù (&lt; 2500 in today‚Äôs example), all of which I have presented in the past also and won‚Äôt explain here again. Together, they form the System of classifiers.\nBut just for reference, I got a 99% correctly classified testing samples (out of 3799 of them, none of which had been seen for training).\n\n\n\nQuality of generated classifier\n\n\nNow on to the explainable part!"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#the-visuals",
    "href": "posts/2025-03-19_ExplainableAI/index.html#the-visuals",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Now tell me: Does the following set of visuals help in actually interpreting the LCS recommendation? (hint: I think it does!)\nI‚Äôve been trying to visualize why the LCS choices are indeed explainable. Of course, matching several rules makes it more confusing‚Ä¶ But each rule that matches is a vote. If you can visualize the ‚Äúensemble‚Äù of rules/classifiers, all of which must have matched the sample, in our case hopefully you can see what the LCS is using as subset of reference for a given image.\nHere an example, whereby a Zero image is correctly classified, among other reasons because it needs to have ‚Äúlines around‚Äù and one ‚Äúempty pixel‚Äù in the center! That‚Äôs enough for the LCS to decide that was a zero, and probably very few of the 156 rules that matched that test sample (all of which agreed on the correct class) would have been sufficient to explain the choice!\nIn other words, I try to show the ensemble and its choice in such a way that you see why the LCS decided on class 0 for that sample.\nGood luck doing that with a Neural network!\n\n\n\nClass 0 - Visualized"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#why-is-this-good-12",
    "href": "posts/2025-03-19_ExplainableAI/index.html#why-is-this-good-12",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Explainable AI is important for many reasons.\nBecause of the very nature of ML (in that sense different from other approaches), sometimes (as rarely as it might be, 1% of the cases in today‚Äôs exercise) it makes a classification mistake. There are several possible reasons for that, but often it has to do with problems with generalization (overfitting).\nRemember we used 500 samples for training, and have tested with 99% correct results on 3800 test samples!\n(Note that part of the trick here is that my compressing the original images has removed more subtle differences that might have otherwise required better generalization‚Ä¶ Fair enough!)\nBut I do hope in the exercise for today, the visualization of the LCS choices is helpful and indeed explainable.\nAt the very least, I feel it‚Äôs important to know, when an error is detected."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#why-is-it-good-22",
    "href": "posts/2025-03-19_ExplainableAI/index.html#why-is-it-good-22",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Resistance to Adversarial attacks!\nOne important issue with some CNNs for instance for images classification is that they are sensible to a type of attack whereby someone can add noise to a picture, invisible to the human eye, and force the classifier to err the classification.\nThis will not happen with LCS, as I hope the visuals I introduced today clearly show. And that‚Äôs pretty cool, too."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#locating-errors",
    "href": "posts/2025-03-19_ExplainableAI/index.html#locating-errors",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "For our exercise today, a few (1%) of the examples were wrongly classified, and in all cases the confidence of the proposed classification was below 65%.\nAs the LCS output is a ‚Äúsystem of classifiers‚Äù, whereby a new sample matches several ‚Äúrules‚Äù, and given in our exercise the high level of compression with loss of information, it was no surprise that some examples were difficult based on the compressed image (I wouldn‚Äôt have been able to do any better myself, without the original image).\n\n\n\nLook at the compressed sample used, you‚Äôll understand the confusion of the LCS\n\n\nIn summary, they matched several classifiers, some of which were voting for one class, while the rest for the other‚Ä¶ Which is reflected in the low confidence of the outcome!\nAnd overall, my code for LCS did a great job, given what they see as (compressed) input."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#a-few-more-examples-for-no-reason",
    "href": "posts/2025-03-19_ExplainableAI/index.html#a-few-more-examples-for-no-reason",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Class 0 - other example\n\n\nIt doesn‚Äôt always happen, but here again a central empty cell is quite self-explanatory. Moreover, one can tell the line needs to spread horizontally for the set of classifiers to agree on a Zero class, here. (That happens more often indeed)\n\n\n\nClass 1 - an example\n\n\nYou can see in the above, how the width of the line is constrained by the ‚Äúempty cells‚Äù recommendations (‚ÄúPixel=0‚Äù). Clearly the line for the one is apparent (‚ÄúPixel=1‚Äù). And the resulting visual shows (green) empty cells surrounding (maroon) full cells, indeed.\n\n\n\nLast example for today"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#conclusions",
    "href": "posts/2025-03-19_ExplainableAI/index.html#conclusions",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Look, I‚Äôm not kidding myself. I will not magically shift the focus of the whole AI community towards the LCS algorithm, overnight.\nPlus, it‚Äôs not a perfect algorithm. Many hyper-parameters to consider, not particularly fast (although‚Ä¶ to be discussed)‚Ä¶\nBut it does work, requires (at least in today‚Äôs example) relatively few (ideally well chosen) samples for training, and is clear in why it makes the decisions it makes, which as explained is (at least from my perspective) very important.\nI do hope, however, that maybe it shows there are alternatives to neural networks, and if nothing else, I would expect this exercise might be considered useful, as a complementary approach: Keep using your neural networks, deep learning and what-not, but maybe you can use an LCS on the side, to confirm (with a level of confidence included, and visuals that express the why) the neural net choices‚Ä¶\nAnd although I still have a lot to do, I hope when it‚Äôs ready, my RLCS package will participate in making that a reality :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kaizen-R.com new home",
    "section": "",
    "text": "RLCS & Explainable AI\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nMar 19, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS for RL: It works again (using books helps)\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nMar 16, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating Contents (slowly)\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 14, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Improving‚Ä¶ And getting worse (at the same time!)\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nMar 9, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Reinforcement Learning: World Explorer v1\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nFeb 16, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: A World to Play RL\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Reinforcement Learning?\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: training in parallel?\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nJan 26, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: for Data Mining\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nJan 25, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 4.5 - Fully Functional v0 and New Tests\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 2 - Full Rule Discovery\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 24, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 1 - Part II: Basic Covering\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 1: Rules formatting, storage, and matching\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nA new project\n\n\n\n\n\n\nML\n\n\nnews\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 8, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Linear Regression\n\n\n\n\n\n\nML\n\n\nmath\n\n\n\n\n\n\n\n\n\nNov 25, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying URLs\n\n\n\n\n\n\nML\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 17, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nEntropy - Identifying compressed files\n\n\n\n\n\n\nmath\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 10, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nML Concepts: Word Embeddings\n\n\n\n\n\n\nML\n\n\nNLP\n\n\n\n\n\n\n\n\n\nNov 2, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Exercises: Interpolation with Lagrange Polynomials\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nOct 20, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nWhile on the Train: Cellular Automata\n\n\n\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nNico\n\n\n\n\n\n\nNo matching items"
  }
]