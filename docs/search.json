[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Today I just started something simple: An Object to represent a simulated World in which an Agent can move.\nAs the “Agent” will need to interact with the World, and the World should return rewards, and understand positions, etc., I decided in this case to go the way of R Objects, more specifically “setRefClass”.\nFor today, I just created a basic object to generate a “random world” that can have walls, food, even enemy(ies) and in which we can throw an agent at random in any empty space.\nEDIT: Added Agent movements and visualizations\nA few peculiarities maybe: I think it will be simpler in the future to not have to deal with “borders” from the Agent perspective, and so I set my Worlds to necessarily be walls all around.\n\n\n\nLet’s start today with a simple video showing the results. The agent is the blue thing. Food is green, enemy is red, black are walls.\nAll is initialized randomly, and so are the moves of the current agent. On the left, I show the agent’s perspective/perception (allocentric) of the World. On the right, the World as it is at each step:\n\nVideos are always more satisfying than matrices, but to understand what happens, let’s have a look under the hood…\n\n\n\nSo in my matrix, I choose 0 = empty cell, 1 is a wall, 2 is food, 3 is an enemy (defaults to no enemies). And 4 will be an agent. This is all arbitrary, but when in the future I encode into binary there will be a specific order, as this will be relevant for the GA to work with.\nAnyhow, it looks like so:\n\n\n\nA simple world object\n\n\nIn this “world”, I needed to add methods to move around (from-to positions, or just “to”), control for ilegal moves (can’t move into a wall), return rewards: good +10 for food, bad -100 if enemy, and empty and walls, well, to be refined.\nAll very simple, but cool at the same time: I can already render “animations” of said worlds… Albeit with a very stupid agent for now…\n\n\n\nWell, I’m afraid now I have most of the pieces. But still:\n\nI will need to implement a variation of my existing RLCS code to account for four possible actions (for now, actions are binary, that’s not enough).\nI will need to add a sensible binary encoding for the agent to interpret its visible environment\nI will need to account for rewards, change the design for the RL case where I need to add prediction and an Action Set\n…\n\nQuite a bit of work still. But all fun.\nTO BE CONTINUED…"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#the-world-should-be-easy-to-interact-with",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#the-world-should-be-easy-to-interact-with",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Today I just started something simple: An Object to represent a simulated World in which an Agent can move.\nAs the “Agent” will need to interact with the World, and the World should return rewards, and understand positions, etc., I decided in this case to go the way of R Objects, more specifically “setRefClass”.\nFor today, I just created a basic object to generate a “random world” that can have walls, food, even enemy(ies) and in which we can throw an agent at random in any empty space.\nEDIT: Added Agent movements and visualizations\nA few peculiarities maybe: I think it will be simpler in the future to not have to deal with “borders” from the Agent perspective, and so I set my Worlds to necessarily be walls all around."
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#a-random-policy-agent-in-a-random-world",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#a-random-policy-agent-in-a-random-world",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Let’s start today with a simple video showing the results. The agent is the blue thing. Food is green, enemy is red, black are walls.\nAll is initialized randomly, and so are the moves of the current agent. On the left, I show the agent’s perspective/perception (allocentric) of the World. On the right, the World as it is at each step:\n\nVideos are always more satisfying than matrices, but to understand what happens, let’s have a look under the hood…"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#todays-results-in-matrix-format",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#todays-results-in-matrix-format",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "So in my matrix, I choose 0 = empty cell, 1 is a wall, 2 is food, 3 is an enemy (defaults to no enemies). And 4 will be an agent. This is all arbitrary, but when in the future I encode into binary there will be a specific order, as this will be relevant for the GA to work with.\nAnyhow, it looks like so:\n\n\n\nA simple world object\n\n\nIn this “world”, I needed to add methods to move around (from-to positions, or just “to”), control for ilegal moves (can’t move into a wall), return rewards: good +10 for food, bad -100 if enemy, and empty and walls, well, to be refined.\nAll very simple, but cool at the same time: I can already render “animations” of said worlds… Albeit with a very stupid agent for now…"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#what-next",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#what-next",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Well, I’m afraid now I have most of the pieces. But still:\n\nI will need to implement a variation of my existing RLCS code to account for four possible actions (for now, actions are binary, that’s not enough).\nI will need to add a sensible binary encoding for the agent to interpret its visible environment\nI will need to account for rewards, change the design for the RL case where I need to add prediction and an Action Set\n…\n\nQuite a bit of work still. But all fun.\nTO BE CONTINUED…"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html",
    "href": "posts/2024-11-02_WordEmbeddings/index.html",
    "title": "ML Concepts: Word Embeddings",
    "section": "",
    "text": "I discussed an unsupervised clustering algorithm, DBScan. We used multi-dimensional points (coordinates) in space. But what if our data is text? That’s common in “general”, but also in Cybersecurity. So the question becomes: Can we treat “text” as points on which to apply such (or similar) algorithms?\nEnters the concept of embedding."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#continuing-on-text-for-cybersecurity-ml",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#continuing-on-text-for-cybersecurity-ml",
    "title": "ML Concepts: Word Embeddings",
    "section": "",
    "text": "I discussed an unsupervised clustering algorithm, DBScan. We used multi-dimensional points (coordinates) in space. But what if our data is text? That’s common in “general”, but also in Cybersecurity. So the question becomes: Can we treat “text” as points on which to apply such (or similar) algorithms?\nEnters the concept of embedding."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#conceptual-understanding",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#conceptual-understanding",
    "title": "ML Concepts: Word Embeddings",
    "section": "Conceptual understanding",
    "text": "Conceptual understanding\nOnce again, who am I to “teach” you what an embedding is, hu? It’s probably better to go to the definition, which hopefully, thanks to the context provided in the last entry, can help intuit where we’re going with all this:\n“In natural language processing, a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning.”\nSo two things: Today is actually about Natural Language Processing, or NLP in short. Not a new topic in this Blog, but hey.\nSecond, we’re looking for a representation of a word as a “real-valued vector”. So think of it like so: A “vector” can represent many things, but today we’re going to consider it a set of coordinates.\nSo in 3 dimensions (3D), a word embedding would represent a word as 3 numbers, representing each a coordinate of the (x, y, z) space. Sounds familiar? Check again the entry on Clustering if not, please, it makes more sense to consider both entries together…\nYou’re back?\nOK. For very large text, maybe the information of a word with only 3 dimensions is not enough to “encode” its relationship to other words. So you would go into higher dimensions, say 15, 30 or 1000 dimensions (again, why not? You and I can’t visualize 30 dimensions in our heads, but it’s easy for a computer… Fun, ain’t it?)\nAnd so with a set of vectors, each representing a word, we get in fact a set of points in an N-dimensional space. And then…\nWhy not use algorithms on these “embeddings”, say the DBScan algorithm?"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#in-practice-embedding-from-texts",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#in-practice-embedding-from-texts",
    "title": "ML Concepts: Word Embeddings",
    "section": "In practice: Embedding from text(s)",
    "text": "In practice: Embedding from text(s)\nWe’re not going to discuss the current algorithms for embeddings in detail. They use… Neural Networks, of all things. The general accepted version of Word2Vec for instance in essence takes pairs of words (out of “training text”) and transforms the “words” into vectors of 300 dimensions (if I remember correctly). Better even, you can get the embeddings, so you need not train anything yourself (thanks Google AI!).\nBut for today, we might just want to go ahead and actually train our own embeddings. Let’s go for it!\nBy the way: Here the code for today.\nNow one important concept for Machine Learning: “Garbage IN? Garbage OUT!” So IF I use cra*py (pardon my french) text as input, I shouldn’t expect much of a result as an output. \nLet’s say I consider the Wikipedia to hold “good” text about some IT and Cybersecurity concepts. If so, I could use for instance the R Package wikipediR. (At least that’s the one I used for today)\nlibrary(WikipediR) # Get Wiki data\n## Simple wrapper\nmy_page_content &lt;- function(keywords) {\n  page_content(language = \"en\",\n    project = \"wikipedia\",\n    page_name = keywords,\n    as_wikitext = FALSE,\n    clean_response = TRUE) |&gt;\n  clean_text_set()\n}\n## Explicitly for explanation:\nfirewall_wiki &lt;- my_page_content(\"firewall (computing)\")\nfirewall_wiki &lt;- firewall_wiki[1:82]\nswitch_wiki &lt;- my_page_content(\"network switch\")\nswitch_wiki &lt;- switch_wiki[2:96]\nAnd it goes on, with a few other keywords of interest (say “router”, “hacker”…). You have the details of this example in the code.\nWhy I filter and keep only 82 paragraphs of the Firewall entry? Just a matter of cleaner stuff, the way I parse the Wiki entries (which is detailed in “clean_text_set()” function), some lines contain a bit of only pairs of words, others a few references with proper Names, etc.  Consider this a manual process in this case because I’m lazy. In the real world, I would look for the key words that mark the section of the Wiki Entry that do not interest me, and keep the ones above that only. And clean those. Like it or not, in this case, I needed to extract meaningful sentences of some HTML pages. It requires a bit of work (my “cleant_text_set()” function, created for today’s exercise specifically, hopefully can show how one has to work, it’s not always as simple as running a function call…). All in all, after pre-processing, I’ll end up with 692 sentences. In traditional ML, a good part of the work is about getting the right data in the right format. And that’s all I say about that today. Moving on.\nThe next step will be to use our data. Here I’m not going to implement anything myself, it’s beyond my point. Suffice to say I’m going to use the “Continuous Bag of Words”, that looks at words AROUND one word to assign positions in space. In concept and simplifying a lot, we’re seeing if “block” and “firewall” appear in the training text near one another more often than “restaurant” and “firewall”. (I’d guess that’s about right :D)\nNow we can use R to train our own Word2Vec Neural Network, with Bag Of Words, on our sample text (692 small blocks of text) and ask it to come up with vectors of 30 dimensions as embeddings for the main keywords found in the text.\n## Lets' move on to something more... Substantial:\nfull_text &lt;- c(\n  switch_wiki,\n  router_wiki,\n  firewall_wiki,\n  hacker_wiki,\n  computer_wiki,\n  cpu_wiki,\n  virus_wiki\n)\n\nmodel &lt;- word2vec(full_text, type=\"cbow\", dim=30, iter = 50)\nembeddings &lt;- as.matrix(model)\nembeddings\nAnd yes, it requires a few things (the “word2vec” R package, for one), a bit of understanding (or trial and error, but understanding is better!). But if all goes as planned you’ll get something like this:\n\n\n\nSo 970 words have been transformed into their corresponding 30-dimensional vectors! Good!\n30 dimensions is going to be hard to “look at”, but let’s do it anyway. What we really want is to understand the similarity of things here. So for instance, FROM OUR SELECTED WIKI text (very small sample, if you ask me, and yet…):\n\nNow I wouldn’t necessarily agree that “pix” is the best nearest word for “firewall” but… That’s what our sample text says, it would seem. Anyhow, it doesn’t sound completely crazy either. (e.g. “Restaurant”, had it been in the sample text, hopefully wouldn’t appear in the top 5 “nearest” terms for Firewall…)\n30 dimensions is going to be hard to “look at”, but let’s do it anyway. What we really want is to understand the similarity of things here. So for instance, FROM OUR SELECTED WIKI text (very small sample, if you ask me, and yet…):\nNow I wouldn’t necessarily agree that “pix” is the best nearest word for “firewall” but… That’s what our sample text says, it would seem. Anyhow, it doesn’t sound completely crazy either. (e.g. “Restaurant”, had it been in the sample text, hopefully wouldn’t appear in the top 5 “nearest” terms for Firewall…)"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#going-2d-and-hint-for-the-future",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#going-2d-and-hint-for-the-future",
    "title": "ML Concepts: Word Embeddings",
    "section": "Going 2D and hint for the future",
    "text": "Going 2D and hint for the future\nI’m going to finish this with one visualization, and then hopefully everything will come together. Now I’ll say this first: I know you can bring 30 dimensions into 2, yes. I was going to try “multi-dimensional scaling” (as PCA is probably too lossy for such a reduction), or look into some algorithm for that. But then I came across examples here, and heck, dimensionality reduction was beyond the point for today, and so I skipped doing it myself. (To this day, I haven’t looked at how the umap() function works. I know, shame on me.)\nBut here is the key of all the conversation for today:\n\n\n\nProjecting Embeddings onto 2D plot\n\n\nWe’ve done it! We have visualized our words, not without first creating embeddings for them, and then projecting into 2 Dimensions.\nAnd let’s have a look at what is where…\nNot bad, “firewalls” is near “firewall” (pfiu!). So are filter, traffic… And maybe the rest is not great, but that’s what we came up with from (again) very little sample text."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#mixing-things-up",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#mixing-things-up",
    "title": "ML Concepts: Word Embeddings",
    "section": "Mixing things up!",
    "text": "Mixing things up!\nLast week, I published a (simplistic) entry about DBScan as an algorithm to cluster things. WHY NOT apply that here?!\n\nCould we have a look at one cluster, maybe one that contains the word “Virus”?\n\nThings like “infection”, “executable”, “scanner”, “malicious” are all in the area… And with the same color!\nWith more text and better cleanup… I’m convinced the approach has its merits 🙂\nAs per 3D visuals, this time using Multi-dimensional Scaling (another algorithm that uses distances!) to project onto 3 dimensions, well it works, but there is a bit too much data, and maybe it’s not super super useful… Still:"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#applications",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#applications",
    "title": "ML Concepts: Word Embeddings",
    "section": "Applications",
    "text": "Applications\nWhat if instead of Wiki entries from the internet, we had taken CVE text (maybe along with their CVSS (or whatever scoring system you prefer))? We could probably do some sort of regression once we encode the text (maybe even mixing things up with other algorithms) and maybe estimate the score?\nHow about classifying threat alerts into groups?\nWhat if we had taken logs from a machine. Could we maybe use all this and find specially anomalous logs, from the complete log file? (Imagine thousands of lines reviewed in seconds by your laptop like so…)\nAnd consider this: Over this and the last Blog entry, we’ve discussed enough to do some basic ML. But there is much more than Clustering applied to text. I just hope this helps give a hint of the possibilities. 🙂"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#conclusion",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#conclusion",
    "title": "ML Concepts: Word Embeddings",
    "section": "Conclusion",
    "text": "Conclusion\nAt this stage, I truly hope we understand what it means for us to be able to put words (or texts, they could be each files… why not!) into vectors (i.e. points in N-dimensional space), to be projected (or not) and for which distances can be calculated to other words/texts.\nWith that, we open a world of possibilities: Topic Modelling, Sentiment Analysis, etc. can all be done using distances between points 🙂 (There is more to NLP, though, and NOT only LLMs, please. More simple/traditional stuff is out there! One example I wrote about forever ago was part of speech tagging, for instance. Another time I used TF-IDF to model a classifier of log files with supervised learning…) Maybe in a future post I’ll discuss more of these concepts, but I’d be very happy for now if somehow I helped someone out there understand a bit better how these things could actually work.\nBy the by: GenAI essentially does self-supervised learning on word embeddings. (WOW! What a bomb to leave at the end of a blog entry, “self-supervised”???)."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#bonus-a-word-about-genai",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#bonus-a-word-about-genai",
    "title": "ML Concepts: Word Embeddings",
    "section": "Bonus: A word about GenAI",
    "text": "Bonus: A word about GenAI\nOK, OK, OK… But real quick then.\nFirst: I don’t particularly like GenAI as a topic because – mostly – of the hype, misunderstanding, risks… but otherwise is undoubtedly incredibly powerful and I’ll admit it must have some cool applications… For expert users! And although very slowly, I myself am considering using it as an assistant R-coder… I have done tests and it’s not bad at all, and it WOULD make me much faster… But I’m mostly resisting for now: Coding and thinking how to approach a problem is what I like, so why externalize that… Unless I really HAVE to…\nThat said… What the heck is “self-supervised” learning? \nSupervised learning needs to have a means of knowing whether it’s doing a good job to rectify its own behaviour while in training.\nIf your job is to predict the best next word for a given text… All you need is to try to predict it, and then read the next word (or “token”). If you guessed wrong, you rectify your behaviour for your next guess. Then you read the next word… And iterate. And in the above scenario, nobody needs to “tag” anything, the information is self-contained! So you just ingest text one “token” at a time (don’t worry, say “one word at a time”, and more or less you’re good). All you need is text (and “attention”, but that’s WAY beyond today’s objectives :D).\nThe more text, the more training examples you get 🙂\nAnd yes: Your words/tokens, are presented to your GenAI (well, LLMs, really) as… Embbedings."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "This is quite literally my first Reinforcement Learning exercise ever. Coded from scratch. With no Neural Network whatsoever, but instead, using my own implementation of the Learning Classifier System(s) I have been discussing lately.\nAnd the point is this: It works!\nHere I present the first actual working results.\n\n\n\nSo we have an agent (blue dot) that can move in any of {up, down, right, left} direction.\nWe have a simulated world, with walls (black dots), some rewarding food (green dots) and some hurting enemies/poison (red dots).\nIndeed the aestetics of this demo are not marvelous, but I’ve never cared much about look&feel, I care about what happens under the hood here.\n\n\n\nmonitoring the learning process\n\n\nSo our agent moves, and gets a reward as informed by our world. For instance:\n\nNo reward if moving to an empty cell\nSmall negative reward if moving backwards (to the cell it was in at immediately anterior step)\nsomewhat larger negative reward if “bumping” into a wall\nlarge positive reward if finding food (obviously)\nand large negative reward if moving onto an enemy/poison cell (duh!)\n\nThe trick is: We do NOT tell the agent what to do! It must learn how to behave on its own, just by maximizing its reward.\nAnd here, the agent will use my implementation of a Learning Classifier System. No neural network or deep learning or anything fancy like that :D\nOh, and every line of code is in R.\n\n\n\nSo there is a bit more to it. I will not explain today in detail my whole implementation of an LCS algorithm to do RL. But some of the logic available to the agent is important to understand current results:\n\nI chose to implement a “small memory” for my agent, so that a reward will inform the current step and be “passed onto” the last action, divided by 2.\nThe reason to do that is the following: the agent only sees one cell in all 8 directions (inlcuding diagonals), and it can move in 4 main directions. The idea of the small memory was to allow the machine to learn rules to choose to “move diagonally”. I haven’t validated exactly if this is the result. But it seems the agent does work better with that little bit of “memory”.\n\nI’ll discuss the complete algorithm later on, but today let’s look at the result in a short video!\n\n\n\nHere is a small video I made of the current results…\n\n\n\n\nI am unclear at this point about whether I just implemented a version – or a mix – of TD, Q-Learning, XCS, CS01… That’s the theory part, and I’ll need to dig deeper into that now, to frame better what I just created.\n(Also, the code is very… messy, right now :D I just wanted to see if I could get it to work…)\nBut from an experimental perspective, this was very satisfying already."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#reinforcement-learning-with-lcs",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#reinforcement-learning-with-lcs",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "This is quite literally my first Reinforcement Learning exercise ever. Coded from scratch. With no Neural Network whatsoever, but instead, using my own implementation of the Learning Classifier System(s) I have been discussing lately.\nAnd the point is this: It works!\nHere I present the first actual working results."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#limited-agent-simple-rules",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#limited-agent-simple-rules",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "So we have an agent (blue dot) that can move in any of {up, down, right, left} direction.\nWe have a simulated world, with walls (black dots), some rewarding food (green dots) and some hurting enemies/poison (red dots).\nIndeed the aestetics of this demo are not marvelous, but I’ve never cared much about look&feel, I care about what happens under the hood here.\n\n\n\nmonitoring the learning process\n\n\nSo our agent moves, and gets a reward as informed by our world. For instance:\n\nNo reward if moving to an empty cell\nSmall negative reward if moving backwards (to the cell it was in at immediately anterior step)\nsomewhat larger negative reward if “bumping” into a wall\nlarge positive reward if finding food (obviously)\nand large negative reward if moving onto an enemy/poison cell (duh!)\n\nThe trick is: We do NOT tell the agent what to do! It must learn how to behave on its own, just by maximizing its reward.\nAnd here, the agent will use my implementation of a Learning Classifier System. No neural network or deep learning or anything fancy like that :D\nOh, and every line of code is in R."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#a-few-more-details",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#a-few-more-details",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "So there is a bit more to it. I will not explain today in detail my whole implementation of an LCS algorithm to do RL. But some of the logic available to the agent is important to understand current results:\n\nI chose to implement a “small memory” for my agent, so that a reward will inform the current step and be “passed onto” the last action, divided by 2.\nThe reason to do that is the following: the agent only sees one cell in all 8 directions (inlcuding diagonals), and it can move in 4 main directions. The idea of the small memory was to allow the machine to learn rules to choose to “move diagonally”. I haven’t validated exactly if this is the result. But it seems the agent does work better with that little bit of “memory”.\n\nI’ll discuss the complete algorithm later on, but today let’s look at the result in a short video!"
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#the-result",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#the-result",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "Here is a small video I made of the current results…"
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#conclusions",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#conclusions",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "I am unclear at this point about whether I just implemented a version – or a mix – of TD, Q-Learning, XCS, CS01… That’s the theory part, and I’ll need to dig deeper into that now, to frame better what I just created.\n(Also, the code is very… messy, right now :D I just wanted to see if I could get it to work…)\nBut from an experimental perspective, this was very satisfying already."
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html",
    "title": "RLCS: Improving… And getting worse (at the same time!)",
    "section": "",
    "text": "Well, it’s been a couple of weeks (I was a bit offline last couple of weekends, I guess). So I’m back at it and, first, I needed to clean things up. It just was messy code.\nSo I did that, removed some duplicate code with functions, unused variables, found out some of it was in fact quite wrong…\nHeck, I half-wonder how it actually worked so well a couple of weeks back learning, that simple agent in that simple small “world” moving around looking for “food”…\nI also studied a bit more on RL (although, I have not read as much as I wanted, I had a couple of other books under way… Anyhow), and modified things so that the reward distribution would better follow the ideas of temporal difference (TD).\nNow code is a bit cleaner, probably running smoother, less cluttered…\nBUT!\n\n\n\nAs it turns out, in fixing the code, I made the learning worse… It still kind-of works, but from the last few runs, it… well, it feels it learns worse.\nAnd I’m really wondering which of the many parameters I have changed, on top of the fixes, have made it bad.\n\n\n\nThere is still stuff to clean. I’m not advancing as much as I wanted with setting things up in an actual package yet. And while the Supervised Learning is working fine (maybe even better), the Reinforcement Learning example is worse off after this weekend.\nNo matter, I’ll review again. And I’ll fix it. I’m just slower than I wanted. Oh well…"
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#better-code",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#better-code",
    "title": "RLCS: Improving… And getting worse (at the same time!)",
    "section": "",
    "text": "Well, it’s been a couple of weeks (I was a bit offline last couple of weekends, I guess). So I’m back at it and, first, I needed to clean things up. It just was messy code.\nSo I did that, removed some duplicate code with functions, unused variables, found out some of it was in fact quite wrong…\nHeck, I half-wonder how it actually worked so well a couple of weeks back learning, that simple agent in that simple small “world” moving around looking for “food”…\nI also studied a bit more on RL (although, I have not read as much as I wanted, I had a couple of other books under way… Anyhow), and modified things so that the reward distribution would better follow the ideas of temporal difference (TD).\nNow code is a bit cleaner, probably running smoother, less cluttered…\nBUT!"
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#worse-results",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#worse-results",
    "title": "RLCS: Improving… And getting worse (at the same time!)",
    "section": "",
    "text": "As it turns out, in fixing the code, I made the learning worse… It still kind-of works, but from the last few runs, it… well, it feels it learns worse.\nAnd I’m really wondering which of the many parameters I have changed, on top of the fixes, have made it bad."
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#conclusions",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#conclusions",
    "title": "RLCS: Improving… And getting worse (at the same time!)",
    "section": "",
    "text": "There is still stuff to clean. I’m not advancing as much as I wanted with setting things up in an actual package yet. And while the Supervised Learning is working fine (maybe even better), the Reinforcement Learning example is worse off after this weekend.\nNo matter, I’ll review again. And I’ll fix it. I’m just slower than I wanted. Oh well…"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "So I wasn’t kidding. I really like this “Learning Classifier System” (aka LCS) idea, and I really want to develop the R package for it, which I will call “RLCS” (clever, hu?).\nNow I have to start somewhere, and so I will keep it simple for now. I will first focus on an LCS that does supervised learning only (that means, no reinforcement learning for now.)\nAlso, I will focus on the simplest binary environments. In LCS lingo, for supervised classification, environment is equivalent to saying dataset. Each entry to classify is presented as binary strings, and the class (aka action) is, for simplicity, also a simple 0 or 1.\nSo binary classification of binary strings.\nThis might sound simplistic (and it is somewhat, yes) but the feature engineering and codificatioin into GA-compatible individuals is often harder than the actual training of a ML algorithm.\nAnyhow. I will forego some details for now, but suffice to say I’m working right now towards a Michigan-style LCS.\n\n\n\nNow we will receive an environment/dataset as input that has “states” in binary format, and actions/classes as either 0 or 1.\n## Suppose test data is:\ntest_environment &lt;- data.frame(state = c('00011', '10010', '01110', '00001'),\n                               class = c(0, 0, 0, 1))\n(It’s not enough info to conclude that, but these examples all agree with a classifier that would say, in human readable format: “The Class is the negated bit 4 of the input”.)\nOne of the key aspects of an LCS, is that it will produce rules. A rule is a pair of a condition and an action.\nA condition in turn is written (in our current approach) as a string of a ternary alphabet, namely characters from the set {0, 1, #}, where # means wildcard and matches either 0 or 1.\nNow for each input of the environment (each entry of the dataset), for today’s purposes, we will want to know which rules it matches (regardless of the action). In other words, we want to match an input to condition(s). That will tell us the “Match set”.\nMatching is one key aspect of the LCS overall algorithm, one that will be run a lot, and therefore I worry that it could be slow.\nA couple more keywords: A classifier is a rule and its statistics. (We will skip the statistics part for today.) And a population is our set of classifiers.\nShort of going the way of C++, I have coded one first simplistic function that takes a classifier set as a dataframe, and another one that uses a list of lists instead.\nConditions can easily be written as strings:\nconditions &lt;- c('#001#', '10#10', '##110', '###0#')\nactions &lt;- c(0, 0, 0, 1)\nFor the first approach, it was quite straightforward:\nlcs_classifier1 &lt;- data.frame(id = 1:4,\n                      condition_string = conditions,\n                      action = actions, #simplest is binary for supervised\n                      accuracy = 1,\n                      numerosity = 1,\n                      first_seen = c(1, 2, 3, 4) ## I'm inventing a coverage trigger\n                      )\n\n## Find Matching Rules\nget_match_set_v001 &lt;- function(ti_condition, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- unlist(strsplit(ti_condition, \"\"))\n    pop &lt;- pop[, c(\"id\", \"condition_string\")] # Same\n\n    which(sapply(1:nrow(pop), function(i) {\n\n        t_cond &lt;- unlist(strsplit(pop$condition_string[i], \"\"))\n        relevant_positions &lt;- which(t_cond %in% c(\"0\", \"1\"))\n        \n        all(t_cond[relevant_positions] == ti_cond[relevant_positions])\n    }))\n}\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v001(test_environment$state[1], lcs_classifier1), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v001(test_environment$state[2], lcs_classifier1),\n        c(1, 2)\n    )\n})\n\n(OUTPUT:) Test passed 🥳\nFor the second approach, I needed some more supporting functions:\n######\n## Option 2: Use list instead of Data.frame and pre-calculate match sets\n\n## Function to create a new rule from a condition string and an action\n## THIS IS VALID FOR CLASSIFICATION, NOT FOR Reinforcement Learning!!!\n## In our case, a new rule has: A condition string, an (correct!) action\n## Since action is correct, this is useful for future \"covering\"\n## AND we set accuracy to 1\nnew_supervised_rule &lt;- function(condition_string, action) {\n    list(id = 1,\n         condition_string = condition_string,\n         condition_length = nchar(condition_string),\n         condition_list = list(\"0\" = which(unlist(strsplit(condition_string, \"\")) == \"0\"),\n                               \"1\" = which(unlist(strsplit(condition_string, \"\")) == \"1\")),\n         action = action,\n         accuracy = 1,\n         numerosity = 1,\n         first_seen = 1)\n}\n\n## Function to return max id from a classifier set in list format\nmax_id_in_classifier &lt;- function(pop) {\n    max(sapply(pop, function(x) x$id))\n}\n\n## Function to add rule to a population.\n## date_rule_born will be useful stat for future, setting as parameter for now.\nadd_valid_rule_to_pop &lt;- function(pop, condition_string, action, date_rule_born = 1) {\n    \n    t_rule &lt;- new_supervised_rule(condition_string, action)\n    t_rule$id &lt;- max_id_in_classifier(pop)+1\n    t_rule$first_seen &lt;- date_rule_born\n    \n    pop[[length(pop)+1]] &lt;- t_rule\n    \n    pop\n}\n\n## Support function for human-compatible printing:\nmake_pop_printable &lt;- function(classifier) {\n    data.frame\n    rbind.fill(lapply(1:length(classifier), function(i) {\n        t_c &lt;- classifier[[i]]\n        data.frame(id = t_c$id,\n                   condition = t_c$condition_string,\n                   action = t_c$action,\n                   accuracy = t_c$accuracy,\n                   numerosity = t_c$numerosity,\n                   first_seen = t_c$first_seen)\n    }))\n}\n\n## Matching with list formatted population.\nget_match_set_v002 &lt;- function(input_state, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- as.integer(unlist(strsplit(input_state, \"\")))\n    \n    which(sapply(1:length(pop), function(i) {\n        rule &lt;- pop[[i]]$condition_list\n        all(ti_cond[rule$'0'] == 0, ti_cond[rule$'1'] == 1)\n    }))\n}\n\nlcs_classifier2 &lt;- list(new_supervised_rule(conditions[1], actions[1]))\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[2], actions[2])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[3], actions[3])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[4], actions[4])\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v002(test_environment$state[1], lcs_classifier2), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v002(test_environment$state[2], lcs_classifier2),\n        c(1, 2)\n    )\n})\n\n\n\nAn example population\n\n\n\n\n\nmicrobenchmark(\n    sapply(test_environment$state, function(x) { get_match_set_v001(x, lcs_classifier1)}),\n    sapply(test_environment$state, function(x) { get_match_set_v002(x, lcs_classifier2)}),\n    times = 10000L\n)\n\n\n\nlists and pre-calculated match positions is faster\n\n\nLists are more flexible and data.frames are not the fastest option out there anyway. Here the pre-calculations in the second format also helps doing faster matching.\nHowever I know this choice might have an impact, particularly when time will come to implement deletion, subsumption and compaction (all useful for better LCS performance/value). And another idea I have in mind will require further lists management (but it’s too early to discuss that).\nBut that’s a problem for future me (and nothing that hasn’t been solved already).\n\n\n\nWell well: Day 1, and we have covered concepts of environment, states, actions, conditions, rules, classifiers, population and matching/match set.\nAlong with developping a functional (basic) list for storing classifiers, and a matching function.\nNot too bad (albeit all too simplistic and not going for “full-power” flexible LCS implementation).\n\n\n\nI’ll keep referring to this video (although the book is better to get acquainted with the vocabulary and concepts): intro explaining the algorithm ."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#intro",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#intro",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "So I wasn’t kidding. I really like this “Learning Classifier System” (aka LCS) idea, and I really want to develop the R package for it, which I will call “RLCS” (clever, hu?).\nNow I have to start somewhere, and so I will keep it simple for now. I will first focus on an LCS that does supervised learning only (that means, no reinforcement learning for now.)\nAlso, I will focus on the simplest binary environments. In LCS lingo, for supervised classification, environment is equivalent to saying dataset. Each entry to classify is presented as binary strings, and the class (aka action) is, for simplicity, also a simple 0 or 1.\nSo binary classification of binary strings.\nThis might sound simplistic (and it is somewhat, yes) but the feature engineering and codificatioin into GA-compatible individuals is often harder than the actual training of a ML algorithm.\nAnyhow. I will forego some details for now, but suffice to say I’m working right now towards a Michigan-style LCS."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#the-ternary-alphabet-for-conditions",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#the-ternary-alphabet-for-conditions",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "Now we will receive an environment/dataset as input that has “states” in binary format, and actions/classes as either 0 or 1.\n## Suppose test data is:\ntest_environment &lt;- data.frame(state = c('00011', '10010', '01110', '00001'),\n                               class = c(0, 0, 0, 1))\n(It’s not enough info to conclude that, but these examples all agree with a classifier that would say, in human readable format: “The Class is the negated bit 4 of the input”.)\nOne of the key aspects of an LCS, is that it will produce rules. A rule is a pair of a condition and an action.\nA condition in turn is written (in our current approach) as a string of a ternary alphabet, namely characters from the set {0, 1, #}, where # means wildcard and matches either 0 or 1.\nNow for each input of the environment (each entry of the dataset), for today’s purposes, we will want to know which rules it matches (regardless of the action). In other words, we want to match an input to condition(s). That will tell us the “Match set”.\nMatching is one key aspect of the LCS overall algorithm, one that will be run a lot, and therefore I worry that it could be slow.\nA couple more keywords: A classifier is a rule and its statistics. (We will skip the statistics part for today.) And a population is our set of classifiers.\nShort of going the way of C++, I have coded one first simplistic function that takes a classifier set as a dataframe, and another one that uses a list of lists instead.\nConditions can easily be written as strings:\nconditions &lt;- c('#001#', '10#10', '##110', '###0#')\nactions &lt;- c(0, 0, 0, 1)\nFor the first approach, it was quite straightforward:\nlcs_classifier1 &lt;- data.frame(id = 1:4,\n                      condition_string = conditions,\n                      action = actions, #simplest is binary for supervised\n                      accuracy = 1,\n                      numerosity = 1,\n                      first_seen = c(1, 2, 3, 4) ## I'm inventing a coverage trigger\n                      )\n\n## Find Matching Rules\nget_match_set_v001 &lt;- function(ti_condition, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- unlist(strsplit(ti_condition, \"\"))\n    pop &lt;- pop[, c(\"id\", \"condition_string\")] # Same\n\n    which(sapply(1:nrow(pop), function(i) {\n\n        t_cond &lt;- unlist(strsplit(pop$condition_string[i], \"\"))\n        relevant_positions &lt;- which(t_cond %in% c(\"0\", \"1\"))\n        \n        all(t_cond[relevant_positions] == ti_cond[relevant_positions])\n    }))\n}\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v001(test_environment$state[1], lcs_classifier1), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v001(test_environment$state[2], lcs_classifier1),\n        c(1, 2)\n    )\n})\n\n(OUTPUT:) Test passed 🥳\nFor the second approach, I needed some more supporting functions:\n######\n## Option 2: Use list instead of Data.frame and pre-calculate match sets\n\n## Function to create a new rule from a condition string and an action\n## THIS IS VALID FOR CLASSIFICATION, NOT FOR Reinforcement Learning!!!\n## In our case, a new rule has: A condition string, an (correct!) action\n## Since action is correct, this is useful for future \"covering\"\n## AND we set accuracy to 1\nnew_supervised_rule &lt;- function(condition_string, action) {\n    list(id = 1,\n         condition_string = condition_string,\n         condition_length = nchar(condition_string),\n         condition_list = list(\"0\" = which(unlist(strsplit(condition_string, \"\")) == \"0\"),\n                               \"1\" = which(unlist(strsplit(condition_string, \"\")) == \"1\")),\n         action = action,\n         accuracy = 1,\n         numerosity = 1,\n         first_seen = 1)\n}\n\n## Function to return max id from a classifier set in list format\nmax_id_in_classifier &lt;- function(pop) {\n    max(sapply(pop, function(x) x$id))\n}\n\n## Function to add rule to a population.\n## date_rule_born will be useful stat for future, setting as parameter for now.\nadd_valid_rule_to_pop &lt;- function(pop, condition_string, action, date_rule_born = 1) {\n    \n    t_rule &lt;- new_supervised_rule(condition_string, action)\n    t_rule$id &lt;- max_id_in_classifier(pop)+1\n    t_rule$first_seen &lt;- date_rule_born\n    \n    pop[[length(pop)+1]] &lt;- t_rule\n    \n    pop\n}\n\n## Support function for human-compatible printing:\nmake_pop_printable &lt;- function(classifier) {\n    data.frame\n    rbind.fill(lapply(1:length(classifier), function(i) {\n        t_c &lt;- classifier[[i]]\n        data.frame(id = t_c$id,\n                   condition = t_c$condition_string,\n                   action = t_c$action,\n                   accuracy = t_c$accuracy,\n                   numerosity = t_c$numerosity,\n                   first_seen = t_c$first_seen)\n    }))\n}\n\n## Matching with list formatted population.\nget_match_set_v002 &lt;- function(input_state, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- as.integer(unlist(strsplit(input_state, \"\")))\n    \n    which(sapply(1:length(pop), function(i) {\n        rule &lt;- pop[[i]]$condition_list\n        all(ti_cond[rule$'0'] == 0, ti_cond[rule$'1'] == 1)\n    }))\n}\n\nlcs_classifier2 &lt;- list(new_supervised_rule(conditions[1], actions[1]))\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[2], actions[2])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[3], actions[3])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[4], actions[4])\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v002(test_environment$state[1], lcs_classifier2), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v002(test_environment$state[2], lcs_classifier2),\n        c(1, 2)\n    )\n})\n\n\n\nAn example population"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#well-keep-the-list-format-for-now",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#well-keep-the-list-format-for-now",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "microbenchmark(\n    sapply(test_environment$state, function(x) { get_match_set_v001(x, lcs_classifier1)}),\n    sapply(test_environment$state, function(x) { get_match_set_v002(x, lcs_classifier2)}),\n    times = 10000L\n)\n\n\n\nlists and pre-calculated match positions is faster\n\n\nLists are more flexible and data.frames are not the fastest option out there anyway. Here the pre-calculations in the second format also helps doing faster matching.\nHowever I know this choice might have an impact, particularly when time will come to implement deletion, subsumption and compaction (all useful for better LCS performance/value). And another idea I have in mind will require further lists management (but it’s too early to discuss that).\nBut that’s a problem for future me (and nothing that hasn’t been solved already)."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#conclusions",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#conclusions",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "Well well: Day 1, and we have covered concepts of environment, states, actions, conditions, rules, classifiers, population and matching/match set.\nAlong with developping a functional (basic) list for storing classifiers, and a matching function.\nNot too bad (albeit all too simplistic and not going for “full-power” flexible LCS implementation)."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#references",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#references",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "I’ll keep referring to this video (although the book is better to get acquainted with the vocabulary and concepts): intro explaining the algorithm ."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "This is probably one of the last entries from my preparation to introduce “Background of ML for Cybersecurity”. This time, it’s really about what should probably have been the first entry of the series. Also, it’s not really put in context of Cybersecurity: I’m just trying to show one needs not be afraid about the math.\nI’m having lots of doubts about this one, too: Can I even use the “Machine Learning” tag? After all, this predates ML by quite a bit. It’s really of the realm of statistics. Then again, the limit of what qualifies as ML and what doesn’t is somewhat blurry (at least to me).\nAnd some of it is very very simple, and maybe shouldn’t warrant writing about it. But I like to write things down, it helps organize my thoughts sometimes, and I believe in the idea that really understanding the basics is helpful to grasp concepts when things get more complicated.\n\n\nIF (and that’s a reasonably big if) you can find a linear relationship between two numerical vectors, then using a linear regression can help “encode” the whole pair of vectors through “just” two numbers: A slope, and an intercept.\nIn effect, you’re “modelling” a relationship - and in doing so, you’re actually compressing information.\nMore to the point, if you have an intuition that such relationship holds true for other points you were not given, then you can interpolate (if the new points are in the range of original vectors - otherwise it’s called “extrapolation”) an expected output from a given input.\nIn other words: If you know (or expect) there is a linear (straight line) relationship between a webserver load and number of concurrently opened sessions, you could estimate a load (say % CPU) from knowing only the number of users currently connected (or the other way around, obviously). You’d expect the load to grow with the number of users.\nThe concept of regression (linear or otherwise) are important in ML. It’s a core part of most models. You doubt? Let me put it this way: Regression is a core part of how training a neural network works. There. Believe me now? (I guess I just justified myself in tagging this entry as Machine Learning, after all.)\n\n\n\nWe’ll start off very easy today. Imagine to begin with you only have two points. So two pairs\\((x_1, y_1), (x_2, y_2)\\) of coordinates then. IF (again, big IF) you somehow expects a straight line could be used to estimate a third \\(y_3\\) from a third \\(x_3\\) coordinate in the range of the two original points… How can you estimate said value of \\(y_3\\).\nWell, it’s basic (elementary? high school? Idk) math, you want to find \\(a, b\\) in the following:\n\\[\ny = ax + b\n\\]\nThere are two numbers to find (a slope, and an intercept), and we have two points. That’s a system of two equations. But instead of just doing the math, let’s think about this from a conceptual perspective.\nYou can probably skip this, as it’s really very basic, but I like to make sure I understand things, not just use the math I can find out there, and so I’m going to explain it nevertheless.\nWhat is a “slope”?\nThink of a bicycle ride, and you’re faced with a 8% upwards (so “positive”) slope; what does that mean? That for every 100 meters you advance horizontally, you should expect to gain 8 meters in elevation. So if you gain 8 units in the y direction, you have moved 100 units in the x direction.\nWith two points: \\(y_2 - y_1 = 8, x_2 - x_1 = 100\\). And you get to the slope by dividing both numbers (which is why we expressed the slope in percentage). So you need then:\n\\[\na = slope = {(y_2 - y_1) \\over (x_2 - x_1)}\n\\]\nNot soooo misterious now, is it? If the slope is downwards, \\(y_2 &lt; y_1\\) and \\(a &lt; 0\\), that’s all.\nAnd what is the “intercept”?\nIf you show lines on a two dimensional graph, you often will see the \\(y\\) and \\(x\\) axis. The “intercept” is simply the point at which our line crosses (“intercepts”) the \\(y\\) axis. Which happens when \\(x = 0\\).\nAnd by now, if we take any of our points, we have the following bits \\(y_1, a, x_1\\) for our equation \\(y = ax + b\\). Can we get to \\(b\\)?\n\\[\nb = y_1 - a x_1\n\\]So that for \\(x = 0\\), we have \\(b\\) as the value of \\(y\\). Which is what we were looking for.\nThis was very very very basic stuff. But hopefully for those that don’t do math in their day-to-day life, or those that use formulas without thinking (or use Excel to estimate these things, if at all), or simply as a refresher… I don’t know. Sorry if that was boring.\n\n## Suppose you have any two points:\nmy_x &lt;- c(1.5, 2.5)\nmy_y &lt;- c(5, 2)\n\n# Line goes through two points\n## By definition of \"slope\":\nmy_line_slope &lt;- (my_y[2] - my_y[1]) / (my_x[2]- my_x[1])\n## Then just replace with one point to get intercept:\nmy_intercept &lt;- my_y[1] - (my_line_slope * my_x[1])\n\ncalculate_resulting_y &lt;- function(x) {\n    my_line_slope * x + my_intercept\n}\n\n## Say then we have any value of x (\"independent variable\"):\nmy_interpolation_x_set &lt;- seq(0, 5, 0.1)\n## In a perfect World, we could then get any dependent value:\nmy_interpolation_y_set &lt;- sapply(my_interpolation_x_set, calculate_resulting_y)\n\nplot(my_interpolation_y_set ~ my_interpolation_x_set, col = \"blue\", type=\"l\")\npoints(my_y ~ my_x, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nIn the above, we took a pretty big leap of faith: We just somehow knew/assumed that two points would represent a linear relationship.\nBut we’d probably feel more confident if we had more points to confirm our expectations first.\nToday we’ll stay with the “Simple Linear Regression”, which is nicely explained in the Wikipedia.\nWe want to find \\(\\alpha, \\beta\\) that best describe not two points but instead estimate a line for several of them, so that we solve \\(y = \\alpha + \\beta x\\) that best estimate all \\(x_i, y_i\\) pairs we’re given (provided they look like a straight line).\nAnd I’ve said “estimate” twice above, as there will be some error, and that’s key here: We essentially try to minimize the error that our estimated \\(\\alpha, \\beta\\) introduce, for each point. Written “fancily”:\n\\[\n\\widehat{\\epsilon}_i = y_i - (\\alpha + \\beta x_i), \\forall i \\in [1..n], n \\in \\mathbb{N}\n\\]\nWhere n is the number of your available points…\n\nMinimizing the error is an important concept in supervised machine learning.\n\nThen using the “Sum of Square” of the residuals, we’re going to try to minimize that.\n\n\n\nI will take the time to dive into this a bit, as it is relevant to understand for other cases. Although the math is better explained here, that approach of minimizing sum of squares is useful in other contexts and so here goes.\nSo today we are trying to “fit” a line to a set of points that are not necessarily exactly on said line (see “the result” below). But where do we start? How do we go about finding one line out of an infinite set of possible lines?\nMore or less, we find the parameters for our line equation that minimizes overall distance from all the points. So we try to minimize the sum of the error of our line from each point. It is a minimization exercise, but sometimes the points will be under the line, and others they’ll be above. To correct for that, we actually minimize the sum of distances, each one squared! That way, all values are positive, and we really try to minimize something.\nThere is another important point hidden right here: The key to understand “Gradient Descent”!\nAs we will see with the “results” below, the errors, when squared, are shaped in a sort of U. But what’s important is that the function behind that (parabolic) shape is that it’s differentiable.\nDifferentiating (that’s calculus) here will mean that we know in which direction (up, down, horizontal) said curve goes for a given point on the x axis. If we know that, we know that if the curve goes upwards, to minimize the thing, we want to go in the opposite direction.\nAnd that’s exactly what “descending the gradient” is all about (although in multivariate settings: A gradient in vector calculus is “the direction and the rate of fastest increase”, so to “descend a gradient”, you want to move in the opposite direction).\nFor linear regression, it’s cool because we can do partial derivatives (as shown on the referenced Wiki page above) to find our \\(\\alpha\\) and \\(\\beta\\) parameters that describe the line that “best fit the data”.\nIt’s quite clever, mind you: You minimize the error of your predicted value compared to the actual value. That’s how your machine can learn! But it’s also what people have been using to train Neural Networks!\nFor today, let’s just store the following information: The “Least Square Minimization” can help find a minimum error through differentiation.\nAnd for a future conversation: Gradient Descent (hidden in today’s Sum of Square Minimization) is one of the two mathematical keys for back-propagation. (And back-propagation was the before and after of connectionism in Machine Learning theory).\nAlright, let’s move on and return to our very simple example.\n\n\n\nRemember how we said at some point that in ML, more data points is often better?\nWe’re going to find the line that best fits a set of points (I know a simple straight line will work here, because I’m the one generating said line… :D)\nWe’re also going to show the square of errors, how it looks like a U shape, and how it’s clear there is a minimum to it…\n\n## Some points that can be used:\nmy_regression_x_set &lt;- seq(0, 5, 0.1) + rnorm(51,mean = 0, sd = 0.2)\nmy_regression_y_set &lt;- my_interpolation_y_set + rnorm(51, mean = 0, sd = 0.2)\n\n## SIMPLE linear regression. Using Least Squares distance.\n## Key idea: Minimize differences.\n## Find minimum for error function using differentiation.\n## Final equations here instead of step by step explaining how to get there.\n\n## Just follow the math:\nmean_reg_x &lt;- mean(my_regression_x_set)\nmean_reg_y &lt;- mean(my_regression_y_set)\n\ndev_x &lt;- (my_regression_x_set - mean_reg_x)\ndev_y &lt;- (my_regression_y_set - mean_reg_y)\n\nbeta_est &lt;- sum(dev_x * dev_y) / sum(dev_x^2)\nalpha_est &lt;- mean_reg_y - (beta_est * mean_reg_x)\n\nplot(my_regression_y_set ~ my_regression_x_set)\n## alpha & beta can be confusing, I usually use Beta as multipliers of X...\nabline(a = alpha_est, b = beta_est, col=\"blue\")\n\n\n\n\n\n\n\n## Importantly, squared error is differentiable and has a global minimum\n## \"easily\". Related to \"Gradient Descent\"\nerr_x &lt;- my_regression_y_set - (alpha_est + my_regression_x_set * beta_est)\nerr_y &lt;- sapply(err_x, \\(x) x^2)\n\nplot(err_y ~ err_x)\n\n\n\n\n\n\n\n\nAnd that’s it! We’ve used the math (skipping some steps), to show one can use it to do a linear regression (although nowadays all that stuff is available with one command, if not directly, then through packages).\nSo here we’ve shown a downward slope: Let’s put it in context of a potential real-world application: Maybe our x axis represented the number of users connected to our system, and the y axis some number to represent the available resources (spare CPU or something) on a given server. If you know a linear relationship applies, you can estimate at which point you will reach 100% CPU… (I could have started with that, sorry.)\nSo in the future, if you see a clear straight line between two things, well, you could use the above not-scary-at-all math approach to “predict” what would happen if you move along the x axis.\nLast note about the math: in this simple case, if you look at the code, we’ve in fact used two statistics concepts to do the calculations: the means of the x and y axis, and the “deviations” of each point to said means.\nDoes it not make some intuitive sense that the line that best fits the points is related to the means and deviations of the points to said means? :)\n\n\n\nIn the above, we’ve used deviations and means. These are basic concepts from statistics. And in the introduction, we have mentioned something about “models” and “compression”. Which is what it is: Statistics are NOT the real world. And they can be misused. Here the 2 most common examples to illustrate how you can go astray with using statistics without looking at the data.\nLet’s illustrate just that, with a couple of typical (like, VERY typical) examples:\n\n\n\n\n\n\n\n\n\nAnd if it’s not clear enough, all these here below have the same mean and standard deviation:\n\n\n\n\n\n\n\n\n\n[1] \"Mean x Dino: 47.8322528169014\"\n\n\n[1] \"Mean x Bullseye: 47.8308231552178\"\n\n\n[1] \"SD x Dino: 16.7651420391168\"\n\n\n[1] \"SD x Bullseye: 16.7692394934267\"\n\n\nSee how they are basically the same numbers?\nStatistics are great, useful and very important today (and have been for quite a while, I’d argue). They’re just not always the right solution (neither is ML in a larger sense, mind you).\nAlways look at the data!\n\n\n\nWe’ve gone from really REALLY simple stuff all the way to mentioning differentiation, gradient descent, statistics…\nAnd we’ve skipped steps! (i.e. solving systems of equations, which can be represented as matrices, which is relevant for completely different reasons :D)\nNot too shabby, after all…\n\nI’d like to go into Logistic Regression and Neural Networks a bit in a future entries. Even discuss a bit the history of ML, maybe. And that would conclude my preparation for my sessions about “ML Background for Cybersecurity”. Surely there is more to it all, but if I manage to convey a few key concepts to my colleagues, I’ll be very very happy. And this here is me trying.\nBut after that, I’ll shift gears and focus on a personal project I have had in mind for some time and that is taking shape: It involves writing an R package, for a set of ML algorithms that for reasons I don’t quite understand have not received as much attention as I personally think they should. To be continued!\n\n\n\n\nThe Wikipedia for Simple Linear Regression does help.\nHere about the Gradient.\nThe math for the simple linear regression we’ve used today.\nThen the more advanced discussion on Linear Regression hopefully can be followed."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#why-linear-regression-in-the-first-place",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#why-linear-regression-in-the-first-place",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "IF (and that’s a reasonably big if) you can find a linear relationship between two numerical vectors, then using a linear regression can help “encode” the whole pair of vectors through “just” two numbers: A slope, and an intercept.\nIn effect, you’re “modelling” a relationship - and in doing so, you’re actually compressing information.\nMore to the point, if you have an intuition that such relationship holds true for other points you were not given, then you can interpolate (if the new points are in the range of original vectors - otherwise it’s called “extrapolation”) an expected output from a given input.\nIn other words: If you know (or expect) there is a linear (straight line) relationship between a webserver load and number of concurrently opened sessions, you could estimate a load (say % CPU) from knowing only the number of users currently connected (or the other way around, obviously). You’d expect the load to grow with the number of users.\nThe concept of regression (linear or otherwise) are important in ML. It’s a core part of most models. You doubt? Let me put it this way: Regression is a core part of how training a neural network works. There. Believe me now? (I guess I just justified myself in tagging this entry as Machine Learning, after all.)"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#two-points-interextrapolation",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#two-points-interextrapolation",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "We’ll start off very easy today. Imagine to begin with you only have two points. So two pairs\\((x_1, y_1), (x_2, y_2)\\) of coordinates then. IF (again, big IF) you somehow expects a straight line could be used to estimate a third \\(y_3\\) from a third \\(x_3\\) coordinate in the range of the two original points… How can you estimate said value of \\(y_3\\).\nWell, it’s basic (elementary? high school? Idk) math, you want to find \\(a, b\\) in the following:\n\\[\ny = ax + b\n\\]\nThere are two numbers to find (a slope, and an intercept), and we have two points. That’s a system of two equations. But instead of just doing the math, let’s think about this from a conceptual perspective.\nYou can probably skip this, as it’s really very basic, but I like to make sure I understand things, not just use the math I can find out there, and so I’m going to explain it nevertheless.\nWhat is a “slope”?\nThink of a bicycle ride, and you’re faced with a 8% upwards (so “positive”) slope; what does that mean? That for every 100 meters you advance horizontally, you should expect to gain 8 meters in elevation. So if you gain 8 units in the y direction, you have moved 100 units in the x direction.\nWith two points: \\(y_2 - y_1 = 8, x_2 - x_1 = 100\\). And you get to the slope by dividing both numbers (which is why we expressed the slope in percentage). So you need then:\n\\[\na = slope = {(y_2 - y_1) \\over (x_2 - x_1)}\n\\]\nNot soooo misterious now, is it? If the slope is downwards, \\(y_2 &lt; y_1\\) and \\(a &lt; 0\\), that’s all.\nAnd what is the “intercept”?\nIf you show lines on a two dimensional graph, you often will see the \\(y\\) and \\(x\\) axis. The “intercept” is simply the point at which our line crosses (“intercepts”) the \\(y\\) axis. Which happens when \\(x = 0\\).\nAnd by now, if we take any of our points, we have the following bits \\(y_1, a, x_1\\) for our equation \\(y = ax + b\\). Can we get to \\(b\\)?\n\\[\nb = y_1 - a x_1\n\\]So that for \\(x = 0\\), we have \\(b\\) as the value of \\(y\\). Which is what we were looking for.\nThis was very very very basic stuff. But hopefully for those that don’t do math in their day-to-day life, or those that use formulas without thinking (or use Excel to estimate these things, if at all), or simply as a refresher… I don’t know. Sorry if that was boring.\n\n## Suppose you have any two points:\nmy_x &lt;- c(1.5, 2.5)\nmy_y &lt;- c(5, 2)\n\n# Line goes through two points\n## By definition of \"slope\":\nmy_line_slope &lt;- (my_y[2] - my_y[1]) / (my_x[2]- my_x[1])\n## Then just replace with one point to get intercept:\nmy_intercept &lt;- my_y[1] - (my_line_slope * my_x[1])\n\ncalculate_resulting_y &lt;- function(x) {\n    my_line_slope * x + my_intercept\n}\n\n## Say then we have any value of x (\"independent variable\"):\nmy_interpolation_x_set &lt;- seq(0, 5, 0.1)\n## In a perfect World, we could then get any dependent value:\nmy_interpolation_y_set &lt;- sapply(my_interpolation_x_set, calculate_resulting_y)\n\nplot(my_interpolation_y_set ~ my_interpolation_x_set, col = \"blue\", type=\"l\")\npoints(my_y ~ my_x, col = \"red\")"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#multiple-points-with-a-linear-relationship",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#multiple-points-with-a-linear-relationship",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "In the above, we took a pretty big leap of faith: We just somehow knew/assumed that two points would represent a linear relationship.\nBut we’d probably feel more confident if we had more points to confirm our expectations first.\nToday we’ll stay with the “Simple Linear Regression”, which is nicely explained in the Wikipedia.\nWe want to find \\(\\alpha, \\beta\\) that best describe not two points but instead estimate a line for several of them, so that we solve \\(y = \\alpha + \\beta x\\) that best estimate all \\(x_i, y_i\\) pairs we’re given (provided they look like a straight line).\nAnd I’ve said “estimate” twice above, as there will be some error, and that’s key here: We essentially try to minimize the error that our estimated \\(\\alpha, \\beta\\) introduce, for each point. Written “fancily”:\n\\[\n\\widehat{\\epsilon}_i = y_i - (\\alpha + \\beta x_i), \\forall i \\in [1..n], n \\in \\mathbb{N}\n\\]\nWhere n is the number of your available points…\n\nMinimizing the error is an important concept in supervised machine learning.\n\nThen using the “Sum of Square” of the residuals, we’re going to try to minimize that."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#sum-of-square-minimization",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#sum-of-square-minimization",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "I will take the time to dive into this a bit, as it is relevant to understand for other cases. Although the math is better explained here, that approach of minimizing sum of squares is useful in other contexts and so here goes.\nSo today we are trying to “fit” a line to a set of points that are not necessarily exactly on said line (see “the result” below). But where do we start? How do we go about finding one line out of an infinite set of possible lines?\nMore or less, we find the parameters for our line equation that minimizes overall distance from all the points. So we try to minimize the sum of the error of our line from each point. It is a minimization exercise, but sometimes the points will be under the line, and others they’ll be above. To correct for that, we actually minimize the sum of distances, each one squared! That way, all values are positive, and we really try to minimize something.\nThere is another important point hidden right here: The key to understand “Gradient Descent”!\nAs we will see with the “results” below, the errors, when squared, are shaped in a sort of U. But what’s important is that the function behind that (parabolic) shape is that it’s differentiable.\nDifferentiating (that’s calculus) here will mean that we know in which direction (up, down, horizontal) said curve goes for a given point on the x axis. If we know that, we know that if the curve goes upwards, to minimize the thing, we want to go in the opposite direction.\nAnd that’s exactly what “descending the gradient” is all about (although in multivariate settings: A gradient in vector calculus is “the direction and the rate of fastest increase”, so to “descend a gradient”, you want to move in the opposite direction).\nFor linear regression, it’s cool because we can do partial derivatives (as shown on the referenced Wiki page above) to find our \\(\\alpha\\) and \\(\\beta\\) parameters that describe the line that “best fit the data”.\nIt’s quite clever, mind you: You minimize the error of your predicted value compared to the actual value. That’s how your machine can learn! But it’s also what people have been using to train Neural Networks!\nFor today, let’s just store the following information: The “Least Square Minimization” can help find a minimum error through differentiation.\nAnd for a future conversation: Gradient Descent (hidden in today’s Sum of Square Minimization) is one of the two mathematical keys for back-propagation. (And back-propagation was the before and after of connectionism in Machine Learning theory).\nAlright, let’s move on and return to our very simple example."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#the-result",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#the-result",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "Remember how we said at some point that in ML, more data points is often better?\nWe’re going to find the line that best fits a set of points (I know a simple straight line will work here, because I’m the one generating said line… :D)\nWe’re also going to show the square of errors, how it looks like a U shape, and how it’s clear there is a minimum to it…\n\n## Some points that can be used:\nmy_regression_x_set &lt;- seq(0, 5, 0.1) + rnorm(51,mean = 0, sd = 0.2)\nmy_regression_y_set &lt;- my_interpolation_y_set + rnorm(51, mean = 0, sd = 0.2)\n\n## SIMPLE linear regression. Using Least Squares distance.\n## Key idea: Minimize differences.\n## Find minimum for error function using differentiation.\n## Final equations here instead of step by step explaining how to get there.\n\n## Just follow the math:\nmean_reg_x &lt;- mean(my_regression_x_set)\nmean_reg_y &lt;- mean(my_regression_y_set)\n\ndev_x &lt;- (my_regression_x_set - mean_reg_x)\ndev_y &lt;- (my_regression_y_set - mean_reg_y)\n\nbeta_est &lt;- sum(dev_x * dev_y) / sum(dev_x^2)\nalpha_est &lt;- mean_reg_y - (beta_est * mean_reg_x)\n\nplot(my_regression_y_set ~ my_regression_x_set)\n## alpha & beta can be confusing, I usually use Beta as multipliers of X...\nabline(a = alpha_est, b = beta_est, col=\"blue\")\n\n\n\n\n\n\n\n## Importantly, squared error is differentiable and has a global minimum\n## \"easily\". Related to \"Gradient Descent\"\nerr_x &lt;- my_regression_y_set - (alpha_est + my_regression_x_set * beta_est)\nerr_y &lt;- sapply(err_x, \\(x) x^2)\n\nplot(err_y ~ err_x)\n\n\n\n\n\n\n\n\nAnd that’s it! We’ve used the math (skipping some steps), to show one can use it to do a linear regression (although nowadays all that stuff is available with one command, if not directly, then through packages).\nSo here we’ve shown a downward slope: Let’s put it in context of a potential real-world application: Maybe our x axis represented the number of users connected to our system, and the y axis some number to represent the available resources (spare CPU or something) on a given server. If you know a linear relationship applies, you can estimate at which point you will reach 100% CPU… (I could have started with that, sorry.)\nSo in the future, if you see a clear straight line between two things, well, you could use the above not-scary-at-all math approach to “predict” what would happen if you move along the x axis.\nLast note about the math: in this simple case, if you look at the code, we’ve in fact used two statistics concepts to do the calculations: the means of the x and y axis, and the “deviations” of each point to said means.\nDoes it not make some intuitive sense that the line that best fits the points is related to the means and deviations of the points to said means? :)"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#look-at-the-data",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#look-at-the-data",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "In the above, we’ve used deviations and means. These are basic concepts from statistics. And in the introduction, we have mentioned something about “models” and “compression”. Which is what it is: Statistics are NOT the real world. And they can be misused. Here the 2 most common examples to illustrate how you can go astray with using statistics without looking at the data.\nLet’s illustrate just that, with a couple of typical (like, VERY typical) examples:\n\n\n\n\n\n\n\n\n\nAnd if it’s not clear enough, all these here below have the same mean and standard deviation:\n\n\n\n\n\n\n\n\n\n[1] \"Mean x Dino: 47.8322528169014\"\n\n\n[1] \"Mean x Bullseye: 47.8308231552178\"\n\n\n[1] \"SD x Dino: 16.7651420391168\"\n\n\n[1] \"SD x Bullseye: 16.7692394934267\"\n\n\nSee how they are basically the same numbers?\nStatistics are great, useful and very important today (and have been for quite a while, I’d argue). They’re just not always the right solution (neither is ML in a larger sense, mind you).\nAlways look at the data!"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#conclusions",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#conclusions",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "We’ve gone from really REALLY simple stuff all the way to mentioning differentiation, gradient descent, statistics…\nAnd we’ve skipped steps! (i.e. solving systems of equations, which can be represented as matrices, which is relevant for completely different reasons :D)\nNot too shabby, after all…\n\nI’d like to go into Logistic Regression and Neural Networks a bit in a future entries. Even discuss a bit the history of ML, maybe. And that would conclude my preparation for my sessions about “ML Background for Cybersecurity”. Surely there is more to it all, but if I manage to convey a few key concepts to my colleagues, I’ll be very very happy. And this here is me trying.\nBut after that, I’ll shift gears and focus on a personal project I have had in mind for some time and that is taking shape: It involves writing an R package, for a set of ML algorithms that for reasons I don’t quite understand have not received as much attention as I personally think they should. To be continued!"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#references",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#references",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "The Wikipedia for Simple Linear Regression does help.\nHere about the Gradient.\nThe math for the simple linear regression we’ve used today.\nThen the more advanced discussion on Linear Regression hopefully can be followed."
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html",
    "href": "posts/2024-12-08_A_new_project/index.html",
    "title": "A new project",
    "section": "",
    "text": "Ever since I read about the concept in M. Mitchell’s book “Complexity The emerging science at the edge of order and chaos” some time last year (I think around this time of the year…), I have been thinking about this, in the background of my head.\n\n\n\nWhat piqued me curiosity\n\n\nThe weird strings above, the 11##10### thing, might mean nothing to most right now (you have to look a bit further into it all, and I’ll probably try and explain some of it in the future), but it was a revelation to me when I first read it.\nSo yes, I’ve had other fish to fry for some time, but now it feels like I might just have the mental space to shift my focus a bit…\n\n\nSo here is my upcoming project, explained in one picture:\n\n\n\nMy new project in one picture\n\n\n(By the way, a personal note here: buying that left book in physical format from Springer was one “tough cookie”, thank goodness I’m motivated, it’s quite possibly the priciest book I ever bought from my own pocket, in €-per-page… Oh well: It looks it still was the right call for me :D)\n\n\n\nAs I said, I’ve been a bit busy. Full-time job, MSc paper (success!), then summer, learning other stuff, (did I mention full-time job? :D)… But it is time.\nI also have been thinking for quite some time I should try my hand eventually at creating my own R Package. This was all the more true after the last R conferences I attended recently, where it looks like “everyone is doing it” in the R (mostly academic) community… So why not me?\nPlus, I have an idea or two. So what if I’m not the greatest coder out there? I still do have some tricks in my sleeves ;) I have proved (at least to myself) I can do highly distributed processing using (among other things) R and create my own thing. (My very own algorithm, that was.)\nPlus, I’m into ML just as much as the next person, but I’ve also played quite a bit with Genetic Algorithms recently. And I believe I have all it takes to put that to good use here. (I’m just a bit sad this will not involve Network Graphs, the way I have it designed (yet, anyway :D).) I believe I understand how the LCS algorithms might be particularly useful in their own specific way. I can see in my head how they must work, what makes them different, all in spite of not having written one line of code yet.\nNow, it might take a while, indeed. Why, with the Holiday season coming, the day job and all. But I have a sort of a plan in mind, already an architecture (in my head, for now), examples of how I would want to validate the implementation (that last thing, one example with application to cybersecurity and logs classification, in particular, woke me up in the middle of the night a couple of weeks back).\nAnd R packages, from what I have read thus far, are not “magic”. It’s going to be work, for sure, but I don’t see anything I can’t manage. If anything, it’s less complicated than I thought. I’ve already played with testthat, microbenchmark, I’ve always separated my code in functions, separated R and C++ code, all that… So yeah, I think I’m up to the task.\nAnd finally, a note about the “why now”: I can’t find it done yet!! Which to me is quite incredible. I have found this niche, that nobody (in the R community) has deemed relevant enough thus far, and I am willing to put the work towards filling that niche!\n\n\n\nThis here today is me announcing, basically, my intention to create and (hopefully) get my future new RLCS package onto CRAN. I know, I know, I’m getting WAAAAY ahead of myself here.\nAnd I realize me publishing this entry might very well mean someone beats me to it. If that happens, well… I’m already convinced my implementation will be “competitive”, so… let’em!\n(Plus, why would anyone decide to go for this now? It would just make no sense whatsoever - if not to bother me. If that’s you: Just give me a few months, and you’ll save yourself the trouble :D)\n\n\n\nIt turns out, for some reason, I know I will. Obviously, I don’t “know know”, it’s more of a feeling.\nI am enthused, to begin with. I have the knowledge. Time is not an issue, although it will take me probably a few months (my best estimate right now, given my current known agenda, is 3-4 months for a functional package, plus whatever time it takes for submission to CRAN of a “v001”).\nMoreover, I somehow needed a new, rather difficult, interesting long-term personal objective. I feel this will help me get in the “flow”, which is a great feeling (but hard to come by).\nAlso, I have this crazy idea that, once the thing works, I can dive further into it and study it towards a bigger goal, maybe research, ideally even (who knows) a PhD…?\n\n\n\nI’ve done my first (of two) presentation about “Background of ML for Cybersecurity”. I actually had trouble with some of the demos during the first session :S But not because of the demos themselves, just, “heat of the moment”, computer resources, demo effect, and so on. Nothing major.\nThankfully, I get a second chance, as there is a second session coming. And I have time to prepare some more examples, so that’s good too, the second session will be all-the-more interesting.\nBut because I have some time, I’m making a pause there and re-focusing on this new project of mine as a personal interest thing.\n\n\n\nI realize nobody cares, but heck: I do. I need to “migrate”, move everything (while reviewing each thing) from the old blog to this new setup, because.. Well, because this platform is more convenient, and – let’s be honest – cheaper… After all, it’s one thing to write for one own’s pleasure, and quite another to spend money with no return year after year just to maintain it.\nSo yes, I need to organize myself to minimize future costs a bit (it just is the sensible thing to do, economically).\n\n\n\n**This is important: I still don’t really know why this family of algorithms have received sooo little attention**, beyond the fact that “Connectionism” has received a lot of said attention (deservingly, for sure).\nBut I’m curious about whether LCS can be made (ideally equally) valuable. And I have the personal conviction that I am in the right position to dig further into it.\nAnd so it is time, I’m in an ideal position, and I’m motivated.\nNow all I have to do is… Start!\n(To start hear means also put thoughts to paper, design, code, test, document… Don’t expect lots of news too soon, either! I’m not in a hurry, nobody should be: this thing has been known since the 70’s and it looks like few - aside from noted exceptions - have cared about it thus far…)\n\n\n\nThis is the best intro explaining the algorithm that I found thus far - and if anything it confirmed I was on the right path. It just makes a lot of sense to me!"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#the-new-project",
    "href": "posts/2024-12-08_A_new_project/index.html#the-new-project",
    "title": "A new project",
    "section": "",
    "text": "So here is my upcoming project, explained in one picture:\n\n\n\nMy new project in one picture\n\n\n(By the way, a personal note here: buying that left book in physical format from Springer was one “tough cookie”, thank goodness I’m motivated, it’s quite possibly the priciest book I ever bought from my own pocket, in €-per-page… Oh well: It looks it still was the right call for me :D)"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#why-now-why-me",
    "href": "posts/2024-12-08_A_new_project/index.html#why-now-why-me",
    "title": "A new project",
    "section": "",
    "text": "As I said, I’ve been a bit busy. Full-time job, MSc paper (success!), then summer, learning other stuff, (did I mention full-time job? :D)… But it is time.\nI also have been thinking for quite some time I should try my hand eventually at creating my own R Package. This was all the more true after the last R conferences I attended recently, where it looks like “everyone is doing it” in the R (mostly academic) community… So why not me?\nPlus, I have an idea or two. So what if I’m not the greatest coder out there? I still do have some tricks in my sleeves ;) I have proved (at least to myself) I can do highly distributed processing using (among other things) R and create my own thing. (My very own algorithm, that was.)\nPlus, I’m into ML just as much as the next person, but I’ve also played quite a bit with Genetic Algorithms recently. And I believe I have all it takes to put that to good use here. (I’m just a bit sad this will not involve Network Graphs, the way I have it designed (yet, anyway :D).) I believe I understand how the LCS algorithms might be particularly useful in their own specific way. I can see in my head how they must work, what makes them different, all in spite of not having written one line of code yet.\nNow, it might take a while, indeed. Why, with the Holiday season coming, the day job and all. But I have a sort of a plan in mind, already an architecture (in my head, for now), examples of how I would want to validate the implementation (that last thing, one example with application to cybersecurity and logs classification, in particular, woke me up in the middle of the night a couple of weeks back).\nAnd R packages, from what I have read thus far, are not “magic”. It’s going to be work, for sure, but I don’t see anything I can’t manage. If anything, it’s less complicated than I thought. I’ve already played with testthat, microbenchmark, I’ve always separated my code in functions, separated R and C++ code, all that… So yeah, I think I’m up to the task.\nAnd finally, a note about the “why now”: I can’t find it done yet!! Which to me is quite incredible. I have found this niche, that nobody (in the R community) has deemed relevant enough thus far, and I am willing to put the work towards filling that niche!"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#the-outcome",
    "href": "posts/2024-12-08_A_new_project/index.html#the-outcome",
    "title": "A new project",
    "section": "",
    "text": "This here today is me announcing, basically, my intention to create and (hopefully) get my future new RLCS package onto CRAN. I know, I know, I’m getting WAAAAY ahead of myself here.\nAnd I realize me publishing this entry might very well mean someone beats me to it. If that happens, well… I’m already convinced my implementation will be “competitive”, so… let’em!\n(Plus, why would anyone decide to go for this now? It would just make no sense whatsoever - if not to bother me. If that’s you: Just give me a few months, and you’ll save yourself the trouble :D)"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#will-i-manage",
    "href": "posts/2024-12-08_A_new_project/index.html#will-i-manage",
    "title": "A new project",
    "section": "",
    "text": "It turns out, for some reason, I know I will. Obviously, I don’t “know know”, it’s more of a feeling.\nI am enthused, to begin with. I have the knowledge. Time is not an issue, although it will take me probably a few months (my best estimate right now, given my current known agenda, is 3-4 months for a functional package, plus whatever time it takes for submission to CRAN of a “v001”).\nMoreover, I somehow needed a new, rather difficult, interesting long-term personal objective. I feel this will help me get in the “flow”, which is a great feeling (but hard to come by).\nAlso, I have this crazy idea that, once the thing works, I can dive further into it and study it towards a bigger goal, maybe research, ideally even (who knows) a PhD…?"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#what-about-the-sessions-on-ml",
    "href": "posts/2024-12-08_A_new_project/index.html#what-about-the-sessions-on-ml",
    "title": "A new project",
    "section": "",
    "text": "I’ve done my first (of two) presentation about “Background of ML for Cybersecurity”. I actually had trouble with some of the demos during the first session :S But not because of the demos themselves, just, “heat of the moment”, computer resources, demo effect, and so on. Nothing major.\nThankfully, I get a second chance, as there is a second session coming. And I have time to prepare some more examples, so that’s good too, the second session will be all-the-more interesting.\nBut because I have some time, I’m making a pause there and re-focusing on this new project of mine as a personal interest thing."
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#and-what-about-the-migration-of-the-old-blog",
    "href": "posts/2024-12-08_A_new_project/index.html#and-what-about-the-migration-of-the-old-blog",
    "title": "A new project",
    "section": "",
    "text": "I realize nobody cares, but heck: I do. I need to “migrate”, move everything (while reviewing each thing) from the old blog to this new setup, because.. Well, because this platform is more convenient, and – let’s be honest – cheaper… After all, it’s one thing to write for one own’s pleasure, and quite another to spend money with no return year after year just to maintain it.\nSo yes, I need to organize myself to minimize future costs a bit (it just is the sensible thing to do, economically)."
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#conclusions",
    "href": "posts/2024-12-08_A_new_project/index.html#conclusions",
    "title": "A new project",
    "section": "",
    "text": "**This is important: I still don’t really know why this family of algorithms have received sooo little attention**, beyond the fact that “Connectionism” has received a lot of said attention (deservingly, for sure).\nBut I’m curious about whether LCS can be made (ideally equally) valuable. And I have the personal conviction that I am in the right position to dig further into it.\nAnd so it is time, I’m in an ideal position, and I’m motivated.\nNow all I have to do is… Start!\n(To start hear means also put thoughts to paper, design, code, test, document… Don’t expect lots of news too soon, either! I’m not in a hurry, nobody should be: this thing has been known since the 70’s and it looks like few - aside from noted exceptions - have cared about it thus far…)"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#references",
    "href": "posts/2024-12-08_A_new_project/index.html#references",
    "title": "A new project",
    "section": "",
    "text": "This is the best intro explaining the algorithm that I found thus far - and if anything it confirmed I was on the right path. It just makes a lot of sense to me!"
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "At least the XCS version of LCS algorithm is a “well-known” (in the rather small sub-world of LCS enthusiasts, that is) approach to do Reinforcement Learning.\nThen again, all step-by-steps for RL in LCS seem to be hidden in more or less archane papers which are pay-per-access, and I don’t know yet if things make enough sense for me to pay for reading said papers. (Until recently, I had access with my University’s MSc Student account… No more, it would seem. Fair enough.) We’ll see how I go about that later…\nBut for now…\n\n\n\nWell, what I can do, is try to come up with my own personal version of an RL implementation with my RLCS “future-package”, see if I can make it work, without worrying about past work too much for now.\nI do have a general understanding of ideas of LCS and RL, I just haven’t found a detailed step-by-step for me to copy from pseudo-code. I also have been reading (but I’m far from done) what I understand is the bible of Reinforcement Learning, i.e. “Reinforcement Learning - An Introduction” by R. S. Sutton & A. G. Bardo. The general ideas of TD and Q-Learning I think I somewhat grasp, and I have my own (limited, sure) experience with Monte-Carlo.\nMixing these concepts with what I have implemented thus far of an LCS, adapting my code with concepts of Action Sets instead of Correct Sets, detect/action/effect/reward with an environment, and a split between exploit and explore, I came up with this “design” this morning (Saturday and Sunday mornings are often a bit quieter than the rest of the week):\n\n\n\nthinking, thinking…\n\n\nThis is all on my small home-office wall right now, not at all implemented, but it seems it should be feasible, and I am curious as to whether it could work (or be a complete bust).\n\n\n\nWell, truth being told… I don’t know.\nIt “sounds” like a cool thing to be able to do. In the above drawing, I suggest I could, using an LCS, implement an “agent” (yes, one of those, but please, I’m not talking about LLMs) that would learn on its own how to navigate a simple grid-world and look for food.\nI will have to code the whole logic and rules of that world, all aspects of movements in it, rewards, etc. It’s going to be a bit of work (somehow I think I’ll manage).\nBut yeah, I don’t know a few things at this point: whether my “design” will work (with whatever adjustments I can come up with), or why even program an RL algorithm.\nI just think it would be cool to have this future package be able to do all of data mining, supervised learning and reinforcement learning, with examples of each.\nAs to when I could use it, I know the answer for data mining and SL – not so much for RL, for now.\n\n\n\nJust another fun exercise, really. It will take a few weeks, probably.\nBut if (big if at this stage) I manage to make it work, that would mean I actually understand enough of LCS as a concept to implement it from scratch and do data mining, Supervised Learning and Reinforcement learning. In turn that would also imply I know a bit more of what Reinforcement Learning entails.\nWhich in itself – for me – would already be pretty cool.\n\n\n\nTo be fair, there are a few papers out there I was able to access for free. In no special order and particularly valuable to inspire my future exercise I found these:\nhttps://arxiv.org/abs/2305.09945\nhttps://dl.acm.org/doi/pdf/10.1145/206944.206955\nhttps://www2.cs.uh.edu/~ceick/6367/bull-lcs.pdf"
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#although-i-know-in-theory",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#although-i-know-in-theory",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "At least the XCS version of LCS algorithm is a “well-known” (in the rather small sub-world of LCS enthusiasts, that is) approach to do Reinforcement Learning.\nThen again, all step-by-steps for RL in LCS seem to be hidden in more or less archane papers which are pay-per-access, and I don’t know yet if things make enough sense for me to pay for reading said papers. (Until recently, I had access with my University’s MSc Student account… No more, it would seem. Fair enough.) We’ll see how I go about that later…\nBut for now…"
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#general-understanding-of-rl-and-lcs-my-own-mix",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#general-understanding-of-rl-and-lcs-my-own-mix",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "Well, what I can do, is try to come up with my own personal version of an RL implementation with my RLCS “future-package”, see if I can make it work, without worrying about past work too much for now.\nI do have a general understanding of ideas of LCS and RL, I just haven’t found a detailed step-by-step for me to copy from pseudo-code. I also have been reading (but I’m far from done) what I understand is the bible of Reinforcement Learning, i.e. “Reinforcement Learning - An Introduction” by R. S. Sutton & A. G. Bardo. The general ideas of TD and Q-Learning I think I somewhat grasp, and I have my own (limited, sure) experience with Monte-Carlo.\nMixing these concepts with what I have implemented thus far of an LCS, adapting my code with concepts of Action Sets instead of Correct Sets, detect/action/effect/reward with an environment, and a split between exploit and explore, I came up with this “design” this morning (Saturday and Sunday mornings are often a bit quieter than the rest of the week):\n\n\n\nthinking, thinking…\n\n\nThis is all on my small home-office wall right now, not at all implemented, but it seems it should be feasible, and I am curious as to whether it could work (or be a complete bust)."
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#why-even-consider-rl",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#why-even-consider-rl",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "Well, truth being told… I don’t know.\nIt “sounds” like a cool thing to be able to do. In the above drawing, I suggest I could, using an LCS, implement an “agent” (yes, one of those, but please, I’m not talking about LLMs) that would learn on its own how to navigate a simple grid-world and look for food.\nI will have to code the whole logic and rules of that world, all aspects of movements in it, rewards, etc. It’s going to be a bit of work (somehow I think I’ll manage).\nBut yeah, I don’t know a few things at this point: whether my “design” will work (with whatever adjustments I can come up with), or why even program an RL algorithm.\nI just think it would be cool to have this future package be able to do all of data mining, supervised learning and reinforcement learning, with examples of each.\nAs to when I could use it, I know the answer for data mining and SL – not so much for RL, for now."
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#conclusions",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#conclusions",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "Just another fun exercise, really. It will take a few weeks, probably.\nBut if (big if at this stage) I manage to make it work, that would mean I actually understand enough of LCS as a concept to implement it from scratch and do data mining, Supervised Learning and Reinforcement learning. In turn that would also imply I know a bit more of what Reinforcement Learning entails.\nWhich in itself – for me – would already be pretty cool."
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#references",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#references",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "To be fair, there are a few papers out there I was able to access for free. In no special order and particularly valuable to inspire my future exercise I found these:\nhttps://arxiv.org/abs/2305.09945\nhttps://dl.acm.org/doi/pdf/10.1145/206944.206955\nhttps://www2.cs.uh.edu/~ceick/6367/bull-lcs.pdf"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "I have yet another day off (that’s the thing with Holiday season), and I really wanted to keep working on this.\nSo I added the Genetic Algorithm part to my Supervised Learning basic Michigan-style LCS.\nThat means, after covering (done last Sunday) I am done with the (basic) implementation of the Rule Discovery features of the LCS algorithm.\nAnd, heck, it works.\n\n\n\nSo we had covering, whereby new rules were created when none in the classifiers matched the a new-coming environment instance.\nBut this is only part of the discovery of new rules. The second part in the Michigan-style LCS is that of a Genetic Algorithm.\nI implemented now a simple version, with tournament parents selection. That is, from the Correct set (see past entries) – but taking numerosity of classifiers into account – I choose N (a parameter) parents randomly. Then I take the best 2 parents by fitness. Which is ellitist.\nThen I run a one-point cross-over, to generate 2 children.\nI apply mutation to the children genes in turn, with a mutation pressure set as yet another parameter (default is 5% in my implementation).\nAnd finally I add the children to the population.\nNote that children mutations ensure only valid rules are created (i.e. condition is compatible with the training instance). Mutations either generalize (add a wildcard) or specify (replaces a wildcard by a 0 or 1, depending on training instance).\nFully general (“all wildcard”) individuals are of course discarded. If a child actually matches by condition an individual in the correct set, it is discarded and the repeated classifier instead get a numerosity increase.\nAnd that’s IT!\n\n\n\nIn my current dummy example, whereby the goal is for the LCS to discover the rule “Class is NOT(bit 4 of input state)”, well… In manual runs (several of them), I often (not always, mind you) find the two best rules are perfect rules:\n\n\n\nPerfect Rules are often discovered!\n\n\n\n\n\nThe first two rules (ordered by accuracy and numerosity, both decreasing) are the exact rules that perfectly will always predict the correct class/action, and are perfectly balanced in this case between general and specific.\nThese are the kind of things that an LCS provides that a Neural Network won’t:\nIt tells you WHY it makes a decision.\nHere, it is saying:\n\nRule 1: If bit 4 is 0, then class is 1. I am 100% accurate, whenever that happens.\nRule 2: If bit 4 is 1, then class is 0. I am 100% accurate, whenever that happens.\n\nWhich is exactly what this dummy training data set was trying to say in the first place, with 70% of the training possibilities shown to train the algorithm.\nObviously this is… A somewhat silly example. But the thing is, I did NOT tell it how to get there, and it got the IF THEN ELSE correctly on its own.\n\n\n\nEvery now and then (after a certain number of Epochs) I run a Deletion process, whereby rules that have Numerosity 1 are discarded, thereby containing the size of the set of classifiers.\nThis is important for interpretability and performance.\n\n\n\nI am quite happy that after only a couple of days of coding, I have a basic, perfectly functional LCS!\nI do miss two components: Subsumption and Compaction.\nPlus of course the Prediction, that comes after training, on a test subset.\nIt’s all on my to-do, but this thing is already promising!\nGranted, it’s not real fast. And the code is far from perfect, or duly organized. But it works!"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#intro",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#intro",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "I have yet another day off (that’s the thing with Holiday season), and I really wanted to keep working on this.\nSo I added the Genetic Algorithm part to my Supervised Learning basic Michigan-style LCS.\nThat means, after covering (done last Sunday) I am done with the (basic) implementation of the Rule Discovery features of the LCS algorithm.\nAnd, heck, it works."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#what-the-ga-does",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#what-the-ga-does",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "So we had covering, whereby new rules were created when none in the classifiers matched the a new-coming environment instance.\nBut this is only part of the discovery of new rules. The second part in the Michigan-style LCS is that of a Genetic Algorithm.\nI implemented now a simple version, with tournament parents selection. That is, from the Correct set (see past entries) – but taking numerosity of classifiers into account – I choose N (a parameter) parents randomly. Then I take the best 2 parents by fitness. Which is ellitist.\nThen I run a one-point cross-over, to generate 2 children.\nI apply mutation to the children genes in turn, with a mutation pressure set as yet another parameter (default is 5% in my implementation).\nAnd finally I add the children to the population.\nNote that children mutations ensure only valid rules are created (i.e. condition is compatible with the training instance). Mutations either generalize (add a wildcard) or specify (replaces a wildcard by a 0 or 1, depending on training instance).\nFully general (“all wildcard”) individuals are of course discarded. If a child actually matches by condition an individual in the correct set, it is discarded and the repeated classifier instead get a numerosity increase.\nAnd that’s IT!"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#results-with-fully-functional-rule-discovery",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#results-with-fully-functional-rule-discovery",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "In my current dummy example, whereby the goal is for the LCS to discover the rule “Class is NOT(bit 4 of input state)”, well… In manual runs (several of them), I often (not always, mind you) find the two best rules are perfect rules:\n\n\n\nPerfect Rules are often discovered!"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#important-what-the-rules-set-tells-us",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#important-what-the-rules-set-tells-us",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "The first two rules (ordered by accuracy and numerosity, both decreasing) are the exact rules that perfectly will always predict the correct class/action, and are perfectly balanced in this case between general and specific.\nThese are the kind of things that an LCS provides that a Neural Network won’t:\nIt tells you WHY it makes a decision.\nHere, it is saying:\n\nRule 1: If bit 4 is 0, then class is 1. I am 100% accurate, whenever that happens.\nRule 2: If bit 4 is 1, then class is 0. I am 100% accurate, whenever that happens.\n\nWhich is exactly what this dummy training data set was trying to say in the first place, with 70% of the training possibilities shown to train the algorithm.\nObviously this is… A somewhat silly example. But the thing is, I did NOT tell it how to get there, and it got the IF THEN ELSE correctly on its own."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#deletion-is-also-implemented",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#deletion-is-also-implemented",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "Every now and then (after a certain number of Epochs) I run a Deletion process, whereby rules that have Numerosity 1 are discarded, thereby containing the size of the set of classifiers.\nThis is important for interpretability and performance."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#conclusion",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#conclusion",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "I am quite happy that after only a couple of days of coding, I have a basic, perfectly functional LCS!\nI do miss two components: Subsumption and Compaction.\nPlus of course the Prediction, that comes after training, on a test subset.\nIt’s all on my to-do, but this thing is already promising!\nGranted, it’s not real fast. And the code is far from perfect, or duly organized. But it works!"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I’ve already tried to parallelize a bit the training of a supervised-learning LCS. It kinda’ works… But the more I think about it, the more I’m convinced it’s not as easy as I thought.\nI initially thought of one way of parallel processing: Break the training samples in N (as many as CPU cores you wanna use) subsets, train N LCS.\nThen “merge”, subsume, remove duplicates, and… Repeat.\nIdeally you would bootstrap the thing with an initial epoch on all the training set, just to have relevant coverage to start with.\nBut there is an issue: What if your dataset is “not perfect”? Let’s suppose there isn’t a clear-cut separation between classes to be found, but rather a fuzzy separation. So there aren’t perfect rules to be discovered. Instead, in each subset, some rules work, but THEN when these are exposed to the complete dataset, the discovered classes in one subset is correct in that subset, but NOT in the overall training data?\nYou end up with classifiers that… Don’t work over test sets.\nAt least that’s my current conclusion after trying it with some messier-than-perfect datasets.\n\n\n\nSo if breaking the dataset in sub-sets does not work… What about breaking the data POINTS in different parts?\nIn my earlier example, I have tested (quite successfully, in my view) my RLCS on a simplified MNIST exercise.\nThere, I was training to classify images. The original images where too big, in their equivalent bit-strings representation, for my laptop to work on in acceptable runtimes (for my taste). I ended up compressing said images into 49 bits strings per image. It was then manageable, and it worked quite good, in little over 20 seconds of training (with the correct hyper-parameters).\nBut could I instead break each images in say 7x7 lines, and train on each line separately (and in parallel), and then train one more LCS on the result of that (so 7 outputs would become the input states for a new classifier system, another 7-bits train set, so to speak)?\nI would then in effect, in terms of overall processing time, train approximately twice on 7 bits strings, instead of once on 49 bits strings.\nWould that even work? More on that later.\nBut first…\n\n\n\nEpistasis!\nWell, at least that’s how I understand it right now: Two (or more) different locations of the “genome” are required to define the phenotype. In my simple RLCS, in that case two or more bits in a state interact in a more or less complex manner so that the class is decided by said interaction. THEN, the second approach would break the assumptions!\nThink of the “Mux6” example explained in this Blog a few entries ago. Two bits pointed to a position, which value was the class to be predicted. In that example, the first two bits pointed to a position in the last 4 bits.\nIF I were to break the 6 bits states in say two sub-strings of 3 bits each, thinking I can learn from each sub-string and then recompose (see “idea version 2” above)… I would fail miserably: you need the complete 6-bit strings in that example for the LCS to come up with the relevant rules!\nThat said…\n\n\n\nI’ve shown ad-nauseam by now the “not-bit-4” example. In the example so far, I have used 5 bits strings. So the “search space” was that of 5 bits. And the solution was simple in that one bit would suffice to predict the whole class.\nBut that was arbitrary, and meant to be simple.\nWhat if we were to be faced with the exact same rule to be found in 10-bits strings? The solution would be just as simple (two rules would suffice), but the search space would be comparatively… Well, way bigger.\nIn 5-bits, the not-bit-4 is fully expressed with 32 states.\nIn 10-bits, the SAME not-bit-4 classifier works with… 1024 states (total). For a perfect data mining exercise, you need to find a ruleset that perfectly matches all 1024 states, and you need to try combinations of 3 possible values per-bit (our ternary alphabet: {‘#’, ‘0’, ‘1’}). If I’m correct, that could be up to about 3^10*2 (including class) 59049-1 possible unique classifiers to potentially check (coverage ensures here that not-bit-4 is respected, so that’s only half the overall 10 bits strings + class, as half are illegal, making it a 3^10 matter… And the ‘##########’ option is not valid for us…). Well, approximately anyway.\nAnd let’s remember, an LCS is expected to output a combination subset of these ~59K classifiers as an output System, which could be a very large search space indeed… (In a real world setup, that is.)\nHowever! Well, let’s try to apply the logic of my “approach number 2” above to break down the exercise and then see what would happen in such a simple case…\nImagine I break down the exercise into 3 “trainings”:\n\nThe first 5 bits (which in fact are more than enough) per state, creating \\(RLCS_1\\)\nThe last 5 bits per state (completely irrelevant, but we don’t know that upfront), creating \\(RLCS_2\\)\nThen the outputs of the above to train a third LCS, \\(RLCS_{combined}\\)\n\nThe hope is that we can train the first two in parallel, and use the predictions of these two in combination to then train a third classifier (which incidentally here would be exposed to 2-bits states). So I’d expect to reduce the search space quite a bit. And hence the training times.\nWell, with no further ado, here come the initial results, and heck!\nParallel runtime using the approach described here to break the problem in just two 5-bit string sub-problems, less than 5 seconds:\n\n\n\nbreaking original problem in two makes it fast!\n\n\nCompared to original sequential time on the 10-bit string, more than 4 minutes:\n\n\n\nThat worked, just… Very slow, comparatively\n\n\nThe result however is slightly more difficult to interpret (that is, in the current version of this exercise, which I just wanted to see working, but it is not clean…). In particular, each subset I have to keep (for now) better than chance results (anything &gt; 0.5), but that’s just because it’s a dirty example, it can be made cleaner for sure:\n\n\n\nFast results, but requires a bit more work to interpret\n\n\nStill, from “&gt; 4 minutes” to “&lt; 5 seconds”! But then again, this will only works for such a problem where the solution did exist in a subset of original bit-strings.\n\n\n\nWell, for one, I am NOT in fact browsing in parallel through 1024 states!\nSee, each sub-string of 5 bits and its class, as a subset can be de-duplicated! There are a lot of repeated instances, say in the first subset, whereby all entries that say “00000 -&gt; class 1” appear 32 times (one for each of the continuing 5-bit strings for the rest of the original state).\nThat kind of deduplication can reduce processing efforts indeed :)\n\n\n\nI haven’t tested it yet, but I would wager this approach should work just fine on images too, as described above, breaking images in lines and then combining classifiers…\nParallelizing training is an obvious idea (to me) in concept: For some examples, training is slow, and so running it in N parallel processes must be faster (for, you know, complicated examples).\nBut as explained in some detail today, it’s not that simple! It might work in some cases, as explained, but in the “real world”, we wouldn’t know upfront whether it would in fact work, would we? And so we might have to do some trial-and-error…\nBut even just running tests on new datasets, as explained above, might allow us to:\n\nEither see how we can run fast in parallel on a given dataset and still work out a solution\nOr find out that the dataset cannot be broken the way we have initially chosen, indicating possible “epistasis”, which in itself would be interesting and would have just supposed a few seconds lost here.\nOr – and that could happen – that the chosen dataset cannot be broken down as we would like it to, for an LCS to come up with a proposed classification solution as there is not a very clear-cut solution (think Yahoo finance historical data… Yes, I’ve had a quick look. Suffice to say, I won’t beat the markets just yet :D:D:D But that’s not relevant… )\n\nI’ll work on that parallelizing stuff some more… Even if that’s not part of the package itself, it’s more about data preparation and how to use the package, I am aware. But demonstrating how the future RLCS package can hopefully be used and competitive compared to a Neural Network (for instance) is part of my overall objective, I guess.\nFun stuff (to me) all of it :)"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I’ve already tried to parallelize a bit the training of a supervised-learning LCS. It kinda’ works… But the more I think about it, the more I’m convinced it’s not as easy as I thought.\nI initially thought of one way of parallel processing: Break the training samples in N (as many as CPU cores you wanna use) subsets, train N LCS.\nThen “merge”, subsume, remove duplicates, and… Repeat.\nIdeally you would bootstrap the thing with an initial epoch on all the training set, just to have relevant coverage to start with.\nBut there is an issue: What if your dataset is “not perfect”? Let’s suppose there isn’t a clear-cut separation between classes to be found, but rather a fuzzy separation. So there aren’t perfect rules to be discovered. Instead, in each subset, some rules work, but THEN when these are exposed to the complete dataset, the discovered classes in one subset is correct in that subset, but NOT in the overall training data?\nYou end up with classifiers that… Don’t work over test sets.\nAt least that’s my current conclusion after trying it with some messier-than-perfect datasets."
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea-version-2",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea-version-2",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "So if breaking the dataset in sub-sets does not work… What about breaking the data POINTS in different parts?\nIn my earlier example, I have tested (quite successfully, in my view) my RLCS on a simplified MNIST exercise.\nThere, I was training to classify images. The original images where too big, in their equivalent bit-strings representation, for my laptop to work on in acceptable runtimes (for my taste). I ended up compressing said images into 49 bits strings per image. It was then manageable, and it worked quite good, in little over 20 seconds of training (with the correct hyper-parameters).\nBut could I instead break each images in say 7x7 lines, and train on each line separately (and in parallel), and then train one more LCS on the result of that (so 7 outputs would become the input states for a new classifier system, another 7-bits train set, so to speak)?\nI would then in effect, in terms of overall processing time, train approximately twice on 7 bits strings, instead of once on 49 bits strings.\nWould that even work? More on that later.\nBut first…"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-caveat",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-caveat",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "Epistasis!\nWell, at least that’s how I understand it right now: Two (or more) different locations of the “genome” are required to define the phenotype. In my simple RLCS, in that case two or more bits in a state interact in a more or less complex manner so that the class is decided by said interaction. THEN, the second approach would break the assumptions!\nThink of the “Mux6” example explained in this Blog a few entries ago. Two bits pointed to a position, which value was the class to be predicted. In that example, the first two bits pointed to a position in the last 4 bits.\nIF I were to break the 6 bits states in say two sub-strings of 3 bits each, thinking I can learn from each sub-string and then recompose (see “idea version 2” above)… I would fail miserably: you need the complete 6-bit strings in that example for the LCS to come up with the relevant rules!\nThat said…"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#counter-caveat-not-bit-4-in-10-bits-states",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#counter-caveat-not-bit-4-in-10-bits-states",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I’ve shown ad-nauseam by now the “not-bit-4” example. In the example so far, I have used 5 bits strings. So the “search space” was that of 5 bits. And the solution was simple in that one bit would suffice to predict the whole class.\nBut that was arbitrary, and meant to be simple.\nWhat if we were to be faced with the exact same rule to be found in 10-bits strings? The solution would be just as simple (two rules would suffice), but the search space would be comparatively… Well, way bigger.\nIn 5-bits, the not-bit-4 is fully expressed with 32 states.\nIn 10-bits, the SAME not-bit-4 classifier works with… 1024 states (total). For a perfect data mining exercise, you need to find a ruleset that perfectly matches all 1024 states, and you need to try combinations of 3 possible values per-bit (our ternary alphabet: {‘#’, ‘0’, ‘1’}). If I’m correct, that could be up to about 3^10*2 (including class) 59049-1 possible unique classifiers to potentially check (coverage ensures here that not-bit-4 is respected, so that’s only half the overall 10 bits strings + class, as half are illegal, making it a 3^10 matter… And the ‘##########’ option is not valid for us…). Well, approximately anyway.\nAnd let’s remember, an LCS is expected to output a combination subset of these ~59K classifiers as an output System, which could be a very large search space indeed… (In a real world setup, that is.)\nHowever! Well, let’s try to apply the logic of my “approach number 2” above to break down the exercise and then see what would happen in such a simple case…\nImagine I break down the exercise into 3 “trainings”:\n\nThe first 5 bits (which in fact are more than enough) per state, creating \\(RLCS_1\\)\nThe last 5 bits per state (completely irrelevant, but we don’t know that upfront), creating \\(RLCS_2\\)\nThen the outputs of the above to train a third LCS, \\(RLCS_{combined}\\)\n\nThe hope is that we can train the first two in parallel, and use the predictions of these two in combination to then train a third classifier (which incidentally here would be exposed to 2-bits states). So I’d expect to reduce the search space quite a bit. And hence the training times.\nWell, with no further ado, here come the initial results, and heck!\nParallel runtime using the approach described here to break the problem in just two 5-bit string sub-problems, less than 5 seconds:\n\n\n\nbreaking original problem in two makes it fast!\n\n\nCompared to original sequential time on the 10-bit string, more than 4 minutes:\n\n\n\nThat worked, just… Very slow, comparatively\n\n\nThe result however is slightly more difficult to interpret (that is, in the current version of this exercise, which I just wanted to see working, but it is not clean…). In particular, each subset I have to keep (for now) better than chance results (anything &gt; 0.5), but that’s just because it’s a dirty example, it can be made cleaner for sure:\n\n\n\nFast results, but requires a bit more work to interpret\n\n\nStill, from “&gt; 4 minutes” to “&lt; 5 seconds”! But then again, this will only works for such a problem where the solution did exist in a subset of original bit-strings."
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#trick-why-so-fast",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#trick-why-so-fast",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "Well, for one, I am NOT in fact browsing in parallel through 1024 states!\nSee, each sub-string of 5 bits and its class, as a subset can be de-duplicated! There are a lot of repeated instances, say in the first subset, whereby all entries that say “00000 -&gt; class 1” appear 32 times (one for each of the continuing 5-bit strings for the rest of the original state).\nThat kind of deduplication can reduce processing efforts indeed :)"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#conclusion",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#conclusion",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I haven’t tested it yet, but I would wager this approach should work just fine on images too, as described above, breaking images in lines and then combining classifiers…\nParallelizing training is an obvious idea (to me) in concept: For some examples, training is slow, and so running it in N parallel processes must be faster (for, you know, complicated examples).\nBut as explained in some detail today, it’s not that simple! It might work in some cases, as explained, but in the “real world”, we wouldn’t know upfront whether it would in fact work, would we? And so we might have to do some trial-and-error…\nBut even just running tests on new datasets, as explained above, might allow us to:\n\nEither see how we can run fast in parallel on a given dataset and still work out a solution\nOr find out that the dataset cannot be broken the way we have initially chosen, indicating possible “epistasis”, which in itself would be interesting and would have just supposed a few seconds lost here.\nOr – and that could happen – that the chosen dataset cannot be broken down as we would like it to, for an LCS to come up with a proposed classification solution as there is not a very clear-cut solution (think Yahoo finance historical data… Yes, I’ve had a quick look. Suffice to say, I won’t beat the markets just yet :D:D:D But that’s not relevant… )\n\nI’ll work on that parallelizing stuff some more… Even if that’s not part of the package itself, it’s more about data preparation and how to use the package, I am aware. But demonstrating how the future RLCS package can hopefully be used and competitive compared to a Neural Network (for instance) is part of my overall objective, I guess.\nFun stuff (to me) all of it :)"
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html",
    "href": "posts/2024-10-20_Interpolation_Example/index.html",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "",
    "text": "As I want to make sure I keep posting here, I keep looking for things to write about in my own personal “library” (book shelves).\nOne topic I want to make sure I don’t forget about is “Numerical Methods”. (Another one is graph theory, another one is anomaly detection… There are a few topics like that :D).\nAnyhow. Today: Interpolation.\nMore specifically, Lagrange polynomials. Which is just one of many possible approaches (and has its own drawbacks)."
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#intro",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#intro",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "",
    "text": "As I want to make sure I keep posting here, I keep looking for things to write about in my own personal “library” (book shelves).\nOne topic I want to make sure I don’t forget about is “Numerical Methods”. (Another one is graph theory, another one is anomaly detection… There are a few topics like that :D).\nAnyhow. Today: Interpolation.\nMore specifically, Lagrange polynomials. Which is just one of many possible approaches (and has its own drawbacks)."
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#concept-of-interpolation",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#concept-of-interpolation",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Concept of Interpolation",
    "text": "Concept of Interpolation\nSo first, what is “Interpolation”. Most people interested in Statistics and Machine Learning (mostly wrt time series and predicting the future of stock markets, say…) will have heard about Extrapolation. And that’s mostly it: Take a distribution or timeline and “predict” what the future of it should look like. (To keep things simple, that is).\nInterpolation is about finding a distribution in between known points. So not about the “future” (in the above comparison) but instead about the “present”, if you will. It’s about “filling the gaps”. Or trying to 🙂"
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#lagrange-polynomials",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#lagrange-polynomials",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Lagrange Polynomials",
    "text": "Lagrange Polynomials\n“For a given set of N+1 data points, we want to find the coefficients of an Nth-degree polynomial function to match them […]”\n\nLagrange polynomials are somewhat intuitive because each term x_m can rather easily be shown to correspond exactly to y_m (by definition of L_{N, m} above).\nAlright, and the math above leads me to the following R code (my own, but not saying “perfect”, of course :D):\n\n(The source code can be found here)\nAnd to validate that, I take the same example points as that of the reference book, and we can plot the results:\n\nSo we see a POSSIBLE approach here, which “looks” sensible (but MIGHT VERY WELL be wrong)."
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#conclusions",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#conclusions",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Conclusions",
    "text": "Conclusions\nToday was about one of many approaches, a short introduction to the topic of Interpolation (as “opposed”, though not really, to extrapolation).\nThere are several alternatives, namely Newton Polynomials, for which improvements can be obtained by better choosing sample points (Chebyshev nodes). Or the well appreciated Cubic Splines.\nNewton Polynomials for instance work with recursion, and so might come in handy if one expects to ADD new reference data points in the future, because with Lagrange Polynomials, all calculations need to be redone from scratch.\nAll of which might make for a nice few future entries of this blog 🙂"
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#resources",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#resources",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Resources",
    "text": "Resources\n“Applied Numerical Methods Using MatLab, 2nd Edition”, Ed. Wiley, by W. Y. Yang, W. Cao, J. Kim & al."
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html",
    "title": "While on the Train: Cellular Automata",
    "section": "",
    "text": "I decidedly LOVE riding on a train: For whatever reason, it’s one particular moment where I somehow focus a bit more than usual. It’s usually just a few hours, reasonably quiet environment, a bit more spacious than say airplanes (airplanes are just not the same, even with earplugs), no TV to distract my attention, the excuse of limited/imperfect mobile coverage… You name it. I just don’t know, but it works, I feel motivated to code & run experiments while on a train…"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#intro",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#intro",
    "title": "While on the Train: Cellular Automata",
    "section": "",
    "text": "I decidedly LOVE riding on a train: For whatever reason, it’s one particular moment where I somehow focus a bit more than usual. It’s usually just a few hours, reasonably quiet environment, a bit more spacious than say airplanes (airplanes are just not the same, even with earplugs), no TV to distract my attention, the excuse of limited/imperfect mobile coverage… You name it. I just don’t know, but it works, I feel motivated to code & run experiments while on a train…"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#introducing-the-cellular-automata",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#introducing-the-cellular-automata",
    "title": "While on the Train: Cellular Automata",
    "section": "Introducing the Cellular Automata",
    "text": "Introducing the Cellular Automata\nOne thing that I liked about the book I recommended a couple of days back (see last entry before this one) is that it provides nice and easy examples (but in Python), and then reasonable exercises. I loved to read through most of them (and some of the math sections), but reading is not that fun in this case, I wanted to try it out (of course!).\nThe following is the result of a simple R implementation of “Cellular Automata” for simulation of a “fire spread” in a theoretical forest setting. The identification of the “neighbour” trees in fire follows the Von Neumann definition of “neighbours” in this 2D configuration.\nHopefully the video is quite self-explanatory. You start with ONE burning tree, and then let time pass… 🙂"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#visualization-trick",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#visualization-trick",
    "title": "While on the Train: Cellular Automata",
    "section": "Visualization trick",
    "text": "Visualization trick\nOne thing I learnt today is how to quickly draw a matrix into a picture. I hereby recommend you look into the “MBCbook” R package, for its function “imshow()”. Which incidentally I found while looking for alternatives to the Python function… imshow! (So yeah, it was a fast search that one…)"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#conclusions",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#conclusions",
    "title": "While on the Train: Cellular Automata",
    "section": "Conclusions",
    "text": "Conclusions\nI hope you enjoyed it (I could definitely make it faster, and more or less dense a forest, and change the neighbours identification, and what not… I’ve tested this a few times with a few parameters).\nAt least to me, this was for no-good-reason quite… Satisfying 🙂\nAnd I’m not sharing the code (not yet anyway) just because this was a first, horrible pasted-together step by step implementation, full of slow and nested “for loops”, to-be-improved matrix indexing, not enough functions… And well, just not quite “presentable”. Find the code linked below. But it does work just fine though 🙂\nCode on my GitHub account"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "",
    "text": "So as I keep working on an upcoming presentation I shall give in a few weeks, I thought I’d prepare something a bit more “live” than just a PPT with Memes (although who doesn’t like a few Memes in a PPT?).\nMy current idea is to talk about some “Machine Learning Key Concepts”, and then bring it back to Cybersecurity applications.\nSo part of it could be about two key things:\n\nSupervised vs Unsupervised Learning\nSymbolic vs Sub-Symbolic “AI”\n\nWhich I feel help frame a bit what we’re talking about when we talk about Machine Learning.\nToday, we’ll do some review, and then talk about Clustering. Here an output of one such algorithm, in 3D:"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#section",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#section",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "",
    "text": "So as I keep working on an upcoming presentation I shall give in a few weeks, I thought I’d prepare something a bit more “live” than just a PPT with Memes (although who doesn’t like a few Memes in a PPT?).\nMy current idea is to talk about some “Machine Learning Key Concepts”, and then bring it back to Cybersecurity applications.\nSo part of it could be about two key things:\n\nSupervised vs Unsupervised Learning\nSymbolic vs Sub-Symbolic “AI”\n\nWhich I feel help frame a bit what we’re talking about when we talk about Machine Learning.\nToday, we’ll do some review, and then talk about Clustering. Here an output of one such algorithm, in 3D:"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#simplified-definition-of-machine-learning",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#simplified-definition-of-machine-learning",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "Simplified definition of Machine Learning",
    "text": "Simplified definition of Machine Learning\nNow I’m probably NOT the right source for you to learn this, so I really suggest you read about that somewhere else. The wiki puts it a bit like so:\n“The study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.”\nThere are quite a FEW THINGS in that sentence right there. But for today:\n\nIn traditional programming, a person WRITES A PROGRAM, that receives INPUT (say a picture), and generates an OUTPUT (say… “Dog” or “Cat”)\nIn a Machine Learning approach, a person PROVIDES (LOTS OF) INPUTS AND CORRESPONDING OUTPUTS, and the COMPUTER CREATES THE PROGRAM (usually then called a “model”).\n\nThe above is specifically applicable to “Supervised” learning, but nevermind that, the key here is: The computer CREATES the MODEL that we (humans) can then apply to new data."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-unsupervised-learning",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-unsupervised-learning",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "Why Unsupervised Learning?",
    "text": "Why Unsupervised Learning?\nIn Cybersecurity, a relevant part of the job for ML can be about detecting anomalies.\nOften times, you don’t get pre-trained neural networks applicable to your scenario. Or more simple: you don’t have access to relevant “big data” (which would help with training your models, indeed), i.e. people (companies) rarely share detailed data (network packets, logs, configurations, etc.) of their breaches. Understandable.\nAnd so “unsupervised” learning might make sense in that scenario. Unsupervised Learning is about discovering structure in your data, that you might not have known about upfront."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "Warning",
    "text": "Warning\nAlso there are lots of potential issues with leveraging ML, but two possibly relevant ones would be:\n\nImbalance between classes (hopefully you have little data as examples of real attacks on your network, and a LOT of “normal traffic” data, for instance),\nBase-rate fallacy (a SOC analyst might, in certain settings, work most of their time on false-positives)\n\nThat said, let’s keep it simple for today, we will keep it clean, no complications (yet, anyway)."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#symbolic-vs-sub-symbolic",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#symbolic-vs-sub-symbolic",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "Symbolic vs Sub-symbolic",
    "text": "Symbolic vs Sub-symbolic\nSimplifying A LOT, let’s just say for today that “Symbolic” can be read by a Human, and so it could look like sets of rules of the type:\nIF (A & NOT B) THEN (ACTION X)\nWhere a person could read A (“number of Errors in 1′ log file &gt; 100”), B (“less than 10 errors are of type ‘login failed’”), X (“Block originating IP on Firewall”). (Note the negation of B in the expression above ;))\nPutting together many of these rules a person COULD setup a reactive security configuration for a firewall based on monitoring logs. I mean, conceptually, why not?! That would be an “Expert System”, as they called them in the 80’s.\nOh: And you COULD have “Machine Learning” on top of Rule-based systems. For example one interesting field (to me) that somehow has received little attention so far is that of the “Learning Classifier Systems” (LCS)… But that’s for another day.\nWhen you enter the realm of Neural Networks, Dimensionality Reduction (say PCA on TF-IDF), BackPropagation, non-linearity, differentiable functions, etc., you quickly leave the realm of “human readable”, and you enter the world of vectors, matrices, tensors… In these settings, you use numbers, linear algebra, and the concept of distance.\nFor instance, distances between the “calculated class” and “real class” for a set of entries (say, images of cats and dogs, or log files, or…), trying to reduce these distances would mean trying to reduce the prediction error. Said like that, it is probably a bit confusing. But to be perfectly clear: That last sentence is a BIG part of what supervised machine learning with Neural Networks is all about! (More exactly in this case the goal is to minimize the difference between predicted and real class, or predicted and real value)\nLet’s just make a note at this point, then: Sub-symbolic is the domain of neural networks, a world of algebra and calculus, weights and thresholds, which often are hard to translate into “human-readable rules”. And more specifically in the case of the current trend with deep neural networks (which are truly an impressive thing!), it’s a big issue, because there is a problem with how we can UNDERSTAND what the algorithm does. And that introduces things like lack of trust, issues with responsibility, and not being able to explain why something works (or, usually more to the point, why something suddenly DOES NOT work).\nBut the goal here and today is not to explain the details (“why backpropagation expects differentiable activation functions, for gradient descent, and the Chain Rule” – or “why ReLU works so good for training a NNet, but it’s not a differentiable function, and so people use approximations like leaky ReLU”… All that might be a bit much – Me at least, I still often need to come back to my books each time I want to explain these things correctly… So some other time :D).\nToday I’ll focus on the concept of distance between points, and leverage that to identify “clusters” of points (and we’ll mention multi-dimensional spaces real quick)."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#clustering",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#clustering",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "Clustering",
    "text": "Clustering\nOne type of “Unsupervised Learning” is what is called clustering. The main idea is to look at data and to try and create “groups of similar data points”. That’s it. That’s what Clustering is all about.\nRight, but… How?\nSo let’s see:\nIf a = 20, b = 21, c = 99 and d = 100… Would you agree you could possibly say “a is nearer from b than from d”. And iterating, you might conclude:\n\nGroup 1: a, b\nGroup 2: c, d\n\nDoes that make sense? Hopefully YES 🙂\n\n\n\n1d and 3 groups of points\n\n\nLet’s move on to two dimensions. You get a set of points (imagine, for Cybersecurity, I don’t know: for each point representing a machine on a network, the x coordinate represents the number of TCP Packets sent by the machine from its TCP Port 80 in the last minute, and the y coordinate represents the number of TCP Packets sent by the same machine from its port TCP 443 in the last minute).\nSo now you might have two sets of points that “cluster together”, some with very little activity on both axis, that is: (x, y) = 0, and others (maybe only a few), that have a range of numbers but overall have maybe lots of activity as per both axis, so say for example (x &gt; 1000, y &gt; 1000).\nLet’s apply the same logic as above. There will probably be two clusters, one of which might have many points but overlapping, and the other a cloud of points on the top right… Representing Web Servers.\nThat’s probably a very dumb example, but it serves a purpose: You COULD identify groups of machines in these two dimensions. Distances here would probably use Euclidean Distance, but if you understand it visually, good enough for today.\n\n\n\n2d and 2 groups of points\n\n\nAnd with more (very similar) dimensions, you might be able to discover groups of machines that are similar to one another, but a bit different from those of another group…\nHere, I just gave you an algorithm to group machines and separate Windows from Linux, or Clients from Servers, or Web from Mail from LDAP from NTP from DNS servers… Obviously, the above categories are a bit… not great, well because most of the time you will KNOW what the machines are to begin with. But what if all you have to work from is a tcpdump file?\nLet’s visualize this, shall we?"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#dbscan",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#dbscan",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "DBScan",
    "text": "DBScan\nOne (of MANY) algorithms out there to do clustering is DBScan. Its very name says most of it: “Density based Spatial Clustering”.\nI’m not going to reinvent the wheel today, and we’ll go right ahead and leverage the dbscan R package and its documentation. The code is here.\nSo first, we’ll generate a set of seemingly almost-random points in a 2-dimensional space.\n\nVisually, a person can already tell there seems to be some structure in there, some groups. How many might be a bit of a judgement call, but still.\nLeveraging a number of neighbours (say, 4 nearest points to identify a group)  to identify an “elbow” of the separation of the groups, we can set a “noise threshold” to the above whereby if a node is too removed from a group, it could be considered as SEPARATE.\nLet’s see:\n\n\n\nidentifying noise in clustering\n\n\nIn the above, there is a “clear” change in trend in the line that finds said distances, at 0.85 approximately (red line) that identifies regions of LOW DENSITY of points, that the DBScan algorithm would then propose as a limit to separate OUTLIERS from the rest of clustered points.\nIt’s a bit of a mess to write down, but hopefully the results are self explanatory:\n\nHere we color the groups of points by cluster, or what the algorithm has proposed as such. Again, the only concept in use was the distance to other points. A detailed look in the last picture would show that maybe something is amiss, at least one point had x &lt; 0 before, and it doesn’t show up here.\nThat’s an identified outlier.\n\nLet’s take a minute here: We’ve identified stuff that goes together, so “clusters”.\nBut one key aspect (value) of DBScan over some alternative algorithms for clustering is, it can help with ANOMALY DETECTION. Indeed, that’s why I have chosen this algorithm today (the typical intro to clustering would have probably focused on KMeans first :D).\nSo now, we have an “automated ML algorithm” that receives coordinates of points, and is capable of identifying groups of points, AND points that seem to not quite fit anywhere.\nRemember earlier when we mentioned “imbalance” of prevalence of “normal traffic” vs “attack traffic” on a corporate network? Well, this is why I mentioned it. With a little luck, what a DBScan output tells us are “outliers” is something that is UNUSUAL, and HOPEFULLY that’s actually identifying attack-related data for us!\nOK, back to the demos."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-bother",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-bother",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "Why bother?",
    "text": "Why bother?\nOK, so beyond finding things out about your data, the data you have… What if you could get information from new data (of the same kind, that is)?\nAfter all, you’ve identified groups. And that’s cool, and maybe you’ve learned that somehow ten computers seem to behave similarly, and quite differently from another set of 50 computers, on the same network. And maybe that leads you to do some digging and conclude that all 10 of the first group were DB servers, and the other were front-end stuff (Idk, Apache). All from network dump files. Not too bad.\nBut now you receive a new dump file, which you’re told contains network traces from other computers. Wouldn’t it be cool to then just feed that to your “trained” model (which, remember, was actually unsupervised to begin with), and get it to tell you – like a supervised algorithm would: That new machine is a “Group 1” machine (and so you can deduce it’s a DB server).\nI know, I know. Just look at ports, and you would know, fair enough. Plus, it’s not clear the example above would even work (there are MANY considerations in there). Anyhow, let’s take your “pre-trained” model from above, and see what would happen with say 12 new points:\n That’s it: New data, and without you having to look at it, the machine will tell you to which group each entry belongs. That’s the cool thing about Machine Learning 🙂"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#going-3d",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#going-3d",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "Going 3D",
    "text": "Going 3D\nAn almost identical exercise, but in 3 Dimensions. I just want to show it so that we can all agree on one thing: We human can conceptualize up to three dimensions. But with this next visualization, I hope to show one important idea: There is nothing precluding an algorithm from going and work into “higher dimensions”. We can easily visualize groups in 1-D, 2-D, now 3-D (maybe, on a screen, with the help of some animation). But 4-D, or 1000-D, is NOT a problem for a computer!\nOK, so in 3D, same algorithm, similarly random-generated data points. What DBScan can do is shown at the top of this Blog post 🙂\n(I just know people are more impressed by 3-D animations than 2D visuals for some reason, and so I put it at the top to keep you interested :D)"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#its-not-magic",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#its-not-magic",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "It’s NOT magic",
    "text": "It’s NOT magic\nLet’s see a very classical example, and how DBScan kinda’ fails. Not really, but still.\nWhen applied to the “Iris” dataset (if you’ve ever studied a bit of data science, you know what it is), DBScan identifies two clusters, while we all know there are 3 types of flowers represented in the dataset.\nThat does NOT mean that DBScan FAILED. It just means that the information it can tell us about that dataset is that one group of flowers is clearly different from the rest. And that’s OK, although we know it’s insufficient. BUT YOU NEED TO KNOW IT’S NOT MAGIC. From a few data points / coordinates, it’s still only working with so much information…\nReal groups: 3\n\n\n\nreal iris groups\n\n\nDBScan (with selected parameters)groups: 2 (and a few outlier points)\n\n\n\ndbscan identified iris groups"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning-2",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning-2",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "Warning 2",
    "text": "Warning 2\nNOT ALL numbers are ordinals/cardinals. Although 20, 21, 22, 23 might seem nearer from one another than say from 80, 123, or 443… That doesn’t mean you can use THAT “distance”.\nIn Cybersecurity (but in any other field), PLEASE remember DOMAIN KNOWLEDGE can “make or break” a data scientist. And not knowing why things don’t work as you expect then is a bad thing. And it’s not always the algorithm fault.\nWeb is different from NTP, while HTTPS uses cryptography and so does SSH, but… In context, port TCP 80 is NOT nearer TCP port 22 than it is from TCP Port 443.\nYou’ve been warned."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#conclusions",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#conclusions",
    "title": "ML Concepts: Unsupervised Learning – Clustering – DBScan",
    "section": "Conclusions",
    "text": "Conclusions\nUnsupervised Machine Learning has potential for applications to Cybersecurity data. Maybe used on network traffic captures or logs, one can identify structure and propose groupings of machines, users (by their activity, accesses, hours, who knows…).\nAlthough we’ve seen one algorithm and how to visualize its decisions with 2- and 3-dimensional data, the “sky is the limit”, and if one can come up with 100 (or 1000) such dimensions (that’s the concept behind “feature engineering”), there is nothing precluding our machines to work with that and propose groupings for us (although not all algorithms deal nicely with “curse of dimensionality”, but that’s a different topic). In ML, more (quality) data is often a good thing. Also, if one of the 100 dimensions helps us separate perfectly some groups, some ML algorithm will find that and use it for us. Would you visualize manually and study 100 dimensions? 1000?\nAnd that’s where it’s powerful: A person might have a hard time grouping hundreds of machines or users while considering several aspects at once, much less when the number of groups or “kinds of groups” to be found are not known upfront… But that’d be no issue for a computer 🙂\nHopefully I can walk some of my colleagues through the above concepts (organised in a PPT) and show them (in R :P) how all the concepts “work”, and then translate into “real world applications”.\nMaybe next week I’ll move on to making text into multi-dimensional data points. And then, we’d be set to apply all the above to text data. Which is quite prevalent in Cybersecurity (CVE descriptions, logs, code… it’s all text :D).\n\nResources\nWikipedia as linked above, and dbscan R Package documentation, mostly.\nAlso this about making a video from 3D scatter plot with RGL"
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "The exact same code to do supervised learning can be used for data mining. In fact, I’ve already shown that, with the “not-bit-4” example.\nThe approach is simple: Instead of training (and then testing with new data), suppose what you want to understand something about your data. Then you can show it all to the LCS for it to come up with rules. If you’re lucky, your data has hidden rules that you didn’t know about.\n\n\n\nSo we’ve seen it already, the “not-bit-4” example. Well, the LCS can try to find out for you:\n\n\n\nit looks easier like this, doesn’t it?\n\n\nBut what if what you have is this? Not so easy to find the rule manually, right?\n\n\n\nWhat if your data looks like so?\n\n\nMaybe too easy? But it’s the concept that matters! Let’s try a different example. See if you can spot it:\n\n\n\nThis one is just slightly trickier…\n\n\nThe LCS only takes a few second…\n\n\n\nprocessing for simple cases is rather fast\n\n\nAnd finds the rules (4 in this case):\n\n\n\nBit 4 XOR bit 5\n\n\nThat’s how the LCS here expresses “bit 4 XOR bit 5”. And yes, that was it.\nEnough with the examples. But conceptually, the “Mux6” example for the last entry? Same-same.\nAnd more to the point, I had 6 bits inputs here, but what if there are 10 bits? 20 bits? Would you be able to manually find similar rules? It would take me some time :D\nSure, the algorithm is slower with 20 bits instead of 6 (can you blame it?). But it’s machine time. :)\n\n\n\nI actually have a use-case at work I want to test this on. As soon as I upload a first version of my project to GitHub, I’ll be able to test it there. I have inventories and some things about them I want to know whether I can explain with short, readable rules. I think the LCS might help. Although… Maybe there is no simple rule to be found, but that’s worth trying.\nThe trick there will be: How to encode the data into binary strings, as this is what my code accepts…? But I actually know how to go about that.\n\n\n\nWell, I just like the idea that the same Supervised Learning algorithm can be used to do Data mining. And provide interpretable information about some datasets (if there is something to be found, that is).\nTake that, Neural Networks! :D\nAlright, I guess next week I’ll return to working on the package itself."
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#lcs-for-data-mining",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#lcs-for-data-mining",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "The exact same code to do supervised learning can be used for data mining. In fact, I’ve already shown that, with the “not-bit-4” example.\nThe approach is simple: Instead of training (and then testing with new data), suppose what you want to understand something about your data. Then you can show it all to the LCS for it to come up with rules. If you’re lucky, your data has hidden rules that you didn’t know about."
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#demonstration",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#demonstration",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "So we’ve seen it already, the “not-bit-4” example. Well, the LCS can try to find out for you:\n\n\n\nit looks easier like this, doesn’t it?\n\n\nBut what if what you have is this? Not so easy to find the rule manually, right?\n\n\n\nWhat if your data looks like so?\n\n\nMaybe too easy? But it’s the concept that matters! Let’s try a different example. See if you can spot it:\n\n\n\nThis one is just slightly trickier…\n\n\nThe LCS only takes a few second…\n\n\n\nprocessing for simple cases is rather fast\n\n\nAnd finds the rules (4 in this case):\n\n\n\nBit 4 XOR bit 5\n\n\nThat’s how the LCS here expresses “bit 4 XOR bit 5”. And yes, that was it.\nEnough with the examples. But conceptually, the “Mux6” example for the last entry? Same-same.\nAnd more to the point, I had 6 bits inputs here, but what if there are 10 bits? 20 bits? Would you be able to manually find similar rules? It would take me some time :D\nSure, the algorithm is slower with 20 bits instead of 6 (can you blame it?). But it’s machine time. :)"
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#so-what",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#so-what",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "I actually have a use-case at work I want to test this on. As soon as I upload a first version of my project to GitHub, I’ll be able to test it there. I have inventories and some things about them I want to know whether I can explain with short, readable rules. I think the LCS might help. Although… Maybe there is no simple rule to be found, but that’s worth trying.\nThe trick there will be: How to encode the data into binary strings, as this is what my code accepts…? But I actually know how to go about that."
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#conclusion",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#conclusion",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "Well, I just like the idea that the same Supervised Learning algorithm can be used to do Data mining. And provide interpretable information about some datasets (if there is something to be found, that is).\nTake that, Neural Networks! :D\nAlright, I guess next week I’ll return to working on the package itself."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html",
    "title": "Classifying URLs",
    "section": "",
    "text": "I’m still working my way through a “practical demos” session on ML background for Cybersecurity.\nSee I have a “plan” in my head about how to introduce Machine Learning, including demystifying a bit the math behind it. The “plan” goes a bit like this:\n\nWe’ve seen recently a bit of what unsupervised learning can be about. For Cybersecurity, anomaly detection is a clear use-case. We’ve used the concept of distance between points. But there is more to this category (clustering per-se, but also dimensionality reduction is quite useful, for instance).\nML can be about making a line go through points (nicely). That’s regression. With sufficient data, you’re compressing information of relationships between dimensions to predict an outcome. In the simplest form, given x, you can estimate y. And the math is quite simple:\n\\[\ny = \\beta_0 + \\beta_1x\n\\]\nSuch a linear regression is then really of the realm of statistics. As such, there is little of practical application of that thing for us in this simplest form, as in Cybersecurity, many things are expected to be black or white. But maybe it’s not completely crazy to maybe estimate that the more time a machine is connected to the Internet, the higher the risk of infection (all other things being equal, that is). That could be a growing line. Sometimes things can be approximated with the appropriate transformations on the dimensions (say “independent variables”) and then used in a linear regression (so the “model is linear on its parameters”).\n\n\nI’ll make an entry about linear regression soon(-ish), when I have time.\n\n\nML can also be about making a line separate groups of things nicely, i.e. classifying stuff. In its simplest form, we will have input data, and will want to know in which one of two categories a given input should belong too. That’s our “run of the mill” Spam classifier.\n\n\nBoth regression (linear or not) and classification are usually considered to be the two most common supervised learning use-cases.\n\n\nThen you have Reinforcement Learning (aka RL). So far, I have yet to personally come across a real-world cybersecurity application of that, although it is pretty cool a concept! (Actually a friend has demonstrated something with Honeypots and such…) But I’ll admit, I have played little with RL myself for now, and it’s a pending task of mine. It does sound appealing, and there are some papers out there to use RL for optimizing pentesting processes and such.\nAnd then you have Self-Supervised Learning. The first time I read that I was like “Whaaaat?”. But that’s LLMs, as noted at the end of one of my most recent posts. You predict next word in a text, and check what actually comes in the… Rest of the text. So you only need… The text. Nifty. But I’m not too fond of it and have yet to think it through enough to come up with something practical beyond LLMs…\n\nThere is plenty more, but this is one way of putting categories to what ML is about.\n\n\n\nAlright, so I’ll skip some of the above, and today jump right onto this:\nHow does Supervised Machine Learning work, as applied to one cybersecurity-relevant classification use-case?\nSomehow I feel it’s more fun to frame things a little into practical use-cases, instead of pure math (which I suppose is boring/threatening to quite a few).\nEven then, be warned: This entry is pretty long. But hopefully it contains some important things and my wish is, it’s also understandable."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#i-have-a-plan",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#i-have-a-plan",
    "title": "Classifying URLs",
    "section": "",
    "text": "I’m still working my way through a “practical demos” session on ML background for Cybersecurity.\nSee I have a “plan” in my head about how to introduce Machine Learning, including demystifying a bit the math behind it. The “plan” goes a bit like this:\n\nWe’ve seen recently a bit of what unsupervised learning can be about. For Cybersecurity, anomaly detection is a clear use-case. We’ve used the concept of distance between points. But there is more to this category (clustering per-se, but also dimensionality reduction is quite useful, for instance).\nML can be about making a line go through points (nicely). That’s regression. With sufficient data, you’re compressing information of relationships between dimensions to predict an outcome. In the simplest form, given x, you can estimate y. And the math is quite simple:\n\\[\ny = \\beta_0 + \\beta_1x\n\\]\nSuch a linear regression is then really of the realm of statistics. As such, there is little of practical application of that thing for us in this simplest form, as in Cybersecurity, many things are expected to be black or white. But maybe it’s not completely crazy to maybe estimate that the more time a machine is connected to the Internet, the higher the risk of infection (all other things being equal, that is). That could be a growing line. Sometimes things can be approximated with the appropriate transformations on the dimensions (say “independent variables”) and then used in a linear regression (so the “model is linear on its parameters”).\n\n\nI’ll make an entry about linear regression soon(-ish), when I have time.\n\n\nML can also be about making a line separate groups of things nicely, i.e. classifying stuff. In its simplest form, we will have input data, and will want to know in which one of two categories a given input should belong too. That’s our “run of the mill” Spam classifier.\n\n\nBoth regression (linear or not) and classification are usually considered to be the two most common supervised learning use-cases.\n\n\nThen you have Reinforcement Learning (aka RL). So far, I have yet to personally come across a real-world cybersecurity application of that, although it is pretty cool a concept! (Actually a friend has demonstrated something with Honeypots and such…) But I’ll admit, I have played little with RL myself for now, and it’s a pending task of mine. It does sound appealing, and there are some papers out there to use RL for optimizing pentesting processes and such.\nAnd then you have Self-Supervised Learning. The first time I read that I was like “Whaaaat?”. But that’s LLMs, as noted at the end of one of my most recent posts. You predict next word in a text, and check what actually comes in the… Rest of the text. So you only need… The text. Nifty. But I’m not too fond of it and have yet to think it through enough to come up with something practical beyond LLMs…\n\nThere is plenty more, but this is one way of putting categories to what ML is about."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#todays-intro",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#todays-intro",
    "title": "Classifying URLs",
    "section": "",
    "text": "Alright, so I’ll skip some of the above, and today jump right onto this:\nHow does Supervised Machine Learning work, as applied to one cybersecurity-relevant classification use-case?\nSomehow I feel it’s more fun to frame things a little into practical use-cases, instead of pure math (which I suppose is boring/threatening to quite a few).\nEven then, be warned: This entry is pretty long. But hopefully it contains some important things and my wish is, it’s also understandable."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#classifying-urls-into-two-categories",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#classifying-urls-into-two-categories",
    "title": "Classifying URLs",
    "section": "Classifying URLs into TWO categories",
    "text": "Classifying URLs into TWO categories\n\nThe data\nIn Cybersecurity sometimes getting to interesting datasets can be a bit challenging. After all, certain things are usually done behind closed doors. You can probably understand why. Which is why I like for instance this resource, secrepo.com.\nToday, we’re gathering a seemingly simple dataset: A list of web URLs, very simply tagged as either good or bad. Nothing else. But, mind you, 420 thousand of’em.\n\nPoint number one: to do ML, it can help to have lots of data. (It’s not always necessary, but it’s usually a good idea.)\n\n\n\nThe objective\nToday is about trying to distinguish (really just trying!) to classify URLs (“webpages”, for most) in two categories: Good or Bad. Why? Applications are for your protection, and can be used to recommend you to avoid certain websites, which in turn can be maybe used as a supplementary control for other security measures, such as detecting Phishing emails.\nCan we make our computer tell us if a given URL is good or bad?\nThat’s it. That’s our goal for today. Using Machine Learning, of course. So we’re aiming to implement one (or more) model(s) to classify URLs. Based on data we already have. That’s supervised learning, more precisely a classifier.\n\nJust to be very clear: This is all part of a preparation to an introduction on ML background for Cybersecurity. “Introduction” is the key here: I’m not aiming for complete, perfect, not even good, as long as I can convey certain concepts that I believe are relevant to grasp an idea at best of how ML works.\nEven the code I put together is… Well, lacking. It’s not meant to be production grade.\n\nThere is nothing in the way of test-driven anything\nSome of the regular expressions are simplistic\nSome stuff will be badly filtered\nThe trained models are not good\nThe data is what it is, and I’m not trying to complement it\n…\n\nPlease don’t come saying “this is not great”. I know. I only have so much spare time. This post is only my way to support with contents an interactive session I plan to give soon. There is a lot of good training on ML, Cybersecurity & al. out there. Go find it, if you want formal and/or good, detailed training.\nIf you’re fine with simply trying to wrap our heads around concepts, do keep reading.\n\n\n\nThe code\nThe code will be on my Github eventually. But for now, as usual, a few blocks of it:\n\nurls_df &lt;- read.csv(\"https://raw.githubusercontent.com/faizann24/Using-machine-learning-to-detect-malicious-URLs/refs/heads/master/data/data.csv\")\n\nurls_df$url &lt;- tolower(urls_df$url)\n## Let's have a quick look, a.k.a. \"EDA\"\ndim(urls_df)\n&gt; [1] 420464      2\ntable(urls_df$label) ## Imbalance, we might want e.g. to under-sample \"good\"\n&gt;  bad   good \n 75643 344821 \n\n\nPoint number two: Imbalance is often bad. Here we have 4.5 times more good entries than bad entries. Now, why could that be bad? Here we’re going to try to learn from our data. If we keep the imbalance in the dataset, to make things simple, our model could learn that there is more good than bad. And maybe that’s what we want, but then that imbalance could affect the outcome of our model. Unless we want to use that imbalance for some reason, it’s probably best to manage it upfront.\n\nHow to remove imbalance? Well, one way (of surely many out there, only I only know a few), is to “under-sample” some of the over-represented class. Today we’re going to take proportionally less entries from good to train our model, making then sure that we have roughly half and half, of each class.\nAs per the class, it’s a binary choice, good or bad. We’ll create a variable to encode that as 0 or 1 (or the other way around, it’s irrelevant). That’s just to make things compatible with certain models, as most will expect numerical data.\nurls_df$label_num &lt;- ifelse(urls_df$label == \"good\", 0, 1)\nurls_df$url_length &lt;- nchar(urls_df$url)\n\n## A bit of domain knowledge helps:\nnrow(urls_df[urls_df$url_length &gt; 300,]) / nrow(urls_df)\n[1] 0.001203432\n\nPoint number 3: Domain Knowledge is important. We’re going to leverage that today quite a bit. To begin with, we have 0.1% of the entries with URL length superior to 300 characters, and to make things cleaner, we’ll assume today these are… Irrelevant. So we remove them. Our classifier will hence not be trained with such data. And maybe that’s a bad idea, depending on your goals. For today, everything is fair game, we want to keep things simple.\n\n\n\nFeature Engineering\nHeck. We only have URLs. And a class. How is a machine suppose to go from there?\nLet’s try to extract something akin’ to a signal out of that. So we’ve got already the length of each URL. And maybe that’s helpful. Are longer URLs more often bad than good? Well, for real long URLs, maybe a bit. But it’s not really definitive, is it?\n\n\n\nComparing densities of URL lengths per class\n\n\n\nPoint number 4: Always look at the data. Don’t just run into the modelling, it’s not a good idea. Get a feeling of the data you want to work with. I can’t stress this enough.\n\nLet’s keep going then. Again, domain knowledge is key. The good news is, most of us have seen thousands of URLs in our lifetime, so maybe we have a little understanding of what we could look for.\nToo many slashes “/”? Too many “dots”? Maybe. So those could be two new “dimensions”. Although maybe these two are already somewhat expressed through the length of the URL? In other words, it might make sense that the longer the URL, the more dots and slashes.\n\nPoint number 5: That’s a correlation right there, and depending on how much two variables are correlated, maybe you’re better off with fewer variables. There is a lot of background statistics on this topic. And for ML algorithms, sometimes too many variables is a bad thing, more so if they don’t add any useful information.\n\nFor today, we’ll keep it. After all, we have for now only what, 3 variables to work with? We need more. I’m going to save you the pain of going through it all one by one, and propose my own few variables I thought we might consider for training our model, ALL extracted from the URLs themselves.\n\nIP as host: Humans use “Domain Names” that are readable. You need a DNS entry for that, and you need to register things as the owner for the DNS entry, for legal reasons. So if you skip the DNS step, you can still have an IP address, but it will look like… An IP. It’s a bit far-fetched, but I’d argue if a URL reflects a Public IP, it’s either known good (ours or provided by some trusted third party), or - more often than not - it’s a bad indicator.\nURL readability: So it’s not direct. A URL can of course contain stuff that’s purely technical. But we usually make an effort to make things readable: variable names, folder names, etc. Bad actors might want to obfuscate stuff or generate random folder or what-not. And so if a URL is mostly unreadable gibberish, I’d guess it’s a bad sign. Which we can “encode” as: How many vowels has the URL relative to its length? Does the URL contain things with 4 consecutive consonants (not usual in written english, although not good an indicator in some other languages…)? Again, both things are probably somewhat related, but not necessarily/completely. So I take both.\nIs a port expressly identified in the URL? After the host, a column and number is usually not required for a normal website, it’s usually a default (443 or 80). So if you see “somehost.somedomain.com:12345”, something exotic is going on. Exotic for normal web browsing is weird (well, it’s exotic :D), and so not expected for good stuff.\nWe can keep going: Domain extension, file extension (a URL ending in .exe is a red flag, for sure :D), or more simply how common is either of these, is probably helpful too.\n\nIt’s not exhaustive (not in the least) but hopefully it makes some sense. From a URL, we’ve put together 14 different variables that way. All chosen from experience, from “domain knowledge”. (See point number 3 above if it wasn’t clear before.)\n\n\n\nWe should keep looking at our data…\n\n\nFrom no variables (except considering the URL itself…) to 14. Not too shabby.\n&gt; names(urls_df)\n [1] \"url\"                       \"label\"                     \"url_length\"               \n [4] \"label_num\"                 \"slashes_count\"             \"dots_count\"               \n [7] \"host_is_ip\"                \"vowels_prev\"               \"ends_in_slash\"            \n[10] \"contains_port\"             \"n_params\"                  \"domain_ext\"               \n[13] \"file_ext\"                  \"is_common_domain_ext\"      \"is_uncommon_domain_ext\"   \n[16] \"is_uncommon_file_ext\"      \"has_4_consonants_straight\"\nThere is sooo much more to consider.\nFor instance if you check out the code (if/when I make it available on my GitHub), you’ll see at one point I “scale” the data. That is, I try to put all the variable in comparable orders of magnitude. This is to avoid one variable overshadowing all the others. Something that varies from 0.5 to 0.6 might otherwise be considered less important than something that varies from 3 to 4000. Which is not always true.\nI also make a BAD thing: I transform extensions to “factors”, and then I encode the levels of the factors as numerical data. This is not great, I know :D\nNamely, factors are not ordered, while two numbers could be, providing ordinal value at least, and distances could be considered, when here there is clearly no such thing. BAD! BAD Nico!\nLook, this is no excuse, but hopefully, if you order things upfront, and then encode to numerical value, say bad entries as factors first, then good, you end up with ordered levels where by lower ones are for bad, and higher for good (or vice-versa). It will turn out wrong for today. This is tricky and let me insist, NOT good practice. As it turns out, I have so many possible extensions (values) in there, that a better approach - such as one-hot-encoding - makes my dataset explode in size and not fit my RAM memory… And I am just too lazy to work through this for what was meant to be a simple demo. So… My apologies, I know, it hurts the eyes to see this. Moving on.\n\n\nTraining Models\nOne last concept, and we’ll dive in actual “Learning”.\n\nPoint number 6: Save some entries for testing you trained model. So say we have 10K entries, of which 5000 are good and 5000 are bad entries. How do you know your trained model “generalizes” correctly? If you were to try and evaluate your model on data you used to train it, you couldn’t know whether it just learnt exactly that case, or if it would work on future data. To verify how it would work on future data, you… Validate using data not seen during training. There is more to that, too, but that’ll be enough for conceptual understanding today.\n\nOK. At last. As today has been dense (I know, sorry), I’ll train just ONE model on our dataset.\ngood_urls &lt;- urls_df[urls_df$label == \"good\",]\nbad_urls &lt;- urls_df[urls_df$label == \"bad\",]\n## Undersampling \"good\" vs \"bad\"\nsample_urls_df &lt;- rbind(bad_urls[sample(1:nrow(bad_urls), size = 10000,replace = FALSE),],\n                        good_urls[sample(1:nrow(good_urls), size = 10000,replace = FALSE),])\n\n## ...\n\nseparate_sets &lt;- sample(c(TRUE, FALSE), nrow(sample_urls_df), replace=TRUE, prob=c(0.7,0.3))\nt_train &lt;- sample_urls_df[separate_sets, ]\nt_test &lt;- sample_urls_df[!separate_sets, ] # i.e. Not train set...\n\n\nPartitioning Tree, train and test\nHere is how you train a Partitioning Tree in R:\n## A Partitioning tree but WITHOUT the bad trick of extensions encoding\n## And low depth:\ntree_model &lt;- rpart(label ~ url_length + slashes_count + dots_count +\n                        host_is_ip + vowels_prev + ends_in_slash + contains_port +\n                        n_params + is_common_domain_ext + is_uncommon_domain_ext +\n                        is_uncommon_file_ext + has_4_consonants_straight,\n                    data = t_train,\n                    method = \"class\",\n                    control = rpart.control(cp = 0.05))\nAnd here how you visualize, and “test” it:\n&gt; tree_model ; plot(tree_model); text(tree_model)\nn= 13929 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 13929 6933 good (0.4977385 0.5022615)  \n  2) dots_count&gt;=0.3877992 2846  729 bad (0.7438510 0.2561490) *\n  3) dots_count&lt; 0.3877992 11083 4816 good (0.4345394 0.5654606)  \n    6) vowels_prev&lt; -0.6992319 1877  658 bad (0.6494406 0.3505594) *\n    7) vowels_prev&gt;=-0.6992319 9206 3597 good (0.3907234 0.6092766) *\n&gt; t_test$predicted &lt;- predict(tree_model, t_test, type=\"class\")\n&gt; table(t_test[, c(\"label\", \"predicted\")])\n      predicted\nlabel   bad good\n  bad  1431 1636\n  good  611 2393\nNow to the important part: We’ve tested on 30% of the data our model trained on the other 70% of the data. In the above, we’ve also excluded the factor-level-encoded variables because they’re a bad thing (but as we’ll see in a second, they contain useful information, unfortunately). And we got some results, as such:\nBased on the data, we have trained a partitioning tree that makes mistakes about 37% of the time. As we have balanced our dataset, we know that randomly choosing one class of the other would have led us to 50% error, approximately. Still, not great.\nLet’s have a look at this “tree”:\n\n\n\nA simplistic partitioning tree\n\n\nLow depth, and still, with only two choices, we get a 63% correct classification on unseen data.\n\nOne thing to note, I’m not sure that this particular implementation of the model in fact uses Shannon’s information entropy to select nodes (it could use Gini impurity, typically). But suffice to say it could, and that’s one way a Partitioning Tree could decide which variable to choose first to make a separation in two branches, and then iterate. And I only mention it because that was the topic of last week’s entry.\n\nIt does look like the number of “dots” in the URL, and our prevalence of vowels (which I explained a bit earlier) are important to help classify our URLs. Take note! Actually, this is a fair point, Trees are nice because they’re readable by a human. That is, the decisions of this algorithm are explainable, and that’s a good thing.\nNow without further ado, what better models I have managed to produce, just increasing depth and/or adding the (badly encoded) extension variables:\n\nWith just more decisions (more branches in the tree, i.e. more depth), I got my classification to a 75% correct classification rate.\nAdding the (incorrectly) encoded extension variables, I go up to 80%.\n\n\n\n\nA somewhat better tree, albeit using bad practices"
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#conclusions",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#conclusions",
    "title": "Classifying URLs",
    "section": "Conclusions",
    "text": "Conclusions\nLots of theory covered. And only a bit of practical outcome, as today we just have a (first) model that is “better than chance”, although well, far from perfect.\nIn a future post, we’ll probably circle back to this exercise, to see potentially things related to other classification algorithms such as logistic regression, random forests, neural nets, and maybe SVM. Now that most of the theory is covered, it should be shorter, more to the point. (I have them all working already, I just don’t want to add content for today, it’s already too much…)\n\nNote: If I have time, I’ll make a Shiny Application, so that you can test whether or not you can beat this simple (bad) model. Fair warning: I don’t know how the URLs were originally tagged; but I’m not much better than my very own simple partitioning tree model :D"
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#references",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#references",
    "title": "Classifying URLs",
    "section": "References",
    "text": "References\nFor today, only the recommended list of potential datasets for Cybersecurity.\nThe rest is of my own doing. Of course, the Internet, Stack Overflow, Wikipedia, etc. as usual."
  },
  {
    "objectID": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html",
    "href": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html",
    "title": "RLCS Day 1 - Part II: Basic Covering",
    "section": "",
    "text": "So I had a bit of time. And I chose to use it to keep going a bit. I moved a couple of things, created a few more supporting functions, modified them somewhat, and added the covering part.\nCovering needs to happen when for a given training instance (state, class), there is no match in the correct set.\nCorrect set is the subset of the Matching set for which, on top of the condition matching, the predicted class/action is also correct.\nAnd well, it’s working:\n\n\n\ntesting basic covering functionality\n\n\n\n\n\nNext up is “Rule Discovery”, whereby, in Michigan-style LCS, a GA-like process will take place to create offspring from two parents taken in the Correct set.\nThat will include cross-over (in one way or another) and mutation (with a preset pressure/probability). With these, on top of coverage, the LCS will hence create new valid rules, that will become part of the population, to be evaluated on further training instances.\nOther parts of the algorithm – meant to contain the population size – will come later. :)"
  },
  {
    "objectID": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#its-sunday",
    "href": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#its-sunday",
    "title": "RLCS Day 1 - Part II: Basic Covering",
    "section": "",
    "text": "So I had a bit of time. And I chose to use it to keep going a bit. I moved a couple of things, created a few more supporting functions, modified them somewhat, and added the covering part.\nCovering needs to happen when for a given training instance (state, class), there is no match in the correct set.\nCorrect set is the subset of the Matching set for which, on top of the condition matching, the predicted class/action is also correct.\nAnd well, it’s working:\n\n\n\ntesting basic covering functionality"
  },
  {
    "objectID": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#conclusion",
    "href": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#conclusion",
    "title": "RLCS Day 1 - Part II: Basic Covering",
    "section": "",
    "text": "Next up is “Rule Discovery”, whereby, in Michigan-style LCS, a GA-like process will take place to create offspring from two parents taken in the Correct set.\nThat will include cross-over (in one way or another) and mutation (with a preset pressure/probability). With these, on top of coverage, the LCS will hence create new valid rules, that will become part of the population, to be evaluated on further training instances.\nOther parts of the algorithm – meant to contain the population size – will come later. :)"
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html",
    "href": "posts/2024-11-10_entropy_of_zip/index.html",
    "title": "Entropy - Identifying compressed files",
    "section": "",
    "text": "About Shannon’s Information Entropy, applied to potentially detecting ciphered or compressed text compared to plain text.\n(First entry of the new platform, let’s see how it goes.)"
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#shannons-information-entropy",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#shannons-information-entropy",
    "title": "Entropy - Identifying compressed files",
    "section": "Shannon’s Information Entropy",
    "text": "Shannon’s Information Entropy\n\nWhy try to understand that?\nLong story short, Information Entropy is useful in quite a few machine learning algorithms, and to name only a few, the following two use it directly:\n\nPartitioning Trees (for nodes selection)\nLogistic Regression (through log loss)\n\nDoesn’t seem like much, said like that, but the Logistic Regression in turn can be used for… Neural Networks :)\n\n\nHow it is defined?\nThe best way I personally managed to try and understand information entropy is through the concept of compression and surprise.\nA few helpful descriptions:\n“[…] the expected amount of information needed to describe the state of the variable […]”\n“Entropy is the measure of uncertainty of a variable. The more uncertain it is, the higher the entropy is.”\nHere is the mathematical expression of it:\n\\[\nH(X) = - \\sum_{x \\in X} p(x) log(p(x))\n\\]\nFrom the Wikipedia (I mean, why not?), this is the part that somehow can make sense for an intuitive understanding of the concept:\n“The information content, also called the surprisal or self-information, of an event \\(E\\) is a function which increases as the probability \\(p(E)\\) of an event decreases. When \\(p(E)\\) is close to 1, the surprisal of the event is low, but if \\(p(E)\\) is close to 0, the surprisal of the event is high. This relationship is described by the function\n\\[\nlog({1 \\over p(E)})\n\\]\nwhere \\(log()\\) is the logarithm, which gives 0 surprise when the probability of the event is 1. In fact, log is the only function that satisfies а specific set of conditions […]“"
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#application-detecting-cipherzip-on-data-streams",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#application-detecting-cipherzip-on-data-streams",
    "title": "Entropy - Identifying compressed files",
    "section": "Application: Detecting cipher/zip on data streams",
    "text": "Application: Detecting cipher/zip on data streams\nWe’re aiming for this today:\n\n\n\nCharacters distribution in Plain vs Zip text for a few Wiki entries\n\n\n\nThe code\nThe code will be on my Github soon enough (if not already).\nBut for now, a few blocks of it:\n\nmake_freq_df &lt;- function(filename) {\n    test1 &lt;- file(filename, open=\"rb\", raw = TRUE)\n    t1_bytes &lt;- t1_chars &lt;- c()\n    while(TRUE) {\n        temp &lt;- readBin(test1, what = \"raw\")\n        if(length(temp) == 0) break;\n        t1_bytes &lt;- c(t1_bytes, temp)\n        t1_chars &lt;- c(t1_chars, rawToChar(temp))\n    }\n    close(test1)\n    t1_df &lt;- data.frame(sort(table(as.character.hexmode(t1_bytes)), decreasing = TRUE))\n    t1_df$char &lt;- names(sort(table(t1_chars), decreasing = TRUE))\n    names(t1_df) &lt;- c(\"x\", \"probs\", \"char\")\n    # Instead of counts (table output), make it probability:\n    t1_df$probs &lt;- t1_df$probs/sum(t1_df$probs)\n    # Alternative could have been:\n    #t1_df$probs &lt;- as.numeric(prop.table(sort(table(t1_chars), decreasing = TRUE)))\n    \n    t1_df\n}\n\nThe above function is a (bad, but functional) way of taking a file, reading it in “raw” format, and output byte-by-byte into a dataframe.\n\nThe first output column will be the “raw byte” (for text, the ASCII code, say “20” for space character).\nThe second column contains the Probability of appearance of a character, compared to the whole text being analysed (so, the frequency of it’s appearance).\nThe third column is for reference only, to “see” what the character would look like in plain text. Note that ” ” (space) and null would look similar… And so would other encoded bytes, but that’s not to worry for today.\n\nWith the above in mind, here is an output of plain and zip’ed text, along with the Shannon’s Entropy of it, correspondingly:\n&gt; firewall_wiki &lt;- compare_clear_zip(1, wiki_pages_df)\nupdating: posts/2024-11-10_entropy_of_zip/firewall_wiki.txt (deflated 63%)\n   x      probs char\n1 20 0.14766670     \n2 65 0.09267745    e\n3 69 0.07790143    i\n4 74 0.06658562    t\n5 6e 0.06621154    n\n6 61 0.06050687    a\n   x       probs char\n1  0 0.012244898     \n2 39 0.006722689    9\n3 72 0.006722689    r\n4 5f 0.006482593    _\n5 34 0.006242497    4\n6 e4 0.006242497 \\xe4\n[1] \"Entropy Plain text: 4.29839806234458\"\n[1] \"Entropy Zip text: 7.94701914818039\"\n\nIn Plain text, the space character appears quite a bit. So do the letters e, i, t, n, a... (That’s for English, and remember these are small sample texts extracted from some Wikipedia pages…). Plain text has repetition on some characters (higher probability of appearance), with varying distributions (and uses fewer different bytes).\nIn Zip, the probabilities are each MUCH lower, and more even across all possible bytes. And that’s our KEY concept for today. Zip is compression, so all its characters have as few repetition as possible (i.e. low probability).\nInterestingly, with the above approach, ciphered data would look like zip data.\n\nOK, so let’s go back to our definitions of the first part:\n“[…] When \\(p(E)\\) is close to 1, the surprisal of the event is low, but if \\(p(E)\\) is close to 0, the surprisal of the event is high[…]“\nHopefully we’re getting somewhere with understanding the concept. Uncommon characters will have higher “surprisal”, and lower probability of appearing.\nOh: And we should not be afraid of the math, it wasn’t that bad. Here is what Shannon’s Entropy actually looks like for varying values of probability of appearance of a given character:\n\n\n\nShannon Entropy across possible probabilities\n\n\n\n\nWhat does it mean in practice?\nWell, it means that if you sample some bytes sniffed on a network, if you see seemingly random characters and no particular prevalence of any given one over the rest, you know it’s not clear-text.\nAnd yes, if you have the file extension, maybe this is all useless. So why you would care?\nFirst, this is pretty cool. If you sample data (from a network stream, or bytes on a disk…), you can distinguish “automagically” what’s plain text and what’s ciphered/zip.\nSecond: Maybe you can use that to detect covert channels out of packet capture? Or maybe let your computer on its own decide to use one set of algorithm to analyse things when there is plain text, and use another set of characters when there is ciphered/compressed text (or images, etc.)."
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#conclusions",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#conclusions",
    "title": "Entropy - Identifying compressed files",
    "section": "Conclusions",
    "text": "Conclusions\nAll this took me quite a while to really understand it. Or think I do, anyway :D\nToday we’ve tried to explain the concept of information entropy through a simple application. If at this point my readers have gotten somewhat of an intuition about the concept, I’ll be very happy.\nAnd the concept is quite relevant for Machine Learning, as we shall see in future posts."
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#references",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#references",
    "title": "Entropy - Identifying compressed files",
    "section": "References",
    "text": "References\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)\nThe original idea about this post I read a few years back in “Network Security Through Data Analysis”, 2nd Ed, by M. Collins (O`Reilly)"
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "I’ve had my own kaizen-r.com blog for some time, and I relaunched on github.io in November last year.\nOne issue I’m facing is I need to migrate all the old contents, which were not on the same platform… And so I need to manually migrate the contents of the older blog.\n\n\n\nSo one thing that happens is I have made many publications about my last MSc project. As the project is published already, it makes little sense for me to re-produce all those entries.\nSome, I might, but the majority will disappear.\nAnd some more of the old contents is probably… Well, I might be better off if it disappears. Saves me the effort.\n\n\n\nText is just copy-paste, almost.\nThe issue is to re-organize the multimedia (mostly images… And a few videos). That, unfortunately, will not be fast…\n\n\n\nIn short, I’ll need to slowly move contents from the old blog, with some criteria hopefully."
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#migrating-more-of-the-old-contents",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#migrating-more-of-the-old-contents",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "I’ve had my own kaizen-r.com blog for some time, and I relaunched on github.io in November last year.\nOne issue I’m facing is I need to migrate all the old contents, which were not on the same platform… And so I need to manually migrate the contents of the older blog."
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#not-everything-goes",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#not-everything-goes",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "So one thing that happens is I have made many publications about my last MSc project. As the project is published already, it makes little sense for me to re-produce all those entries.\nSome, I might, but the majority will disappear.\nAnd some more of the old contents is probably… Well, I might be better off if it disappears. Saves me the effort."
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#easy-and-slow",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#easy-and-slow",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "Text is just copy-paste, almost.\nThe issue is to re-organize the multimedia (mostly images… And a few videos). That, unfortunately, will not be fast…"
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#conclusions",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#conclusions",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "In short, I’ll need to slowly move contents from the old blog, with some criteria hopefully."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Version “0” is done and working. And I will say this, it is already showing some of its power. Let me show you.\n\n\n\nAlright so this is what I have so far:\n\nA basic, fully functional, Michigan-style LCS for Supervised Learning\nA wrapper makes it easier to call the thing for training\nGA (tournament-based parents selection, one-point cross over reproduction, simple mutation) and Covering for rules discovery\nDeletion was somewhat fixed\nAccuracy and other stats work\nPrinting support functions\nNew: It includes subsumption. But calling subsumption affects performance – both ways – depending on the when/where/how, a known fact. But when used – and that’s an important running decision, by the way –, it condensates the relevant information encoded in the system… Making it, when applicable, much better!\n\n\n\nsubsumption and deletion are costly, but effective to contain the size of the system\n\n\nNew too: Testing can be called now too, which is very helpful to validate the value of the LCS\nThis LCS for SL accepts an environment that, importantly, currently supports only:\n\nBinary classes/actions (so 0 or 1)\nBinary strings as input/states (no more no less) (and I mean, strings)\nAll states must be of equal length (whatever that is)\n\n\nThat’s on the plus side. On the not-so-great, well:\n\nLittle-to-no error control in there yet\nNext-to-zero documentation so far\nAnd I’ll make a note on performance later in this post.\nImperfect/still-a-bit-disorganized code is probably an understatement.\n\nBut that’s just what needs to happen next, I guess. Now on to what you can do with this thing.\n\n\n\nSo I’ve already shown that one, but it wasn’t “perfect”. Here the new version and a few tricks allow me to show how the LCS can be used for data mining.\nSo I give it 70% of the sample space, and it comes up with the following “Classifiers Set”:\n\n\n\nUsing LCS to discover the hidden rules in the dataset\n\n\nNotice how this is quite exactly what the LCS was supposed to try and discover. And also how it indeed tells us what that particular “environment” (dataset and its classes) is all about.\n(Oh, and in this particular exercise, I really did not need to run so many epochs, it’s just my default hyperparameter setting, but after a hundred or so epochs, it was already done.)\nLet’s move to a more interesting example.\n\n\n\nThe “Introduction to Learning Classifier Systems” book – referenced in my first post on LCS – explains the algorithm with (among much more explanations) one particular example, called a “6-bit multiplexer”.\nThe rules of that particular example are like so:\n\nyou get a string of 6 bits as input\nthe first two bits point to an address in the resting 4 bits\nthe “class” of your classifier should be whatever the bit is at the designated address.\n\nA couple of examples:\n\n“010000” says “Bit 2 of the last 4 bits is the class, so 0”\n“110001” says “Bit 4 of the last 4 bits is the class, so 1”\n“101101” says “Bit 3 of the last 4 bits is the class, so 0”\n…\n\nYou get the idea. One aspect of this is meant to show that the LCS SL can somehow recognize things that are interrelated in the data, here how two different parts interact. They call this “epistasis”. For now, suffice to say, it’s a bit more interesting use-case and we hope our LCS can discover relevant rules.\nAgain, with 70% of the whole dataset (64 entries total, for 6 bits), we train our LCS and hope to identify important rules (ideally, up to 8 perfect rules in this case – but that’s not guaranteed). Here I show one such trained Classifier Set (aka “LCS” or “Learning Classifier System”, where the S stands for “System”, implying more than one classifier working together… but I digress) that gets it almost perfect:\n\n\n\nAlmost perfect MUX6 LCS\n\n\nYou have to look somewhat carefully but you should find 7 of the 8 “perfect” classifiers in there. And I haven’t checked for this particular example, but if the training set (randomly selected) did contain all the needed cases (probably did), this LCS should also almost cover for the missing classifier (11###0 0). Hint, the following would be the only one that would match for that missing classifier, so they would participate to recommend the action/class, and see how they all recommend class 0 (…):\n\n1####0 0\n#1#0#0 0\n##00#0 0\n##0#10 0\n\nSo in spite of missing one perfect rule (and that’s also depending on the random environment and non-deterministic nature of the LCS training…), it would work in all testing cases.\nLet’s move on to a (much) harder use-case.\n\n\n\nNow this was a stretch. the MNIST dataset is nothing like the dummy tests above, for sure. First, I don’t know how to manually write a perfect program manually for it (the above cases were trivial, comparatively).\nSecond, many trained neural nets on the MNIST dataset don’t get perfect record (but that’s understandable, if you spend enough time with that dataset).\nThird, and quite important, the “states” here are numerical strings representing the shade (black-white) of 784 pixels (28*28) integers between 0 and 255. With a simple binary transformation I could end up with my states represented each as 6272 binary strings (8 bits per pixel).\nAnd there are a few thousand entries (samples) per represented number.\nI knew upfront, doing thousands of exposures per sample to get anywhere would take forever.\nFair enough, and so I simplified the problem first. Here is what I did:\n\nCompress the data, from shades to 0-1 values per pixel. I used a threshold to “sharpen” the dataset, so only if the value was &gt; 100 (out of 255) I would set my new pixel to 1, otherwise I set it to 0.\nCompress further, from 28*28 to 7*7 pixels. Each original 4-pixels-square is encoded as only 1 pixel, with value 1 if any of the 4*4 subsquare was one, 0 otherwise.\nThe above gives me 7*7 binary strings, so my states are now 49 bits long.\n\n\n\nI “compress” the MNIST data before training my LCS\n\n\nClass stays the same, but I also will do a simpler exercise. My LCS should distinguish between classes 0 and 1 only. (This is not an issue, it’s a choice. I just choose to keep things simple, that’s all, but the value of the approach is the same, think of it as a one-versus-rest approach if you want). This is in part to make things faster, and in (big) part because the code so far only allows two-classes-output only.\n\nLet me skip directly to the results here:\n\n\n\nShowing a couple of runs of an LCS SL on MNIST simplified dataset\n\n\n(BTW, I meant to write “confusion matrix”, not “contention table” above, obviously… Apologies, I hadn’t slept much yesterday…)\nI just want to make this clear:\n\nTraining on a total 500 random (sampled) training inputs only (that’s the size of my environment here), so roughly 250 examples of pictures of Zero and 250 of One\nTraining on these 500 examples takes me between 7.4 and 9.3 minutes (in the above example)\nIt generalizes very well: I end up with an LCS that correctly classified 96 to 97+% of the 3799 sample tests.\n\n\n\nJust FYI, I worked a bit more (31/12) on this and reduced the running times for better results in just above 20 seconds for the mono-thread version:\n\n\n\nBetter speed AND accuracy after slight code update\n\n\nThis gives us in fact 98.6+% of correctly classified test samples after 20s training (still on only 500 training samples)!\n\n\n\n\nOne thing that has worried me the past few days, something I actually knew was a concern with LCS, was performance. I mean speed, here. (SEE EDIT ABOVE before you continue)\nTraining requires exposing the LCS to sufficient training instances to “represent” the overall data. Here I believe, in LCS in particular, too many training instances would hinder training because of the speed of processing power needed. I truly believe better chosen training instances are more important than more training instances. Kinda contrary of what Deep Learning would have you think. Although, this belief is, right now, just that: A belief.\nLCS on the other hand, for each training instance, require a lot of “epochs”, so seeing the same data quite a few times. That is because after covering, the GA kicks in and that happens with choosing what best rules are available, which are evolved over time and where “best” depends on how many matches, how general but specific enough a rule is, whether a rule not only matches but also returns the correct action… The point is, ideally, you want to train your LCS on a dataset, but circle over that dataset many times. I’m talking, thousands of times ideally - for each entry.\nThe other aspect of it is, of course, choosing R as a programming language. As much as I am comfortable with it, it is a slow language, if you consider compiled alternatives and such.\nI wanted to dive a bit into the details of what in my current (basic) implementation I could do, and so I ran the thing through “profvis” quite a few times, separate functions at separate times, then overall, with larger problems, etc.\nOne thing I do a lot, because I’m using lists for everything right now, is use “lapply()” (and al.). I have tried as much as I could to not use dependencies if I could avoid it. So much so, I believe the only actual dependency I have is the plyr package for the rbind() function, and that’s for printing stuff…\nNot having too many dependencies is partly motivated by my end goal of producing a package, which I’d like to make “light” as much as possible.\nAnyhow, so I use lapply() a lot, but I have also used %in% quite a few times. And in some cases, I mix both and check quite a few instances, quite a few times!\nMatching, which happens a lot, for instance, can have an impact:\n\n\n\nProfiling run-times of mono-thread R interpreted code with LOTS of iterations in there…\n\n\n\n\n\nYou might know this by now: I like to parallelize stuff over my CPU cores/threads. It’s just a thing I think about when I see my processing times are… Long. Even if long is a few seconds. And here, we’re talking many iterations, which on bigger datasets/longer states means minutes.\n(Actually, before I decided to simplify to 49 bits the states of the MNIST exercise, I was working with 784 bits states, and well, it would NOT converge after several minutes…)\nWell, I have to keep working on this, but the cool thing is, an LCS can be pre-trained, and then re-trained.\nSo the LCS (in supervised learning setting for now) is nothing but a set of classifiers, which are rules learnt from examples. You can improve it by exposing it, simply, to more examples. Or let it run more iterations over the same examples!\nEither are valid approaches.\nNow… What if I “sharded” my data? Say, if I have 7 CPU cores…\nI could do one shorter run over all the data to generate a first ruleset. I could then use at the same time that pretrained LCS, and what if I trained 7 sub-LCS on 7 subsets of the data for more iterations (quite a few more epochs now, but in parallel, each on smaller data samples)?\nWhat if then I loop over that, with some sort of “compaction” of the 7 newly re-trained sub-LCS and generate a new single base LCS, to then re-distribute to 7 sub processes.\nWhat if I did that a few times?\nI still have to think (seriously, and hard) through all the implications of the simple “algorithm” I just proposed above. I came up with it quite independently, mind you. And maybe it’s a very bad idea. (not showing all the training examples hinders the selection and then generalization of classifiers, for one… So maybe it’s a stupid idea… I just haven’t thought it through yet. For now I do think maybe it does in fact kinda make sense.)\nBUT, I have done it already! One version of my code allows for parallel processing like just described.\nAnd well, in (quite a few) tests with the MNIST example above, for roughly the same overall runtimes (and “compaction” is costly!) I get a gain of 1-2% of correctly classified samples.\nFrom 96% to 97+%. (It might sound marginal, and indeed I’m talking about a LOT more processing, but the same runtimes with marginal gains in such near-100% ranges, sounds like potentially valuable).\nWell, indeed, I’ve done that too:\n\n\n\nCPU-based parallel training of my LCS\n\n\n\n\n\nSo I’ve got all the pieces of the puzzle. And it works. And I have validated some marvellous (to me) results. All in a few days.\nWell, not a “few days”. See, I’ve coded for only a few hours maybe, but I’ve been thinking about this for a long time. And I also think it’d be unfair to say I was fast: Coding time per-se doesn’t account for waking up in the middle of the night coming up with my own design to approach the MNIST example. Waking up in the middle of the night, when I have a project like this, somehow happens more often. I have a bit of insomnia, but these projects make it worse, although they also make me more… Productive.\nI guess what I’m saying is: coding time might not have been too many hours. But thinking time, or “obsessing” (rather) did account for much more than 4 or 5 days (in 4 to 8h/day). I mean, I have designed everything in between the days of coding, and that started long before.\nAnyhow, to the next step: Moving from a few functions and examples to an actual Package!\nThat’s what I’ll be up to when I next have time to dedicate to this.\n\n\n\nWell, that is, indeed, when I next have the time. I’ll be disconnecting for a couple of weeks during January. I simply have other personal priorities (all great).\nSo I’ll take this where I left it maybe a bit later. No matter. I have gone through half the book of packages, and I actually am more convinced than ever that this is the perfect opportunity to create a package for me.\n\n\n\nWell, I am just so… Proud of this thing already!\nSee I had only ever read about the idea until… Well, basically a week ago. Thanks to the holiday season, and long week-end hours, I can now say I have a working LCS in R.\nNot only that:\n\nI know it’s already not too bad in terms of what you could expect about performance. I know (from profiling) where I could try to go RCpp. I also have ideas to run it in more “highly” parallel setups. With more cores on a machine, or… More machines! (plumbeR, here I come… Well, later though, that’s not urgent)\nBut I also know, if I can “encode” a problem into a binary string for binary classification, it turns out LCS are a very valid idea to consider! Just as good as a Neural Network! Heck, in some cases, better!\nI have done some testthat() already, I have several separate functions, very few (almost none) dependencies, the thing is somewhat organized… So the next step comes rather naturally.\n\nAnyhow, overall, I am happy to report, I do believe this (future) package can prove very valuable to the community!\nAnd at this stage, I already hope I can explain that in future forums - I will convince people, I’m sure of it.\nSo this is turning out to be as great a project as I had (unreasonably) imagined already a long time ago. Which, let me tell you, is very satisfying at a very personal level.\n\n\n\nAgain, if some keywords above are confusing, the how of the algorithm is neatly explained in this 12’ video, which I highly recommend."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#intro-imperfect-but-very-cool",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#intro-imperfect-but-very-cool",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Version “0” is done and working. And I will say this, it is already showing some of its power. Let me show you."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#whats-in-it",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#whats-in-it",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Alright so this is what I have so far:\n\nA basic, fully functional, Michigan-style LCS for Supervised Learning\nA wrapper makes it easier to call the thing for training\nGA (tournament-based parents selection, one-point cross over reproduction, simple mutation) and Covering for rules discovery\nDeletion was somewhat fixed\nAccuracy and other stats work\nPrinting support functions\nNew: It includes subsumption. But calling subsumption affects performance – both ways – depending on the when/where/how, a known fact. But when used – and that’s an important running decision, by the way –, it condensates the relevant information encoded in the system… Making it, when applicable, much better!\n\n\n\nsubsumption and deletion are costly, but effective to contain the size of the system\n\n\nNew too: Testing can be called now too, which is very helpful to validate the value of the LCS\nThis LCS for SL accepts an environment that, importantly, currently supports only:\n\nBinary classes/actions (so 0 or 1)\nBinary strings as input/states (no more no less) (and I mean, strings)\nAll states must be of equal length (whatever that is)\n\n\nThat’s on the plus side. On the not-so-great, well:\n\nLittle-to-no error control in there yet\nNext-to-zero documentation so far\nAnd I’ll make a note on performance later in this post.\nImperfect/still-a-bit-disorganized code is probably an understatement.\n\nBut that’s just what needs to happen next, I guess. Now on to what you can do with this thing."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-1-not-bit-4",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-1-not-bit-4",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "So I’ve already shown that one, but it wasn’t “perfect”. Here the new version and a few tricks allow me to show how the LCS can be used for data mining.\nSo I give it 70% of the sample space, and it comes up with the following “Classifiers Set”:\n\n\n\nUsing LCS to discover the hidden rules in the dataset\n\n\nNotice how this is quite exactly what the LCS was supposed to try and discover. And also how it indeed tells us what that particular “environment” (dataset and its classes) is all about.\n(Oh, and in this particular exercise, I really did not need to run so many epochs, it’s just my default hyperparameter setting, but after a hundred or so epochs, it was already done.)\nLet’s move to a more interesting example."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-2-mux6",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-2-mux6",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "The “Introduction to Learning Classifier Systems” book – referenced in my first post on LCS – explains the algorithm with (among much more explanations) one particular example, called a “6-bit multiplexer”.\nThe rules of that particular example are like so:\n\nyou get a string of 6 bits as input\nthe first two bits point to an address in the resting 4 bits\nthe “class” of your classifier should be whatever the bit is at the designated address.\n\nA couple of examples:\n\n“010000” says “Bit 2 of the last 4 bits is the class, so 0”\n“110001” says “Bit 4 of the last 4 bits is the class, so 1”\n“101101” says “Bit 3 of the last 4 bits is the class, so 0”\n…\n\nYou get the idea. One aspect of this is meant to show that the LCS SL can somehow recognize things that are interrelated in the data, here how two different parts interact. They call this “epistasis”. For now, suffice to say, it’s a bit more interesting use-case and we hope our LCS can discover relevant rules.\nAgain, with 70% of the whole dataset (64 entries total, for 6 bits), we train our LCS and hope to identify important rules (ideally, up to 8 perfect rules in this case – but that’s not guaranteed). Here I show one such trained Classifier Set (aka “LCS” or “Learning Classifier System”, where the S stands for “System”, implying more than one classifier working together… but I digress) that gets it almost perfect:\n\n\n\nAlmost perfect MUX6 LCS\n\n\nYou have to look somewhat carefully but you should find 7 of the 8 “perfect” classifiers in there. And I haven’t checked for this particular example, but if the training set (randomly selected) did contain all the needed cases (probably did), this LCS should also almost cover for the missing classifier (11###0 0). Hint, the following would be the only one that would match for that missing classifier, so they would participate to recommend the action/class, and see how they all recommend class 0 (…):\n\n1####0 0\n#1#0#0 0\n##00#0 0\n##0#10 0\n\nSo in spite of missing one perfect rule (and that’s also depending on the random environment and non-deterministic nature of the LCS training…), it would work in all testing cases.\nLet’s move on to a (much) harder use-case."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-3-mnist-with-tricks",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-3-mnist-with-tricks",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Now this was a stretch. the MNIST dataset is nothing like the dummy tests above, for sure. First, I don’t know how to manually write a perfect program manually for it (the above cases were trivial, comparatively).\nSecond, many trained neural nets on the MNIST dataset don’t get perfect record (but that’s understandable, if you spend enough time with that dataset).\nThird, and quite important, the “states” here are numerical strings representing the shade (black-white) of 784 pixels (28*28) integers between 0 and 255. With a simple binary transformation I could end up with my states represented each as 6272 binary strings (8 bits per pixel).\nAnd there are a few thousand entries (samples) per represented number.\nI knew upfront, doing thousands of exposures per sample to get anywhere would take forever.\nFair enough, and so I simplified the problem first. Here is what I did:\n\nCompress the data, from shades to 0-1 values per pixel. I used a threshold to “sharpen” the dataset, so only if the value was &gt; 100 (out of 255) I would set my new pixel to 1, otherwise I set it to 0.\nCompress further, from 28*28 to 7*7 pixels. Each original 4-pixels-square is encoded as only 1 pixel, with value 1 if any of the 4*4 subsquare was one, 0 otherwise.\nThe above gives me 7*7 binary strings, so my states are now 49 bits long.\n\n\n\nI “compress” the MNIST data before training my LCS\n\n\nClass stays the same, but I also will do a simpler exercise. My LCS should distinguish between classes 0 and 1 only. (This is not an issue, it’s a choice. I just choose to keep things simple, that’s all, but the value of the approach is the same, think of it as a one-versus-rest approach if you want). This is in part to make things faster, and in (big) part because the code so far only allows two-classes-output only.\n\nLet me skip directly to the results here:\n\n\n\nShowing a couple of runs of an LCS SL on MNIST simplified dataset\n\n\n(BTW, I meant to write “confusion matrix”, not “contention table” above, obviously… Apologies, I hadn’t slept much yesterday…)\nI just want to make this clear:\n\nTraining on a total 500 random (sampled) training inputs only (that’s the size of my environment here), so roughly 250 examples of pictures of Zero and 250 of One\nTraining on these 500 examples takes me between 7.4 and 9.3 minutes (in the above example)\nIt generalizes very well: I end up with an LCS that correctly classified 96 to 97+% of the 3799 sample tests.\n\n\n\nJust FYI, I worked a bit more (31/12) on this and reduced the running times for better results in just above 20 seconds for the mono-thread version:\n\n\n\nBetter speed AND accuracy after slight code update\n\n\nThis gives us in fact 98.6+% of correctly classified test samples after 20s training (still on only 500 training samples)!"
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#performance",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#performance",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "One thing that has worried me the past few days, something I actually knew was a concern with LCS, was performance. I mean speed, here. (SEE EDIT ABOVE before you continue)\nTraining requires exposing the LCS to sufficient training instances to “represent” the overall data. Here I believe, in LCS in particular, too many training instances would hinder training because of the speed of processing power needed. I truly believe better chosen training instances are more important than more training instances. Kinda contrary of what Deep Learning would have you think. Although, this belief is, right now, just that: A belief.\nLCS on the other hand, for each training instance, require a lot of “epochs”, so seeing the same data quite a few times. That is because after covering, the GA kicks in and that happens with choosing what best rules are available, which are evolved over time and where “best” depends on how many matches, how general but specific enough a rule is, whether a rule not only matches but also returns the correct action… The point is, ideally, you want to train your LCS on a dataset, but circle over that dataset many times. I’m talking, thousands of times ideally - for each entry.\nThe other aspect of it is, of course, choosing R as a programming language. As much as I am comfortable with it, it is a slow language, if you consider compiled alternatives and such.\nI wanted to dive a bit into the details of what in my current (basic) implementation I could do, and so I ran the thing through “profvis” quite a few times, separate functions at separate times, then overall, with larger problems, etc.\nOne thing I do a lot, because I’m using lists for everything right now, is use “lapply()” (and al.). I have tried as much as I could to not use dependencies if I could avoid it. So much so, I believe the only actual dependency I have is the plyr package for the rbind() function, and that’s for printing stuff…\nNot having too many dependencies is partly motivated by my end goal of producing a package, which I’d like to make “light” as much as possible.\nAnyhow, so I use lapply() a lot, but I have also used %in% quite a few times. And in some cases, I mix both and check quite a few instances, quite a few times!\nMatching, which happens a lot, for instance, can have an impact:\n\n\n\nProfiling run-times of mono-thread R interpreted code with LOTS of iterations in there…"
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#nicos-trick-parallelizing",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#nicos-trick-parallelizing",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "You might know this by now: I like to parallelize stuff over my CPU cores/threads. It’s just a thing I think about when I see my processing times are… Long. Even if long is a few seconds. And here, we’re talking many iterations, which on bigger datasets/longer states means minutes.\n(Actually, before I decided to simplify to 49 bits the states of the MNIST exercise, I was working with 784 bits states, and well, it would NOT converge after several minutes…)\nWell, I have to keep working on this, but the cool thing is, an LCS can be pre-trained, and then re-trained.\nSo the LCS (in supervised learning setting for now) is nothing but a set of classifiers, which are rules learnt from examples. You can improve it by exposing it, simply, to more examples. Or let it run more iterations over the same examples!\nEither are valid approaches.\nNow… What if I “sharded” my data? Say, if I have 7 CPU cores…\nI could do one shorter run over all the data to generate a first ruleset. I could then use at the same time that pretrained LCS, and what if I trained 7 sub-LCS on 7 subsets of the data for more iterations (quite a few more epochs now, but in parallel, each on smaller data samples)?\nWhat if then I loop over that, with some sort of “compaction” of the 7 newly re-trained sub-LCS and generate a new single base LCS, to then re-distribute to 7 sub processes.\nWhat if I did that a few times?\nI still have to think (seriously, and hard) through all the implications of the simple “algorithm” I just proposed above. I came up with it quite independently, mind you. And maybe it’s a very bad idea. (not showing all the training examples hinders the selection and then generalization of classifiers, for one… So maybe it’s a stupid idea… I just haven’t thought it through yet. For now I do think maybe it does in fact kinda make sense.)\nBUT, I have done it already! One version of my code allows for parallel processing like just described.\nAnd well, in (quite a few) tests with the MNIST example above, for roughly the same overall runtimes (and “compaction” is costly!) I get a gain of 1-2% of correctly classified samples.\nFrom 96% to 97+%. (It might sound marginal, and indeed I’m talking about a LOT more processing, but the same runtimes with marginal gains in such near-100% ranges, sounds like potentially valuable).\nWell, indeed, I’ve done that too:\n\n\n\nCPU-based parallel training of my LCS"
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#next-steps-packaging",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#next-steps-packaging",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "So I’ve got all the pieces of the puzzle. And it works. And I have validated some marvellous (to me) results. All in a few days.\nWell, not a “few days”. See, I’ve coded for only a few hours maybe, but I’ve been thinking about this for a long time. And I also think it’d be unfair to say I was fast: Coding time per-se doesn’t account for waking up in the middle of the night coming up with my own design to approach the MNIST example. Waking up in the middle of the night, when I have a project like this, somehow happens more often. I have a bit of insomnia, but these projects make it worse, although they also make me more… Productive.\nI guess what I’m saying is: coding time might not have been too many hours. But thinking time, or “obsessing” (rather) did account for much more than 4 or 5 days (in 4 to 8h/day). I mean, I have designed everything in between the days of coding, and that started long before.\nAnyhow, to the next step: Moving from a few functions and examples to an actual Package!\nThat’s what I’ll be up to when I next have time to dedicate to this."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#a-few-days-off",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#a-few-days-off",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Well, that is, indeed, when I next have the time. I’ll be disconnecting for a couple of weeks during January. I simply have other personal priorities (all great).\nSo I’ll take this where I left it maybe a bit later. No matter. I have gone through half the book of packages, and I actually am more convinced than ever that this is the perfect opportunity to create a package for me."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#conclusion",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#conclusion",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Well, I am just so… Proud of this thing already!\nSee I had only ever read about the idea until… Well, basically a week ago. Thanks to the holiday season, and long week-end hours, I can now say I have a working LCS in R.\nNot only that:\n\nI know it’s already not too bad in terms of what you could expect about performance. I know (from profiling) where I could try to go RCpp. I also have ideas to run it in more “highly” parallel setups. With more cores on a machine, or… More machines! (plumbeR, here I come… Well, later though, that’s not urgent)\nBut I also know, if I can “encode” a problem into a binary string for binary classification, it turns out LCS are a very valid idea to consider! Just as good as a Neural Network! Heck, in some cases, better!\nI have done some testthat() already, I have several separate functions, very few (almost none) dependencies, the thing is somewhat organized… So the next step comes rather naturally.\n\nAnyhow, overall, I am happy to report, I do believe this (future) package can prove very valuable to the community!\nAnd at this stage, I already hope I can explain that in future forums - I will convince people, I’m sure of it.\nSo this is turning out to be as great a project as I had (unreasonably) imagined already a long time ago. Which, let me tell you, is very satisfying at a very personal level."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#references",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#references",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Again, if some keywords above are confusing, the how of the algorithm is neatly explained in this 12’ video, which I highly recommend."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "Recently, I had “broken” my Reinforcement Learning implementation, in the sense that it would learn at first, and then it would get worse with more experience, instead of better.\nWith so many hyper-parameters to consider, I knew I had touched too many levers while trying to improve…\nOne key thing was my exploration prevalence. But I didn’t know that. I start training with high exploration (1 step in each 2). That’s meant to expose to the maximum possible situations at first. In the smaller worlds, that should cover more environment states, and so reward of different actions would be learnt early on. That was the idea behind my choice, anyway. It kind of made sense…\nBut then I knew of course that too much exploration would mean… Worse rewards.\nThat was correct, but a few more things were happening… The short list here:\n\nmy reward update function per classifier of the LCS was not great\nand my choosing of best classifiers (influencing the algorithm step of going from match-set to action-set) was in fact all wrong too 🤦‍♂️\n\nTheory to the rescue!\n\n\n\nI bought the “bible of RL” (see Resources below) and recently received it at home. And it’s already making a difference! (Actually, having a physical copy was a great choice for me.)\nI have now corrected the above issues, but inspired by only the first two chapters (the second one really, as I had glanced at the first one earlier…) I was able to implement 2 different reward-update functions that (along with exploration pressure increase) made all the difference.\nPlus I now have initialized my rewards with a high positive value to foment more exploration at the earlier stages.\n\n\n\nTo emit a judgement on the quality of my implementation, along with the right mix of hyper-parameters (then again, I’m not saying they’re optimal, I’m saying they’re OK)…\nI had to sit down and choose what would demonstrate “learning” in my small scenario.\nSo I kept things very simple:\n\n1 agent\n4 actions corresponding to the 4 directions (horizontal and vertical only)\nAt all steps, the agent can see empty cells, wall cells, food cells, enemy cells and where it came from (allocentric perception forced me to do that), 1 cell in any direction (including diagonals).\n\nIn a simple scenario, 5x5 world, there are at any point: 16 wall cells, 5 empty cells, 1 food, 1 enemy, 1 walk-back.\n\n\n\na simple world to study learning quality\n\n\nGiven that, I think this demonstrates how good the agent is after some training:\n\n\n\nlearning with sample average reward update\n\n\nThe above is using a “sample average” reward update function, whereby the reward contribution gets smaller and smaller with time.\nThis learns the best exploitation policy in certain scenarios where there is a unique perfect policy. However, in our case, given the limited agent perception, one could consider that it would be best to keep learning over time.\nThat is because the “food” and the “enemy” positions change once the agent collides with them. Although in the longer run that is probably incorrect, there are many possible perceived states even in this very small scenario (and in spite of the LCS and its GA pressuring to learn of each scenario only the relevant part of the states…), we could think that with limited learning steps, an option that ensures the agent keeps learning in a non-stationary environment will be better off.\nWell, so I implemented the equations with fixed alpha (set to 0.1).\nNote that in all the above, I keep starting training with one exploratory step in each 2, then reduce exploration every 2000 time-steps, until I get to 1 in 6. That forces exploration too, with more pressure at the beginning.\nWith the new rules this might be in fact unnecessary, I am aware, but I still haven’t tested with fixed exploration pressure (say always 1 in 6 in my example). It should work!\nAnyhow, the results with fixed alpha are almost identical in my simple scenario:\n\n\n\nlearning with fixed alpha\n\n\n\n\n\nWell, we’ve seen there are:\n\nMore wall cells (16) than empty cells (5)\nmore empty cells (5) than enemy, food and “walk-back” cell (of which there is one)\n\nAnd yet, after some training, the agent averages, over 1000 most recent steps taken:\n\n275 food eaten (most rewarding)\n3 enemy encounters (most penalized)\nless than 100 wall bumps (heavily penalized)\nabout 150 walk-backs (slightly penalized)\nand well, quite a bit of walking around in empty cells\n\nI haven’t done the math, but in some of the perceived states:\n\nCorner: 5 walls/8 total cells. There are 4 corners. 62.5%\nSide: 3 walls/8. 37.5%\nThe above account for 8 of 9 possible agent positions (exceptuating center cell)\n\nAnd yet, only 10% of the time the agent bumps into a wall on average (and that’s with a 17% chance of exploratory move!).\nAs the agent has very limited state perception, I believe this is proof right there that it has indeed learnt something.\nI’d have to run many more numbers (4 moves possible, 9 locations, 5 different possible cells-status per cell, in different combinations… I’m not doing that until someone asks me to do a PhD on the topic 😁)\nSimilar numbers could demonstrate that there are lots of food-related actions compared to white cell prevalence…\nAnd indeed, supposing the identical probability of encountering food or enemy at any stage, the fact that our agent has chosen the right action when faced with both possibilities, choosing food 275 times for only 3 times the enemy (again, exploration step 1 in 6 in all described statistics above), well…\nI’m sold.\n\n\n\nI’m very happy about it all. The Reinforcement Learning example I chose is working nicely! And it makes sense, based on the little theory I have studied thus far.\n(Note: I shall actually post a short entry on real-world application of data mining with my LCS implementation. Any day now, it has happened already…).\n\n\n\n“Reinforcement Learning, An introduction.”, 2nd Ed. Bradford Books, by R. S. Sutton & A. G. Bardo."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#using-theory-is-proving-useful-indeed",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#using-theory-is-proving-useful-indeed",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "Recently, I had “broken” my Reinforcement Learning implementation, in the sense that it would learn at first, and then it would get worse with more experience, instead of better.\nWith so many hyper-parameters to consider, I knew I had touched too many levers while trying to improve…\nOne key thing was my exploration prevalence. But I didn’t know that. I start training with high exploration (1 step in each 2). That’s meant to expose to the maximum possible situations at first. In the smaller worlds, that should cover more environment states, and so reward of different actions would be learnt early on. That was the idea behind my choice, anyway. It kind of made sense…\nBut then I knew of course that too much exploration would mean… Worse rewards.\nThat was correct, but a few more things were happening… The short list here:\n\nmy reward update function per classifier of the LCS was not great\nand my choosing of best classifiers (influencing the algorithm step of going from match-set to action-set) was in fact all wrong too 🤦‍♂️\n\nTheory to the rescue!"
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#i-only-have-gone-through-chapter-1-2-and-already",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#i-only-have-gone-through-chapter-1-2-and-already",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "I bought the “bible of RL” (see Resources below) and recently received it at home. And it’s already making a difference! (Actually, having a physical copy was a great choice for me.)\nI have now corrected the above issues, but inspired by only the first two chapters (the second one really, as I had glanced at the first one earlier…) I was able to implement 2 different reward-update functions that (along with exploration pressure increase) made all the difference.\nPlus I now have initialized my rewards with a high positive value to foment more exploration at the earlier stages."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#results",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#results",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "To emit a judgement on the quality of my implementation, along with the right mix of hyper-parameters (then again, I’m not saying they’re optimal, I’m saying they’re OK)…\nI had to sit down and choose what would demonstrate “learning” in my small scenario.\nSo I kept things very simple:\n\n1 agent\n4 actions corresponding to the 4 directions (horizontal and vertical only)\nAt all steps, the agent can see empty cells, wall cells, food cells, enemy cells and where it came from (allocentric perception forced me to do that), 1 cell in any direction (including diagonals).\n\nIn a simple scenario, 5x5 world, there are at any point: 16 wall cells, 5 empty cells, 1 food, 1 enemy, 1 walk-back.\n\n\n\na simple world to study learning quality\n\n\nGiven that, I think this demonstrates how good the agent is after some training:\n\n\n\nlearning with sample average reward update\n\n\nThe above is using a “sample average” reward update function, whereby the reward contribution gets smaller and smaller with time.\nThis learns the best exploitation policy in certain scenarios where there is a unique perfect policy. However, in our case, given the limited agent perception, one could consider that it would be best to keep learning over time.\nThat is because the “food” and the “enemy” positions change once the agent collides with them. Although in the longer run that is probably incorrect, there are many possible perceived states even in this very small scenario (and in spite of the LCS and its GA pressuring to learn of each scenario only the relevant part of the states…), we could think that with limited learning steps, an option that ensures the agent keeps learning in a non-stationary environment will be better off.\nWell, so I implemented the equations with fixed alpha (set to 0.1).\nNote that in all the above, I keep starting training with one exploratory step in each 2, then reduce exploration every 2000 time-steps, until I get to 1 in 6. That forces exploration too, with more pressure at the beginning.\nWith the new rules this might be in fact unnecessary, I am aware, but I still haven’t tested with fixed exploration pressure (say always 1 in 6 in my example). It should work!\nAnyhow, the results with fixed alpha are almost identical in my simple scenario:\n\n\n\nlearning with fixed alpha"
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#what-do-the-above-screenshot-say",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#what-do-the-above-screenshot-say",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "Well, we’ve seen there are:\n\nMore wall cells (16) than empty cells (5)\nmore empty cells (5) than enemy, food and “walk-back” cell (of which there is one)\n\nAnd yet, after some training, the agent averages, over 1000 most recent steps taken:\n\n275 food eaten (most rewarding)\n3 enemy encounters (most penalized)\nless than 100 wall bumps (heavily penalized)\nabout 150 walk-backs (slightly penalized)\nand well, quite a bit of walking around in empty cells\n\nI haven’t done the math, but in some of the perceived states:\n\nCorner: 5 walls/8 total cells. There are 4 corners. 62.5%\nSide: 3 walls/8. 37.5%\nThe above account for 8 of 9 possible agent positions (exceptuating center cell)\n\nAnd yet, only 10% of the time the agent bumps into a wall on average (and that’s with a 17% chance of exploratory move!).\nAs the agent has very limited state perception, I believe this is proof right there that it has indeed learnt something.\nI’d have to run many more numbers (4 moves possible, 9 locations, 5 different possible cells-status per cell, in different combinations… I’m not doing that until someone asks me to do a PhD on the topic 😁)\nSimilar numbers could demonstrate that there are lots of food-related actions compared to white cell prevalence…\nAnd indeed, supposing the identical probability of encountering food or enemy at any stage, the fact that our agent has chosen the right action when faced with both possibilities, choosing food 275 times for only 3 times the enemy (again, exploration step 1 in 6 in all described statistics above), well…\nI’m sold."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#conclusions",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#conclusions",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "I’m very happy about it all. The Reinforcement Learning example I chose is working nicely! And it makes sense, based on the little theory I have studied thus far.\n(Note: I shall actually post a short entry on real-world application of data mining with my LCS implementation. Any day now, it has happened already…)."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#resources",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#resources",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "“Reinforcement Learning, An introduction.”, 2nd Ed. Bradford Books, by R. S. Sutton & A. G. Bardo."
  },
  {
    "objectID": "posts/2024-11-08_welcome/index.html",
    "href": "posts/2024-11-08_welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in this future Blog. Currently testing all kinds of things, will updated hopefully shortly!\nSeems like it will nicely manage some defaults.\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2024-11-08_welcome/index.html#references-for-future-use",
    "href": "posts/2024-11-08_welcome/index.html#references-for-future-use",
    "title": "Welcome To My Blog",
    "section": "References for future use",
    "text": "References for future use\nFor now, I need to keep references of what allowed me to get here:\n\nManage GitHub access\nhttps://usethis.r-lib.org/articles/git-credentials.html\n\n\nSet things up\nhttps://sites.northwestern.edu/researchcomputing/2022/05/11/git-with-rstudio-order-matters/\nhttps://sites.northwestern.edu/researchcomputing/resources/using-git-and-github-with-r-rstudio/\nhttps://ucsb-meds.github.io/creating-quarto-websites/"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html",
    "href": "posts/2025-03-19_ExplainableAI/index.html",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "I have been open about some of my worries with GenAI (and in fact Deep Learning with Neural Networks).\nOne of these worries had to do with the fact that DeepLearning outputs, while very precise in the immense majority of cases, are never easily interpreted. In other words, one cannot easily understand why a (deep) neural network (in the supervised learning case, for instance) makes the choices it makes.\nThey say “If you talk the talk… Walk the walk”. Well, here goes my own personal attempt at just that.\n\n\n\nI’ve shown in the past, how for data mining the rules themselves are showing how a classifier (separately, each of the classifier systems) actually “shows” where to put our attention (see https://kaizen-r.github.io/posts/2025-01-25_RLCS_4_DataMining/ for instance).\nWell, today we’ll see how using LCS outcome can be used itself directly to help interpret the algorithm’s own choices.\nAnd how that can be important.\nAnd to be perfectly clear, this was one of the key reasons why I got interested in LCS as a family of algorithms in the first place!\n\n\n\nIf you use a neural network trained to classify numbers, say on MNIST dataset, it will work. Most assuredly!\nAnd training it can be faster than with the LCS (although, that is… not that clear. More on that some other time).\nBut more importantly: although the neural network output will work, you will have a hard time understanding why it chose one class or another. (At least, without SHAP, LIME, etc.)\nWell, I present to you, interpreting LCS output as images classifier:\n\n\n\nLCS are expressive in their choices!\n\n\nA few things happen here: First, I take MNIST, but as explained in the past, to contain the processing needs, I compress the input image first (that’s not part of the LCS algorithm 😁). Once I’m down to a 7x7 binary image, I use that as my state and I train the LCS with an environment of several such “states” (i.e. compressed pictures).\nDifferent from a Neural Network, I do not need to train on that many examples (also already clarified in the past). So I train on only 500 samples, chosen randomly, and here I train for quite some time (almost 7 minutes, but single-thread, mind you) on said 500 samples.\nAs an output, I get a set of a few thousand “classifiers” (&lt; 2500 in today’s example), all of which I have presented in the past also and won’t explain here again. Together, they form the System of classifiers.\nBut just for reference, I got a 99% correctly classified testing samples (out of 3799 of them, none of which had been seen for training).\n\n\n\nQuality of generated classifier\n\n\nNow on to the explainable part!\n\n\n\nNow tell me: Does the following set of visuals help in actually interpreting the LCS recommendation? (hint: I think it does!)\nI’ve been trying to visualize why the LCS choices are indeed explainable. Of course, matching several rules makes it more confusing… But each rule that matches is a vote. If you can visualize the “ensemble” of rules/classifiers, all of which must have matched the sample, in our case hopefully you can see what the LCS is using as subset of reference for a given image.\nHere an example, whereby a Zero image is correctly classified, among other reasons because it needs to have “lines around” and one “empty pixel” in the center! That’s enough for the LCS to decide that was a zero, and probably very few of the 156 rules that matched that test sample (all of which agreed on the correct class) would have been sufficient to explain the choice!\nIn other words, I try to show the ensemble and its choice in such a way that you see why the LCS decided on class 0 for that sample.\nGood luck doing that with a Neural network!\n\n\n\nClass 0 - Visualized\n\n\n\n\n\nExplainable AI is important for many reasons.\nBecause of the very nature of ML (in that sense different from other approaches), sometimes (as rarely as it might be, 1% of the cases in today’s exercise) it makes a classification mistake. There are several possible reasons for that, but often it has to do with problems with generalization (overfitting).\nRemember we used 500 samples for training, and have tested with 99% correct results on 3800 test samples!\n(Note that part of the trick here is that my compressing the original images has removed more subtle differences that might have otherwise required better generalization… Fair enough!)\nBut I do hope in the exercise for today, the visualization of the LCS choices is helpful and indeed explainable.\nAt the very least, I feel it’s important to know, when an error is detected.\n\n\n\nResistance to Adversarial attacks!\nOne important issue with some CNNs for instance for images classification is that they are sensible to a type of attack whereby someone can add noise to a picture, invisible to the human eye, and force the classifier to err the classification.\nThis will not happen with LCS, as I hope the visuals I introduced today clearly show. And that’s pretty cool, too.\n\n\n\nFor our exercise today, a few (1%) of the examples were wrongly classified, and in all cases the confidence of the proposed classification was below 65%.\nAs the LCS output is a “system of classifiers”, whereby a new sample matches several “rules”, and given in our exercise the high level of compression with loss of information, it was no surprise that some examples were difficult based on the compressed image (I wouldn’t have been able to do any better myself, without the original image).\n\n\n\nLook at the compressed sample used, you’ll understand the confusion of the LCS\n\n\nIn summary, they matched several classifiers, some of which were voting for one class, while the rest for the other… Which is reflected in the low confidence of the outcome!\nAnd overall, my code for LCS did a great job, given what they see as (compressed) input.\n\n\n\n\n\n\nClass 0 - other example\n\n\nIt doesn’t always happen, but here again a central empty cell is quite self-explanatory. Moreover, one can tell the line needs to spread horizontally for the set of classifiers to agree on a Zero class, here. (That happens more often indeed)\n\n\n\nClass 1 - an example\n\n\nYou can see in the above, how the width of the line is constrained by the “empty cells” recommendations (“Pixel=0”). Clearly the line for the one is apparent (“Pixel=1”). And the resulting visual shows (green) empty cells surrounding (maroon) full cells, indeed.\n\n\n\nLast example for today\n\n\n\n\n\nLook, I’m not kidding myself. I will not magically shift the focus of the whole AI community towards the LCS algorithm, overnight.\nPlus, it’s not a perfect algorithm. Many hyper-parameters to consider, not particularly fast (although… to be discussed)…\nBut it does work, requires (at least in today’s example) relatively few (ideally well chosen) samples for training, and is clear in why it makes the decisions it makes, which as explained is (at least from my perspective) very important.\nI do hope, however, that maybe it shows there are alternatives to neural networks, and if nothing else, I would expect this exercise might be considered useful, as a complementary approach: Keep using your neural networks, deep learning and what-not, but maybe you can use an LCS on the side, to confirm (with a level of confidence included, and visuals that express the why) the neural net choices…\nAnd although I still have a lot to do, I hope when it’s ready, my RLCS package will participate in making that a reality :)"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#foreword",
    "href": "posts/2025-03-19_ExplainableAI/index.html#foreword",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "I have been open about some of my worries with GenAI (and in fact Deep Learning with Neural Networks).\nOne of these worries had to do with the fact that DeepLearning outputs, while very precise in the immense majority of cases, are never easily interpreted. In other words, one cannot easily understand why a (deep) neural network (in the supervised learning case, for instance) makes the choices it makes.\nThey say “If you talk the talk… Walk the walk”. Well, here goes my own personal attempt at just that."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#lcs-outputs-can-be-interpreted",
    "href": "posts/2025-03-19_ExplainableAI/index.html#lcs-outputs-can-be-interpreted",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "I’ve shown in the past, how for data mining the rules themselves are showing how a classifier (separately, each of the classifier systems) actually “shows” where to put our attention (see https://kaizen-r.github.io/posts/2025-01-25_RLCS_4_DataMining/ for instance).\nWell, today we’ll see how using LCS outcome can be used itself directly to help interpret the algorithm’s own choices.\nAnd how that can be important.\nAnd to be perfectly clear, this was one of the key reasons why I got interested in LCS as a family of algorithms in the first place!"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#explainable-ai",
    "href": "posts/2025-03-19_ExplainableAI/index.html#explainable-ai",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "If you use a neural network trained to classify numbers, say on MNIST dataset, it will work. Most assuredly!\nAnd training it can be faster than with the LCS (although, that is… not that clear. More on that some other time).\nBut more importantly: although the neural network output will work, you will have a hard time understanding why it chose one class or another. (At least, without SHAP, LIME, etc.)\nWell, I present to you, interpreting LCS output as images classifier:\n\n\n\nLCS are expressive in their choices!\n\n\nA few things happen here: First, I take MNIST, but as explained in the past, to contain the processing needs, I compress the input image first (that’s not part of the LCS algorithm 😁). Once I’m down to a 7x7 binary image, I use that as my state and I train the LCS with an environment of several such “states” (i.e. compressed pictures).\nDifferent from a Neural Network, I do not need to train on that many examples (also already clarified in the past). So I train on only 500 samples, chosen randomly, and here I train for quite some time (almost 7 minutes, but single-thread, mind you) on said 500 samples.\nAs an output, I get a set of a few thousand “classifiers” (&lt; 2500 in today’s example), all of which I have presented in the past also and won’t explain here again. Together, they form the System of classifiers.\nBut just for reference, I got a 99% correctly classified testing samples (out of 3799 of them, none of which had been seen for training).\n\n\n\nQuality of generated classifier\n\n\nNow on to the explainable part!"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#the-visuals",
    "href": "posts/2025-03-19_ExplainableAI/index.html#the-visuals",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Now tell me: Does the following set of visuals help in actually interpreting the LCS recommendation? (hint: I think it does!)\nI’ve been trying to visualize why the LCS choices are indeed explainable. Of course, matching several rules makes it more confusing… But each rule that matches is a vote. If you can visualize the “ensemble” of rules/classifiers, all of which must have matched the sample, in our case hopefully you can see what the LCS is using as subset of reference for a given image.\nHere an example, whereby a Zero image is correctly classified, among other reasons because it needs to have “lines around” and one “empty pixel” in the center! That’s enough for the LCS to decide that was a zero, and probably very few of the 156 rules that matched that test sample (all of which agreed on the correct class) would have been sufficient to explain the choice!\nIn other words, I try to show the ensemble and its choice in such a way that you see why the LCS decided on class 0 for that sample.\nGood luck doing that with a Neural network!\n\n\n\nClass 0 - Visualized"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#why-is-this-good-12",
    "href": "posts/2025-03-19_ExplainableAI/index.html#why-is-this-good-12",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Explainable AI is important for many reasons.\nBecause of the very nature of ML (in that sense different from other approaches), sometimes (as rarely as it might be, 1% of the cases in today’s exercise) it makes a classification mistake. There are several possible reasons for that, but often it has to do with problems with generalization (overfitting).\nRemember we used 500 samples for training, and have tested with 99% correct results on 3800 test samples!\n(Note that part of the trick here is that my compressing the original images has removed more subtle differences that might have otherwise required better generalization… Fair enough!)\nBut I do hope in the exercise for today, the visualization of the LCS choices is helpful and indeed explainable.\nAt the very least, I feel it’s important to know, when an error is detected."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#why-is-it-good-22",
    "href": "posts/2025-03-19_ExplainableAI/index.html#why-is-it-good-22",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Resistance to Adversarial attacks!\nOne important issue with some CNNs for instance for images classification is that they are sensible to a type of attack whereby someone can add noise to a picture, invisible to the human eye, and force the classifier to err the classification.\nThis will not happen with LCS, as I hope the visuals I introduced today clearly show. And that’s pretty cool, too."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#locating-errors",
    "href": "posts/2025-03-19_ExplainableAI/index.html#locating-errors",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "For our exercise today, a few (1%) of the examples were wrongly classified, and in all cases the confidence of the proposed classification was below 65%.\nAs the LCS output is a “system of classifiers”, whereby a new sample matches several “rules”, and given in our exercise the high level of compression with loss of information, it was no surprise that some examples were difficult based on the compressed image (I wouldn’t have been able to do any better myself, without the original image).\n\n\n\nLook at the compressed sample used, you’ll understand the confusion of the LCS\n\n\nIn summary, they matched several classifiers, some of which were voting for one class, while the rest for the other… Which is reflected in the low confidence of the outcome!\nAnd overall, my code for LCS did a great job, given what they see as (compressed) input."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#a-few-more-examples-for-no-reason",
    "href": "posts/2025-03-19_ExplainableAI/index.html#a-few-more-examples-for-no-reason",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Class 0 - other example\n\n\nIt doesn’t always happen, but here again a central empty cell is quite self-explanatory. Moreover, one can tell the line needs to spread horizontally for the set of classifiers to agree on a Zero class, here. (That happens more often indeed)\n\n\n\nClass 1 - an example\n\n\nYou can see in the above, how the width of the line is constrained by the “empty cells” recommendations (“Pixel=0”). Clearly the line for the one is apparent (“Pixel=1”). And the resulting visual shows (green) empty cells surrounding (maroon) full cells, indeed.\n\n\n\nLast example for today"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#conclusions",
    "href": "posts/2025-03-19_ExplainableAI/index.html#conclusions",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Look, I’m not kidding myself. I will not magically shift the focus of the whole AI community towards the LCS algorithm, overnight.\nPlus, it’s not a perfect algorithm. Many hyper-parameters to consider, not particularly fast (although… to be discussed)…\nBut it does work, requires (at least in today’s example) relatively few (ideally well chosen) samples for training, and is clear in why it makes the decisions it makes, which as explained is (at least from my perspective) very important.\nI do hope, however, that maybe it shows there are alternatives to neural networks, and if nothing else, I would expect this exercise might be considered useful, as a complementary approach: Keep using your neural networks, deep learning and what-not, but maybe you can use an LCS on the side, to confirm (with a level of confidence included, and visuals that express the why) the neural net choices…\nAnd although I still have a lot to do, I hope when it’s ready, my RLCS package will participate in making that a reality :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kaizen-R.com new home",
    "section": "",
    "text": "RLCS & Explainable AI\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nMar 19, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS for RL: It works again (using books helps)\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nMar 16, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating Contents (slowly)\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 14, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Improving… And getting worse (at the same time!)\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nMar 9, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Reinforcement Learning: World Explorer v1\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nFeb 16, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: A World to Play RL\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Reinforcement Learning?\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: training in parallel?\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nJan 26, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: for Data Mining\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nJan 25, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 4.5 - Fully Functional v0 and New Tests\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 2 - Full Rule Discovery\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 24, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 1 - Part II: Basic Covering\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 1: Rules formatting, storage, and matching\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nA new project\n\n\n\n\n\n\nML\n\n\nnews\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 8, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Linear Regression\n\n\n\n\n\n\nML\n\n\nmath\n\n\n\n\n\n\n\n\n\nNov 25, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying URLs\n\n\n\n\n\n\nML\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 17, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nEntropy - Identifying compressed files\n\n\n\n\n\n\nmath\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 10, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nML Concepts: Word Embeddings\n\n\n\n\n\n\nML\n\n\nNLP\n\n\n\n\n\n\n\n\n\nNov 2, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nML Concepts: Unsupervised Learning – Clustering – DBScan\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Exercises: Interpolation with Lagrange Polynomials\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nOct 20, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nWhile on the Train: Cellular Automata\n\n\n\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nNico\n\n\n\n\n\n\nNo matching items"
  }
]