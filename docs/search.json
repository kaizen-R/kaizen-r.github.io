[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html",
    "href": "posts/2025-03-19_ExplainableAI/index.html",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "I have been open about some of my worries with GenAI (and in fact Deep Learning with Neural Networks).\nOne of these worries had to do with the fact that DeepLearning outputs, while very precise in the immense majority of cases, are never easily interpreted. In other words, one cannot easily understand why a (deep) neural network (in the supervised learning case, for instance) makes the choices it makes.\nThey say ‚ÄúIf you talk the talk‚Ä¶ Walk the walk‚Äù. Well, here goes my own personal attempt at just that.\n\n\n\nI‚Äôve shown in the past, how for data mining the rules themselves are showing how a classifier (separately, each of the classifier systems) actually ‚Äúshows‚Äù where to put our attention (see https://kaizen-r.github.io/posts/2025-01-25_RLCS_4_DataMining/ for instance).\nWell, today we‚Äôll see how using LCS outcome can be used itself directly to help interpret the algorithm‚Äôs own choices.\nAnd how that can be important.\nAnd to be perfectly clear, this was one of the key reasons why I got interested in LCS as a family of algorithms in the first place!\n\n\n\nIf you use a neural network trained to classify numbers, say on MNIST dataset, it will work. Most assuredly!\nAnd training it can be faster than with the LCS (although, that is‚Ä¶ not that clear. More on that some other time).\nBut more importantly: although the neural network output will work, you will have a hard time understanding why it chose one class or another. (At least, without SHAP, LIME, etc.)\nWell, I present to you, interpreting LCS output as images classifier:\n\n\n\nLCS are expressive in their choices!\n\n\nA few things happen here: First, I take MNIST, but as explained in the past, to contain the processing needs, I compress the input image first (that‚Äôs not part of the LCS algorithm üòÅ). Once I‚Äôm down to a 7x7 binary image, I use that as my state and I train the LCS with an environment of several such ‚Äústates‚Äù (i.e.¬†compressed pictures).\nDifferent from a Neural Network, I do not need to train on that many examples (also already clarified in the past). So I train on only 500 samples, chosen randomly, and here I train for quite some time (almost 7 minutes, but single-thread, mind you) on said 500 samples.\nAs an output, I get a set of a few thousand ‚Äúclassifiers‚Äù (&lt; 2500 in today‚Äôs example), all of which I have presented in the past also and won‚Äôt explain here again. Together, they form the System of classifiers.\nBut just for reference, I got a 99% correctly classified testing samples (out of 3799 of them, none of which had been seen for training).\n\n\n\nQuality of generated classifier\n\n\nNow on to the explainable part!\n\n\n\nNow tell me: Does the following set of visuals help in actually interpreting the LCS recommendation? (hint: I think it does!)\nI‚Äôve been trying to visualize why the LCS choices are indeed explainable. Of course, matching several rules makes it more confusing‚Ä¶ But each rule that matches is a vote. If you can visualize the ‚Äúensemble‚Äù of rules/classifiers, all of which must have matched the sample, in our case hopefully you can see what the LCS is using as subset of reference for a given image.\nHere an example, whereby a Zero image is correctly classified, among other reasons because it needs to have ‚Äúlines around‚Äù and one ‚Äúempty pixel‚Äù in the center! That‚Äôs enough for the LCS to decide that was a zero, and probably very few of the 156 rules that matched that test sample (all of which agreed on the correct class) would have been sufficient to explain the choice!\nIn other words, I try to show the ensemble and its choice in such a way that you see why the LCS decided on class 0 for that sample.\nGood luck doing that with a Neural network!\n\n\n\nClass 0 - Visualized\n\n\n\n\n\nExplainable AI is important for many reasons.\nBecause of the very nature of ML (in that sense different from other approaches), sometimes (as rarely as it might be, 1% of the cases in today‚Äôs exercise) it makes a classification mistake. There are several possible reasons for that, but often it has to do with problems with generalization (overfitting).\nRemember we used 500 samples for training, and have tested with 99% correct results on 3800 test samples!\n(Note that part of the trick here is that my compressing the original images has removed more subtle differences that might have otherwise required better generalization‚Ä¶ Fair enough!)\nBut I do hope in the exercise for today, the visualization of the LCS choices is helpful and indeed explainable.\nAt the very least, I feel it‚Äôs important to know, when an error is detected.\n\n\n\nResistance to Adversarial attacks!\nOne important issue with some CNNs for instance for images classification is that they are sensible to a type of attack whereby someone can add noise to a picture, invisible to the human eye, and force the classifier to err the classification.\nThis will not happen with LCS, as I hope the visuals I introduced today clearly show. And that‚Äôs pretty cool, too.\n\n\n\nFor our exercise today, a few (1%) of the examples were wrongly classified, and in all cases the confidence of the proposed classification was below 65%.\nAs the LCS output is a ‚Äúsystem of classifiers‚Äù, whereby a new sample matches several ‚Äúrules‚Äù, and given in our exercise the high level of compression with loss of information, it was no surprise that some examples were difficult based on the compressed image (I wouldn‚Äôt have been able to do any better myself, without the original image).\n\n\n\nLook at the compressed sample used, you‚Äôll understand the confusion of the LCS\n\n\nIn summary, they matched several classifiers, some of which were voting for one class, while the rest for the other‚Ä¶ Which is reflected in the low confidence of the outcome!\nAnd overall, my code for LCS did a great job, given what they see as (compressed) input.\n\n\n\n\n\n\nClass 0 - other example\n\n\nIt doesn‚Äôt always happen, but here again a central empty cell is quite self-explanatory. Moreover, one can tell the line needs to spread horizontally for the set of classifiers to agree on a Zero class, here. (That happens more often indeed)\n\n\n\nClass 1 - an example\n\n\nYou can see in the above, how the width of the line is constrained by the ‚Äúempty cells‚Äù recommendations (‚ÄúPixel=0‚Äù). Clearly the line for the one is apparent (‚ÄúPixel=1‚Äù). And the resulting visual shows (green) empty cells surrounding (maroon) full cells, indeed.\n\n\n\nLast example for today\n\n\n\n\n\nLook, I‚Äôm not kidding myself. I will not magically shift the focus of the whole AI community towards the LCS algorithm, overnight.\nPlus, it‚Äôs not a perfect algorithm. Many hyper-parameters to consider, not particularly fast (although‚Ä¶ to be discussed)‚Ä¶\nBut it does work, requires (at least in today‚Äôs example) relatively few (ideally well chosen) samples for training, and is clear in why it makes the decisions it makes, which as explained is (at least from my perspective) very important.\nI do hope, however, that maybe it shows there are alternatives to neural networks, and if nothing else, I would expect this exercise might be considered useful, as a complementary approach: Keep using your neural networks, deep learning and what-not, but maybe you can use an LCS on the side, to confirm (with a level of confidence included, and visuals that express the why) the neural net choices‚Ä¶\nAnd although I still have a lot to do, I hope when it‚Äôs ready, my RLCS package will participate in making that a reality :)"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#foreword",
    "href": "posts/2025-03-19_ExplainableAI/index.html#foreword",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "I have been open about some of my worries with GenAI (and in fact Deep Learning with Neural Networks).\nOne of these worries had to do with the fact that DeepLearning outputs, while very precise in the immense majority of cases, are never easily interpreted. In other words, one cannot easily understand why a (deep) neural network (in the supervised learning case, for instance) makes the choices it makes.\nThey say ‚ÄúIf you talk the talk‚Ä¶ Walk the walk‚Äù. Well, here goes my own personal attempt at just that."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#lcs-outputs-can-be-interpreted",
    "href": "posts/2025-03-19_ExplainableAI/index.html#lcs-outputs-can-be-interpreted",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "I‚Äôve shown in the past, how for data mining the rules themselves are showing how a classifier (separately, each of the classifier systems) actually ‚Äúshows‚Äù where to put our attention (see https://kaizen-r.github.io/posts/2025-01-25_RLCS_4_DataMining/ for instance).\nWell, today we‚Äôll see how using LCS outcome can be used itself directly to help interpret the algorithm‚Äôs own choices.\nAnd how that can be important.\nAnd to be perfectly clear, this was one of the key reasons why I got interested in LCS as a family of algorithms in the first place!"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#explainable-ai",
    "href": "posts/2025-03-19_ExplainableAI/index.html#explainable-ai",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "If you use a neural network trained to classify numbers, say on MNIST dataset, it will work. Most assuredly!\nAnd training it can be faster than with the LCS (although, that is‚Ä¶ not that clear. More on that some other time).\nBut more importantly: although the neural network output will work, you will have a hard time understanding why it chose one class or another. (At least, without SHAP, LIME, etc.)\nWell, I present to you, interpreting LCS output as images classifier:\n\n\n\nLCS are expressive in their choices!\n\n\nA few things happen here: First, I take MNIST, but as explained in the past, to contain the processing needs, I compress the input image first (that‚Äôs not part of the LCS algorithm üòÅ). Once I‚Äôm down to a 7x7 binary image, I use that as my state and I train the LCS with an environment of several such ‚Äústates‚Äù (i.e.¬†compressed pictures).\nDifferent from a Neural Network, I do not need to train on that many examples (also already clarified in the past). So I train on only 500 samples, chosen randomly, and here I train for quite some time (almost 7 minutes, but single-thread, mind you) on said 500 samples.\nAs an output, I get a set of a few thousand ‚Äúclassifiers‚Äù (&lt; 2500 in today‚Äôs example), all of which I have presented in the past also and won‚Äôt explain here again. Together, they form the System of classifiers.\nBut just for reference, I got a 99% correctly classified testing samples (out of 3799 of them, none of which had been seen for training).\n\n\n\nQuality of generated classifier\n\n\nNow on to the explainable part!"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#the-visuals",
    "href": "posts/2025-03-19_ExplainableAI/index.html#the-visuals",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Now tell me: Does the following set of visuals help in actually interpreting the LCS recommendation? (hint: I think it does!)\nI‚Äôve been trying to visualize why the LCS choices are indeed explainable. Of course, matching several rules makes it more confusing‚Ä¶ But each rule that matches is a vote. If you can visualize the ‚Äúensemble‚Äù of rules/classifiers, all of which must have matched the sample, in our case hopefully you can see what the LCS is using as subset of reference for a given image.\nHere an example, whereby a Zero image is correctly classified, among other reasons because it needs to have ‚Äúlines around‚Äù and one ‚Äúempty pixel‚Äù in the center! That‚Äôs enough for the LCS to decide that was a zero, and probably very few of the 156 rules that matched that test sample (all of which agreed on the correct class) would have been sufficient to explain the choice!\nIn other words, I try to show the ensemble and its choice in such a way that you see why the LCS decided on class 0 for that sample.\nGood luck doing that with a Neural network!\n\n\n\nClass 0 - Visualized"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#why-is-this-good-12",
    "href": "posts/2025-03-19_ExplainableAI/index.html#why-is-this-good-12",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Explainable AI is important for many reasons.\nBecause of the very nature of ML (in that sense different from other approaches), sometimes (as rarely as it might be, 1% of the cases in today‚Äôs exercise) it makes a classification mistake. There are several possible reasons for that, but often it has to do with problems with generalization (overfitting).\nRemember we used 500 samples for training, and have tested with 99% correct results on 3800 test samples!\n(Note that part of the trick here is that my compressing the original images has removed more subtle differences that might have otherwise required better generalization‚Ä¶ Fair enough!)\nBut I do hope in the exercise for today, the visualization of the LCS choices is helpful and indeed explainable.\nAt the very least, I feel it‚Äôs important to know, when an error is detected."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#why-is-it-good-22",
    "href": "posts/2025-03-19_ExplainableAI/index.html#why-is-it-good-22",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Resistance to Adversarial attacks!\nOne important issue with some CNNs for instance for images classification is that they are sensible to a type of attack whereby someone can add noise to a picture, invisible to the human eye, and force the classifier to err the classification.\nThis will not happen with LCS, as I hope the visuals I introduced today clearly show. And that‚Äôs pretty cool, too."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#locating-errors",
    "href": "posts/2025-03-19_ExplainableAI/index.html#locating-errors",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "For our exercise today, a few (1%) of the examples were wrongly classified, and in all cases the confidence of the proposed classification was below 65%.\nAs the LCS output is a ‚Äúsystem of classifiers‚Äù, whereby a new sample matches several ‚Äúrules‚Äù, and given in our exercise the high level of compression with loss of information, it was no surprise that some examples were difficult based on the compressed image (I wouldn‚Äôt have been able to do any better myself, without the original image).\n\n\n\nLook at the compressed sample used, you‚Äôll understand the confusion of the LCS\n\n\nIn summary, they matched several classifiers, some of which were voting for one class, while the rest for the other‚Ä¶ Which is reflected in the low confidence of the outcome!\nAnd overall, my code for LCS did a great job, given what they see as (compressed) input."
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#a-few-more-examples-for-no-reason",
    "href": "posts/2025-03-19_ExplainableAI/index.html#a-few-more-examples-for-no-reason",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Class 0 - other example\n\n\nIt doesn‚Äôt always happen, but here again a central empty cell is quite self-explanatory. Moreover, one can tell the line needs to spread horizontally for the set of classifiers to agree on a Zero class, here. (That happens more often indeed)\n\n\n\nClass 1 - an example\n\n\nYou can see in the above, how the width of the line is constrained by the ‚Äúempty cells‚Äù recommendations (‚ÄúPixel=0‚Äù). Clearly the line for the one is apparent (‚ÄúPixel=1‚Äù). And the resulting visual shows (green) empty cells surrounding (maroon) full cells, indeed.\n\n\n\nLast example for today"
  },
  {
    "objectID": "posts/2025-03-19_ExplainableAI/index.html#conclusions",
    "href": "posts/2025-03-19_ExplainableAI/index.html#conclusions",
    "title": "RLCS & Explainable AI",
    "section": "",
    "text": "Look, I‚Äôm not kidding myself. I will not magically shift the focus of the whole AI community towards the LCS algorithm, overnight.\nPlus, it‚Äôs not a perfect algorithm. Many hyper-parameters to consider, not particularly fast (although‚Ä¶ to be discussed)‚Ä¶\nBut it does work, requires (at least in today‚Äôs example) relatively few (ideally well chosen) samples for training, and is clear in why it makes the decisions it makes, which as explained is (at least from my perspective) very important.\nI do hope, however, that maybe it shows there are alternatives to neural networks, and if nothing else, I would expect this exercise might be considered useful, as a complementary approach: Keep using your neural networks, deep learning and what-not, but maybe you can use an LCS on the side, to confirm (with a level of confidence included, and visuals that express the why) the neural net choices‚Ä¶\nAnd although I still have a lot to do, I hope when it‚Äôs ready, my RLCS package will participate in making that a reality :)"
  },
  {
    "objectID": "posts/2024-11-08_welcome/index.html",
    "href": "posts/2024-11-08_welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in this future Blog. Currently testing all kinds of things, will updated hopefully shortly!\nSeems like it will nicely manage some defaults.\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2024-11-08_welcome/index.html#references-for-future-use",
    "href": "posts/2024-11-08_welcome/index.html#references-for-future-use",
    "title": "Welcome To My Blog",
    "section": "References for future use",
    "text": "References for future use\nFor now, I need to keep references of what allowed me to get here:\n\nManage GitHub access\nhttps://usethis.r-lib.org/articles/git-credentials.html\n\n\nSet things up\nhttps://sites.northwestern.edu/researchcomputing/2022/05/11/git-with-rstudio-order-matters/\nhttps://sites.northwestern.edu/researchcomputing/resources/using-git-and-github-with-r-rstudio/\nhttps://ucsb-meds.github.io/creating-quarto-websites/"
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "Recently, I had ‚Äúbroken‚Äù my Reinforcement Learning implementation, in the sense that it would learn at first, and then it would get worse with more experience, instead of better.\nWith so many hyper-parameters to consider, I knew I had touched too many levers while trying to improve‚Ä¶\nOne key thing was my exploration prevalence. But I didn‚Äôt know that. I start training with high exploration (1 step in each 2). That‚Äôs meant to expose to the maximum possible situations at first. In the smaller worlds, that should cover more environment states, and so reward of different actions would be learnt early on. That was the idea behind my choice, anyway. It kind of made sense‚Ä¶\nBut then I knew of course that too much exploration would mean‚Ä¶ Worse rewards.\nThat was correct, but a few more things were happening‚Ä¶ The short list here:\n\nmy reward update function per classifier of the LCS was not great\nand my choosing of best classifiers (influencing the algorithm step of going from match-set to action-set) was in fact all wrong too ü§¶‚Äç‚ôÇÔ∏è\n\nTheory to the rescue!\n\n\n\nI bought the ‚Äúbible of RL‚Äù (see Resources below) and recently received it at home. And it‚Äôs already making a difference! (Actually, having a physical copy was a great choice for me.)\nI have now corrected the above issues, but inspired by only the first two chapters (the second one really, as I had glanced at the first one earlier‚Ä¶) I was able to implement 2 different reward-update functions that (along with exploration pressure increase) made all the difference.\nPlus I now have initialized my rewards with a high positive value to foment more exploration at the earlier stages.\n\n\n\nTo emit a judgement on the quality of my implementation, along with the right mix of hyper-parameters (then again, I‚Äôm not saying they‚Äôre optimal, I‚Äôm saying they‚Äôre OK)‚Ä¶\nI had to sit down and choose what would demonstrate ‚Äúlearning‚Äù in my small scenario.\nSo I kept things very simple:\n\n1 agent\n4 actions corresponding to the 4 directions (horizontal and vertical only)\nAt all steps, the agent can see empty cells, wall cells, food cells, enemy cells and where it came from (allocentric perception forced me to do that), 1 cell in any direction (including diagonals).\n\nIn a simple scenario, 5x5 world, there are at any point: 16 wall cells, 5 empty cells, 1 food, 1 enemy, 1 walk-back.\n\n\n\na simple world to study learning quality\n\n\nGiven that, I think this demonstrates how good the agent is after some training:\n\n\n\nlearning with sample average reward update\n\n\nThe above is using a ‚Äúsample average‚Äù reward update function, whereby the reward contribution gets smaller and smaller with time.\nThis learns the best exploitation policy in certain scenarios where there is a unique perfect policy. However, in our case, given the limited agent perception, one could consider that it would be best to keep learning over time.\nThat is because the ‚Äúfood‚Äù and the ‚Äúenemy‚Äù positions change once the agent collides with them. Although in the longer run that is probably incorrect, there are many possible perceived states even in this very small scenario (and in spite of the LCS and its GA pressuring to learn of each scenario only the relevant part of the states‚Ä¶), we could think that with limited learning steps, an option that ensures the agent keeps learning in a non-stationary environment will be better off.\nWell, so I implemented the equations with fixed alpha (set to 0.1).\nNote that in all the above, I keep starting training with one exploratory step in each 2, then reduce exploration every 2000 time-steps, until I get to 1 in 6. That forces exploration too, with more pressure at the beginning.\nWith the new rules this might be in fact unnecessary, I am aware, but I still haven‚Äôt tested with fixed exploration pressure (say always 1 in 6 in my example). It should work!\nAnyhow, the results with fixed alpha are almost identical in my simple scenario:\n\n\n\nlearning with fixed alpha\n\n\n\n\n\nWell, we‚Äôve seen there are:\n\nMore wall cells (16) than empty cells (5)\nmore empty cells (5) than enemy, food and ‚Äúwalk-back‚Äù cell (of which there is one)\n\nAnd yet, after some training, the agent averages, over 1000 most recent steps taken:\n\n275 food eaten (most rewarding)\n3 enemy encounters (most penalized)\nless than 100 wall bumps (heavily penalized)\nabout 150 walk-backs (slightly penalized)\nand well, quite a bit of walking around in empty cells\n\nI haven‚Äôt done the math, but in some of the perceived states:\n\nCorner: 5 walls/8 total cells. There are 4 corners. 62.5%\nSide: 3 walls/8. 37.5%\nThe above account for 8 of 9 possible agent positions (exceptuating center cell)\n\nAnd yet, only 10% of the time the agent bumps into a wall on average (and that‚Äôs with a 17% chance of exploratory move!).\nAs the agent has very limited state perception, I believe this is proof right there that it has indeed learnt something.\nI‚Äôd have to run many more numbers (4 moves possible, 9 locations, 5 different possible cells-status per cell, in different combinations‚Ä¶ I‚Äôm not doing that until someone asks me to do a PhD on the topic üòÅ)\nSimilar numbers could demonstrate that there are lots of food-related actions compared to white cell prevalence‚Ä¶\nAnd indeed, supposing the identical probability of encountering food or enemy at any stage, the fact that our agent has chosen the right action when faced with both possibilities, choosing food 275 times for only 3 times the enemy (again, exploration step 1 in 6 in all described statistics above), well‚Ä¶\nI‚Äôm sold.\n\n\n\nI‚Äôm very happy about it all. The Reinforcement Learning example I chose is working nicely! And it makes sense, based on the little theory I have studied thus far.\n(Note: I shall actually post a short entry on real-world application of data mining with my LCS implementation. Any day now, it has happened already‚Ä¶).\n\n\n\n‚ÄúReinforcement Learning, An introduction.‚Äù, 2nd Ed. Bradford Books, by R. S. Sutton & A. G. Bardo."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#using-theory-is-proving-useful-indeed",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#using-theory-is-proving-useful-indeed",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "Recently, I had ‚Äúbroken‚Äù my Reinforcement Learning implementation, in the sense that it would learn at first, and then it would get worse with more experience, instead of better.\nWith so many hyper-parameters to consider, I knew I had touched too many levers while trying to improve‚Ä¶\nOne key thing was my exploration prevalence. But I didn‚Äôt know that. I start training with high exploration (1 step in each 2). That‚Äôs meant to expose to the maximum possible situations at first. In the smaller worlds, that should cover more environment states, and so reward of different actions would be learnt early on. That was the idea behind my choice, anyway. It kind of made sense‚Ä¶\nBut then I knew of course that too much exploration would mean‚Ä¶ Worse rewards.\nThat was correct, but a few more things were happening‚Ä¶ The short list here:\n\nmy reward update function per classifier of the LCS was not great\nand my choosing of best classifiers (influencing the algorithm step of going from match-set to action-set) was in fact all wrong too ü§¶‚Äç‚ôÇÔ∏è\n\nTheory to the rescue!"
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#i-only-have-gone-through-chapter-1-2-and-already",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#i-only-have-gone-through-chapter-1-2-and-already",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "I bought the ‚Äúbible of RL‚Äù (see Resources below) and recently received it at home. And it‚Äôs already making a difference! (Actually, having a physical copy was a great choice for me.)\nI have now corrected the above issues, but inspired by only the first two chapters (the second one really, as I had glanced at the first one earlier‚Ä¶) I was able to implement 2 different reward-update functions that (along with exploration pressure increase) made all the difference.\nPlus I now have initialized my rewards with a high positive value to foment more exploration at the earlier stages."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#results",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#results",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "To emit a judgement on the quality of my implementation, along with the right mix of hyper-parameters (then again, I‚Äôm not saying they‚Äôre optimal, I‚Äôm saying they‚Äôre OK)‚Ä¶\nI had to sit down and choose what would demonstrate ‚Äúlearning‚Äù in my small scenario.\nSo I kept things very simple:\n\n1 agent\n4 actions corresponding to the 4 directions (horizontal and vertical only)\nAt all steps, the agent can see empty cells, wall cells, food cells, enemy cells and where it came from (allocentric perception forced me to do that), 1 cell in any direction (including diagonals).\n\nIn a simple scenario, 5x5 world, there are at any point: 16 wall cells, 5 empty cells, 1 food, 1 enemy, 1 walk-back.\n\n\n\na simple world to study learning quality\n\n\nGiven that, I think this demonstrates how good the agent is after some training:\n\n\n\nlearning with sample average reward update\n\n\nThe above is using a ‚Äúsample average‚Äù reward update function, whereby the reward contribution gets smaller and smaller with time.\nThis learns the best exploitation policy in certain scenarios where there is a unique perfect policy. However, in our case, given the limited agent perception, one could consider that it would be best to keep learning over time.\nThat is because the ‚Äúfood‚Äù and the ‚Äúenemy‚Äù positions change once the agent collides with them. Although in the longer run that is probably incorrect, there are many possible perceived states even in this very small scenario (and in spite of the LCS and its GA pressuring to learn of each scenario only the relevant part of the states‚Ä¶), we could think that with limited learning steps, an option that ensures the agent keeps learning in a non-stationary environment will be better off.\nWell, so I implemented the equations with fixed alpha (set to 0.1).\nNote that in all the above, I keep starting training with one exploratory step in each 2, then reduce exploration every 2000 time-steps, until I get to 1 in 6. That forces exploration too, with more pressure at the beginning.\nWith the new rules this might be in fact unnecessary, I am aware, but I still haven‚Äôt tested with fixed exploration pressure (say always 1 in 6 in my example). It should work!\nAnyhow, the results with fixed alpha are almost identical in my simple scenario:\n\n\n\nlearning with fixed alpha"
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#what-do-the-above-screenshot-say",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#what-do-the-above-screenshot-say",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "Well, we‚Äôve seen there are:\n\nMore wall cells (16) than empty cells (5)\nmore empty cells (5) than enemy, food and ‚Äúwalk-back‚Äù cell (of which there is one)\n\nAnd yet, after some training, the agent averages, over 1000 most recent steps taken:\n\n275 food eaten (most rewarding)\n3 enemy encounters (most penalized)\nless than 100 wall bumps (heavily penalized)\nabout 150 walk-backs (slightly penalized)\nand well, quite a bit of walking around in empty cells\n\nI haven‚Äôt done the math, but in some of the perceived states:\n\nCorner: 5 walls/8 total cells. There are 4 corners. 62.5%\nSide: 3 walls/8. 37.5%\nThe above account for 8 of 9 possible agent positions (exceptuating center cell)\n\nAnd yet, only 10% of the time the agent bumps into a wall on average (and that‚Äôs with a 17% chance of exploratory move!).\nAs the agent has very limited state perception, I believe this is proof right there that it has indeed learnt something.\nI‚Äôd have to run many more numbers (4 moves possible, 9 locations, 5 different possible cells-status per cell, in different combinations‚Ä¶ I‚Äôm not doing that until someone asks me to do a PhD on the topic üòÅ)\nSimilar numbers could demonstrate that there are lots of food-related actions compared to white cell prevalence‚Ä¶\nAnd indeed, supposing the identical probability of encountering food or enemy at any stage, the fact that our agent has chosen the right action when faced with both possibilities, choosing food 275 times for only 3 times the enemy (again, exploration step 1 in 6 in all described statistics above), well‚Ä¶\nI‚Äôm sold."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#conclusions",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#conclusions",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "I‚Äôm very happy about it all. The Reinforcement Learning example I chose is working nicely! And it makes sense, based on the little theory I have studied thus far.\n(Note: I shall actually post a short entry on real-world application of data mining with my LCS implementation. Any day now, it has happened already‚Ä¶)."
  },
  {
    "objectID": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#resources",
    "href": "posts/2025-03-16_RLCS_RL_WorksAgain/index.html#resources",
    "title": "RLCS for RL: It works again (using books helps)",
    "section": "",
    "text": "‚ÄúReinforcement Learning, An introduction.‚Äù, 2nd Ed. Bradford Books, by R. S. Sutton & A. G. Bardo."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Version ‚Äú0‚Äù is done and working. And I will say this, it is already showing some of its power. Let me show you.\n\n\n\nAlright so this is what I have so far:\n\nA basic, fully functional, Michigan-style LCS for Supervised Learning\nA wrapper makes it easier to call the thing for training\nGA (tournament-based parents selection, one-point cross over reproduction, simple mutation) and Covering for rules discovery\nDeletion was somewhat fixed\nAccuracy and other stats work\nPrinting support functions\nNew: It includes subsumption. But calling subsumption affects performance ‚Äì both ways ‚Äì depending on the when/where/how, a known fact. But when used ‚Äì and that‚Äôs an important running decision, by the way ‚Äì, it condensates the relevant information encoded in the system‚Ä¶ Making it, when applicable, much better!\n\n\n\nsubsumption and deletion are costly, but effective to contain the size of the system\n\n\nNew too: Testing can be called now too, which is very helpful to validate the value of the LCS\nThis LCS for SL accepts an environment that, importantly, currently supports only:\n\nBinary classes/actions (so 0 or 1)\nBinary strings as input/states (no more no less) (and I mean, strings)\nAll states must be of equal length (whatever that is)\n\n\nThat‚Äôs on the plus side. On the not-so-great, well:\n\nLittle-to-no error control in there yet\nNext-to-zero documentation so far\nAnd I‚Äôll make a note on performance later in this post.\nImperfect/still-a-bit-disorganized code is probably an understatement.\n\nBut that‚Äôs just what needs to happen next, I guess. Now on to what you can do with this thing.\n\n\n\nSo I‚Äôve already shown that one, but it wasn‚Äôt ‚Äúperfect‚Äù. Here the new version and a few tricks allow me to show how the LCS can be used for data mining.\nSo I give it 70% of the sample space, and it comes up with the following ‚ÄúClassifiers Set‚Äù:\n\n\n\nUsing LCS to discover the hidden rules in the dataset\n\n\nNotice how this is quite exactly what the LCS was supposed to try and discover. And also how it indeed tells us what that particular ‚Äúenvironment‚Äù (dataset and its classes) is all about.\n(Oh, and in this particular exercise, I really did not need to run so many epochs, it‚Äôs just my default hyperparameter setting, but after a hundred or so epochs, it was already done.)\nLet‚Äôs move to a more interesting example.\n\n\n\nThe ‚ÄúIntroduction to Learning Classifier Systems‚Äù book ‚Äì referenced in my first post on LCS ‚Äì explains the algorithm with (among much more explanations) one particular example, called a ‚Äú6-bit multiplexer‚Äù.\nThe rules of that particular example are like so:\n\nyou get a string of 6 bits as input\nthe first two bits point to an address in the resting 4 bits\nthe ‚Äúclass‚Äù of your classifier should be whatever the bit is at the designated address.\n\nA couple of examples:\n\n‚Äú010000‚Äù says ‚ÄúBit 2 of the last 4 bits is the class, so 0‚Äù\n‚Äú110001‚Äù says ‚ÄúBit 4 of the last 4 bits is the class, so 1‚Äù\n‚Äú101101‚Äù says ‚ÄúBit 3 of the last 4 bits is the class, so 0‚Äù\n‚Ä¶\n\nYou get the idea. One aspect of this is meant to show that the LCS SL can somehow recognize things that are interrelated in the data, here how two different parts interact. They call this ‚Äúepistasis‚Äù. For now, suffice to say, it‚Äôs a bit more interesting use-case and we hope our LCS can discover relevant rules.\nAgain, with 70% of the whole dataset (64 entries total, for 6 bits), we train our LCS and hope to identify important rules (ideally, up to 8 perfect rules in this case ‚Äì but that‚Äôs not guaranteed). Here I show one such trained Classifier Set (aka ‚ÄúLCS‚Äù or ‚ÄúLearning Classifier System‚Äù, where the S stands for ‚ÄúSystem‚Äù, implying more than one classifier working together‚Ä¶ but I digress) that gets it almost perfect:\n\n\n\nAlmost perfect MUX6 LCS\n\n\nYou have to look somewhat carefully but you should find 7 of the 8 ‚Äúperfect‚Äù classifiers in there. And I haven‚Äôt checked for this particular example, but if the training set (randomly selected) did contain all the needed cases (probably did), this LCS should also almost cover for the missing classifier (11###0 0). Hint, the following would be the only one that would match for that missing classifier, so they would participate to recommend the action/class, and see how they all recommend class 0 (‚Ä¶):\n\n1####0 0\n#1#0#0 0\n##00#0 0\n##0#10 0\n\nSo in spite of missing one perfect rule (and that‚Äôs also depending on the random environment and non-deterministic nature of the LCS training‚Ä¶), it would work in all testing cases.\nLet‚Äôs move on to a (much) harder use-case.\n\n\n\nNow this was a stretch. the MNIST dataset is nothing like the dummy tests above, for sure. First, I don‚Äôt know how to manually write a perfect program manually for it (the above cases were trivial, comparatively).\nSecond, many trained neural nets on the MNIST dataset don‚Äôt get perfect record (but that‚Äôs understandable, if you spend enough time with that dataset).\nThird, and quite important, the ‚Äústates‚Äù here are numerical strings representing the shade (black-white) of 784 pixels (28*28) integers between 0 and 255. With a simple binary transformation I could end up with my states represented each as 6272 binary strings (8 bits per pixel).\nAnd there are a few thousand entries (samples) per represented number.\nI knew upfront, doing thousands of exposures per sample to get anywhere would take forever.\nFair enough, and so I simplified the problem first. Here is what I did:\n\nCompress the data, from shades to 0-1 values per pixel. I used a threshold to ‚Äúsharpen‚Äù the dataset, so only if the value was &gt; 100 (out of 255) I would set my new pixel to 1, otherwise I set it to 0.\nCompress further, from 28*28 to 7*7 pixels. Each original 4-pixels-square is encoded as only 1 pixel, with value 1 if any of the 4*4 subsquare was one, 0 otherwise.\nThe above gives me 7*7 binary strings, so my states are now 49 bits long.\n\n\n\nI ‚Äúcompress‚Äù the MNIST data before training my LCS\n\n\nClass stays the same, but I also will do a simpler exercise. My LCS should distinguish between classes 0 and 1 only. (This is not an issue, it‚Äôs a choice. I just choose to keep things simple, that‚Äôs all, but the value of the approach is the same, think of it as a one-versus-rest approach if you want). This is in part to make things faster, and in (big) part because the code so far only allows two-classes-output only.\n\nLet me skip directly to the results here:\n\n\n\nShowing a couple of runs of an LCS SL on MNIST simplified dataset\n\n\n(BTW, I meant to write ‚Äúconfusion matrix‚Äù, not ‚Äúcontention table‚Äù above, obviously‚Ä¶ Apologies, I hadn‚Äôt slept much yesterday‚Ä¶)\nI just want to make this clear:\n\nTraining on a total 500 random (sampled) training inputs only (that‚Äôs the size of my environment here), so roughly 250 examples of pictures of Zero and 250 of One\nTraining on these 500 examples takes me between 7.4 and 9.3 minutes (in the above example)\nIt generalizes very well: I end up with an LCS that correctly classified 96 to 97+% of the 3799 sample tests.\n\n\n\nJust FYI, I worked a bit more (31/12) on this and reduced the running times for better results in just above 20 seconds for the mono-thread version:\n\n\n\nBetter speed AND accuracy after slight code update\n\n\nThis gives us in fact 98.6+% of correctly classified test samples after 20s training (still on only 500 training samples)!\n\n\n\n\nOne thing that has worried me the past few days, something I actually knew was a concern with LCS, was performance. I mean speed, here. (SEE EDIT ABOVE before you continue)\nTraining requires exposing the LCS to sufficient training instances to ‚Äúrepresent‚Äù the overall data. Here I believe, in LCS in particular, too many training instances would hinder training because of the speed of processing power needed. I truly believe better chosen training instances are more important than more training instances. Kinda contrary of what Deep Learning would have you think. Although, this belief is, right now, just that: A belief.\nLCS on the other hand, for each training instance, require a lot of ‚Äúepochs‚Äù, so seeing the same data quite a few times. That is because after covering, the GA kicks in and that happens with choosing what best rules are available, which are evolved over time and where ‚Äúbest‚Äù depends on how many matches, how general but specific enough a rule is, whether a rule not only matches but also returns the correct action‚Ä¶ The point is, ideally, you want to train your LCS on a dataset, but circle over that dataset many times. I‚Äôm talking, thousands of times ideally - for each entry.\nThe other aspect of it is, of course, choosing R as a programming language. As much as I am comfortable with it, it is a slow language, if you consider compiled alternatives and such.\nI wanted to dive a bit into the details of what in my current (basic) implementation I could do, and so I ran the thing through ‚Äúprofvis‚Äù quite a few times, separate functions at separate times, then overall, with larger problems, etc.\nOne thing I do a lot, because I‚Äôm using lists for everything right now, is use ‚Äúlapply()‚Äù (and al.). I have tried as much as I could to not use dependencies if I could avoid it. So much so, I believe the only actual dependency I have is the plyr package for the rbind() function, and that‚Äôs for printing stuff‚Ä¶\nNot having too many dependencies is partly motivated by my end goal of producing a package, which I‚Äôd like to make ‚Äúlight‚Äù as much as possible.\nAnyhow, so I use lapply() a lot, but I have also used %in% quite a few times. And in some cases, I mix both and check quite a few instances, quite a few times!\nMatching, which happens a lot, for instance, can have an impact:\n\n\n\nProfiling run-times of mono-thread R interpreted code with LOTS of iterations in there‚Ä¶\n\n\n\n\n\nYou might know this by now: I like to parallelize stuff over my CPU cores/threads. It‚Äôs just a thing I think about when I see my processing times are‚Ä¶ Long. Even if long is a few seconds. And here, we‚Äôre talking many iterations, which on bigger datasets/longer states means minutes.\n(Actually, before I decided to simplify to 49 bits the states of the MNIST exercise, I was working with 784 bits states, and well, it would NOT converge after several minutes‚Ä¶)\nWell, I have to keep working on this, but the cool thing is, an LCS can be pre-trained, and then re-trained.\nSo the LCS (in supervised learning setting for now) is nothing but a set of classifiers, which are rules learnt from examples. You can improve it by exposing it, simply, to more examples. Or let it run more iterations over the same examples!\nEither are valid approaches.\nNow‚Ä¶ What if I ‚Äúsharded‚Äù my data? Say, if I have 7 CPU cores‚Ä¶\nI could do one shorter run over all the data to generate a first ruleset. I could then use at the same time that pretrained LCS, and what if I trained 7 sub-LCS on 7 subsets of the data for more iterations (quite a few more epochs now, but in parallel, each on smaller data samples)?\nWhat if then I loop over that, with some sort of ‚Äúcompaction‚Äù of the 7 newly re-trained sub-LCS and generate a new single base LCS, to then re-distribute to 7 sub processes.\nWhat if I did that a few times?\nI still have to think (seriously, and hard) through all the implications of the simple ‚Äúalgorithm‚Äù I just proposed above. I came up with it quite independently, mind you. And maybe it‚Äôs a very bad idea. (not showing all the training examples hinders the selection and then generalization of classifiers, for one‚Ä¶ So maybe it‚Äôs a stupid idea‚Ä¶ I just haven‚Äôt thought it through yet. For now I do think maybe it does in fact kinda make sense.)\nBUT, I have done it already! One version of my code allows for parallel processing like just described.\nAnd well, in (quite a few) tests with the MNIST example above, for roughly the same overall runtimes (and ‚Äúcompaction‚Äù is costly!) I get a gain of 1-2% of correctly classified samples.\nFrom 96% to 97+%. (It might sound marginal, and indeed I‚Äôm talking about a LOT more processing, but the same runtimes with marginal gains in such near-100% ranges, sounds like potentially valuable).\nWell, indeed, I‚Äôve done that too:\n\n\n\nCPU-based parallel training of my LCS\n\n\n\n\n\nSo I‚Äôve got all the pieces of the puzzle. And it works. And I have validated some marvellous (to me) results. All in a few days.\nWell, not a ‚Äúfew days‚Äù. See, I‚Äôve coded for only a few hours maybe, but I‚Äôve been thinking about this for a long time. And I also think it‚Äôd be unfair to say I was fast: Coding time per-se doesn‚Äôt account for waking up in the middle of the night coming up with my own design to approach the MNIST example. Waking up in the middle of the night, when I have a project like this, somehow happens more often. I have a bit of insomnia, but these projects make it worse, although they also make me more‚Ä¶ Productive.\nI guess what I‚Äôm saying is: coding time might not have been too many hours. But thinking time, or ‚Äúobsessing‚Äù (rather) did account for much more than 4 or 5 days (in 4 to 8h/day). I mean, I have designed everything in between the days of coding, and that started long before.\nAnyhow, to the next step: Moving from a few functions and examples to an actual Package!\nThat‚Äôs what I‚Äôll be up to when I next have time to dedicate to this.\n\n\n\nWell, that is, indeed, when I next have the time. I‚Äôll be disconnecting for a couple of weeks during January. I simply have other personal priorities (all great).\nSo I‚Äôll take this where I left it maybe a bit later. No matter. I have gone through half the book of packages, and I actually am more convinced than ever that this is the perfect opportunity to create a package for me.\n\n\n\nWell, I am just so‚Ä¶ Proud of this thing already!\nSee I had only ever read about the idea until‚Ä¶ Well, basically a week ago. Thanks to the holiday season, and long week-end hours, I can now say I have a working LCS in R.\nNot only that:\n\nI know it‚Äôs already not too bad in terms of what you could expect about performance. I know (from profiling) where I could try to go RCpp. I also have ideas to run it in more ‚Äúhighly‚Äù parallel setups. With more cores on a machine, or‚Ä¶ More machines! (plumbeR, here I come‚Ä¶ Well, later though, that‚Äôs not urgent)\nBut I also know, if I can ‚Äúencode‚Äù a problem into a binary string for binary classification, it turns out LCS are a very valid idea to consider! Just as good as a Neural Network! Heck, in some cases, better!\nI have done some testthat() already, I have several separate functions, very few (almost none) dependencies, the thing is somewhat organized‚Ä¶ So the next step comes rather naturally.\n\nAnyhow, overall, I am happy to report, I do believe this (future) package can prove very valuable to the community!\nAnd at this stage, I already hope I can explain that in future forums - I will convince people, I‚Äôm sure of it.\nSo this is turning out to be as great a project as I had (unreasonably) imagined already a long time ago. Which, let me tell you, is very satisfying at a very personal level.\n\n\n\nAgain, if some keywords above are confusing, the how of the algorithm is neatly explained in this 12‚Äô video, which I highly recommend."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#intro-imperfect-but-very-cool",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#intro-imperfect-but-very-cool",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Version ‚Äú0‚Äù is done and working. And I will say this, it is already showing some of its power. Let me show you."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#whats-in-it",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#whats-in-it",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Alright so this is what I have so far:\n\nA basic, fully functional, Michigan-style LCS for Supervised Learning\nA wrapper makes it easier to call the thing for training\nGA (tournament-based parents selection, one-point cross over reproduction, simple mutation) and Covering for rules discovery\nDeletion was somewhat fixed\nAccuracy and other stats work\nPrinting support functions\nNew: It includes subsumption. But calling subsumption affects performance ‚Äì both ways ‚Äì depending on the when/where/how, a known fact. But when used ‚Äì and that‚Äôs an important running decision, by the way ‚Äì, it condensates the relevant information encoded in the system‚Ä¶ Making it, when applicable, much better!\n\n\n\nsubsumption and deletion are costly, but effective to contain the size of the system\n\n\nNew too: Testing can be called now too, which is very helpful to validate the value of the LCS\nThis LCS for SL accepts an environment that, importantly, currently supports only:\n\nBinary classes/actions (so 0 or 1)\nBinary strings as input/states (no more no less) (and I mean, strings)\nAll states must be of equal length (whatever that is)\n\n\nThat‚Äôs on the plus side. On the not-so-great, well:\n\nLittle-to-no error control in there yet\nNext-to-zero documentation so far\nAnd I‚Äôll make a note on performance later in this post.\nImperfect/still-a-bit-disorganized code is probably an understatement.\n\nBut that‚Äôs just what needs to happen next, I guess. Now on to what you can do with this thing."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-1-not-bit-4",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-1-not-bit-4",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "So I‚Äôve already shown that one, but it wasn‚Äôt ‚Äúperfect‚Äù. Here the new version and a few tricks allow me to show how the LCS can be used for data mining.\nSo I give it 70% of the sample space, and it comes up with the following ‚ÄúClassifiers Set‚Äù:\n\n\n\nUsing LCS to discover the hidden rules in the dataset\n\n\nNotice how this is quite exactly what the LCS was supposed to try and discover. And also how it indeed tells us what that particular ‚Äúenvironment‚Äù (dataset and its classes) is all about.\n(Oh, and in this particular exercise, I really did not need to run so many epochs, it‚Äôs just my default hyperparameter setting, but after a hundred or so epochs, it was already done.)\nLet‚Äôs move to a more interesting example."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-2-mux6",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-2-mux6",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "The ‚ÄúIntroduction to Learning Classifier Systems‚Äù book ‚Äì referenced in my first post on LCS ‚Äì explains the algorithm with (among much more explanations) one particular example, called a ‚Äú6-bit multiplexer‚Äù.\nThe rules of that particular example are like so:\n\nyou get a string of 6 bits as input\nthe first two bits point to an address in the resting 4 bits\nthe ‚Äúclass‚Äù of your classifier should be whatever the bit is at the designated address.\n\nA couple of examples:\n\n‚Äú010000‚Äù says ‚ÄúBit 2 of the last 4 bits is the class, so 0‚Äù\n‚Äú110001‚Äù says ‚ÄúBit 4 of the last 4 bits is the class, so 1‚Äù\n‚Äú101101‚Äù says ‚ÄúBit 3 of the last 4 bits is the class, so 0‚Äù\n‚Ä¶\n\nYou get the idea. One aspect of this is meant to show that the LCS SL can somehow recognize things that are interrelated in the data, here how two different parts interact. They call this ‚Äúepistasis‚Äù. For now, suffice to say, it‚Äôs a bit more interesting use-case and we hope our LCS can discover relevant rules.\nAgain, with 70% of the whole dataset (64 entries total, for 6 bits), we train our LCS and hope to identify important rules (ideally, up to 8 perfect rules in this case ‚Äì but that‚Äôs not guaranteed). Here I show one such trained Classifier Set (aka ‚ÄúLCS‚Äù or ‚ÄúLearning Classifier System‚Äù, where the S stands for ‚ÄúSystem‚Äù, implying more than one classifier working together‚Ä¶ but I digress) that gets it almost perfect:\n\n\n\nAlmost perfect MUX6 LCS\n\n\nYou have to look somewhat carefully but you should find 7 of the 8 ‚Äúperfect‚Äù classifiers in there. And I haven‚Äôt checked for this particular example, but if the training set (randomly selected) did contain all the needed cases (probably did), this LCS should also almost cover for the missing classifier (11###0 0). Hint, the following would be the only one that would match for that missing classifier, so they would participate to recommend the action/class, and see how they all recommend class 0 (‚Ä¶):\n\n1####0 0\n#1#0#0 0\n##00#0 0\n##0#10 0\n\nSo in spite of missing one perfect rule (and that‚Äôs also depending on the random environment and non-deterministic nature of the LCS training‚Ä¶), it would work in all testing cases.\nLet‚Äôs move on to a (much) harder use-case."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-3-mnist-with-tricks",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#test-3-mnist-with-tricks",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Now this was a stretch. the MNIST dataset is nothing like the dummy tests above, for sure. First, I don‚Äôt know how to manually write a perfect program manually for it (the above cases were trivial, comparatively).\nSecond, many trained neural nets on the MNIST dataset don‚Äôt get perfect record (but that‚Äôs understandable, if you spend enough time with that dataset).\nThird, and quite important, the ‚Äústates‚Äù here are numerical strings representing the shade (black-white) of 784 pixels (28*28) integers between 0 and 255. With a simple binary transformation I could end up with my states represented each as 6272 binary strings (8 bits per pixel).\nAnd there are a few thousand entries (samples) per represented number.\nI knew upfront, doing thousands of exposures per sample to get anywhere would take forever.\nFair enough, and so I simplified the problem first. Here is what I did:\n\nCompress the data, from shades to 0-1 values per pixel. I used a threshold to ‚Äúsharpen‚Äù the dataset, so only if the value was &gt; 100 (out of 255) I would set my new pixel to 1, otherwise I set it to 0.\nCompress further, from 28*28 to 7*7 pixels. Each original 4-pixels-square is encoded as only 1 pixel, with value 1 if any of the 4*4 subsquare was one, 0 otherwise.\nThe above gives me 7*7 binary strings, so my states are now 49 bits long.\n\n\n\nI ‚Äúcompress‚Äù the MNIST data before training my LCS\n\n\nClass stays the same, but I also will do a simpler exercise. My LCS should distinguish between classes 0 and 1 only. (This is not an issue, it‚Äôs a choice. I just choose to keep things simple, that‚Äôs all, but the value of the approach is the same, think of it as a one-versus-rest approach if you want). This is in part to make things faster, and in (big) part because the code so far only allows two-classes-output only.\n\nLet me skip directly to the results here:\n\n\n\nShowing a couple of runs of an LCS SL on MNIST simplified dataset\n\n\n(BTW, I meant to write ‚Äúconfusion matrix‚Äù, not ‚Äúcontention table‚Äù above, obviously‚Ä¶ Apologies, I hadn‚Äôt slept much yesterday‚Ä¶)\nI just want to make this clear:\n\nTraining on a total 500 random (sampled) training inputs only (that‚Äôs the size of my environment here), so roughly 250 examples of pictures of Zero and 250 of One\nTraining on these 500 examples takes me between 7.4 and 9.3 minutes (in the above example)\nIt generalizes very well: I end up with an LCS that correctly classified 96 to 97+% of the 3799 sample tests.\n\n\n\nJust FYI, I worked a bit more (31/12) on this and reduced the running times for better results in just above 20 seconds for the mono-thread version:\n\n\n\nBetter speed AND accuracy after slight code update\n\n\nThis gives us in fact 98.6+% of correctly classified test samples after 20s training (still on only 500 training samples)!"
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#performance",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#performance",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "One thing that has worried me the past few days, something I actually knew was a concern with LCS, was performance. I mean speed, here. (SEE EDIT ABOVE before you continue)\nTraining requires exposing the LCS to sufficient training instances to ‚Äúrepresent‚Äù the overall data. Here I believe, in LCS in particular, too many training instances would hinder training because of the speed of processing power needed. I truly believe better chosen training instances are more important than more training instances. Kinda contrary of what Deep Learning would have you think. Although, this belief is, right now, just that: A belief.\nLCS on the other hand, for each training instance, require a lot of ‚Äúepochs‚Äù, so seeing the same data quite a few times. That is because after covering, the GA kicks in and that happens with choosing what best rules are available, which are evolved over time and where ‚Äúbest‚Äù depends on how many matches, how general but specific enough a rule is, whether a rule not only matches but also returns the correct action‚Ä¶ The point is, ideally, you want to train your LCS on a dataset, but circle over that dataset many times. I‚Äôm talking, thousands of times ideally - for each entry.\nThe other aspect of it is, of course, choosing R as a programming language. As much as I am comfortable with it, it is a slow language, if you consider compiled alternatives and such.\nI wanted to dive a bit into the details of what in my current (basic) implementation I could do, and so I ran the thing through ‚Äúprofvis‚Äù quite a few times, separate functions at separate times, then overall, with larger problems, etc.\nOne thing I do a lot, because I‚Äôm using lists for everything right now, is use ‚Äúlapply()‚Äù (and al.). I have tried as much as I could to not use dependencies if I could avoid it. So much so, I believe the only actual dependency I have is the plyr package for the rbind() function, and that‚Äôs for printing stuff‚Ä¶\nNot having too many dependencies is partly motivated by my end goal of producing a package, which I‚Äôd like to make ‚Äúlight‚Äù as much as possible.\nAnyhow, so I use lapply() a lot, but I have also used %in% quite a few times. And in some cases, I mix both and check quite a few instances, quite a few times!\nMatching, which happens a lot, for instance, can have an impact:\n\n\n\nProfiling run-times of mono-thread R interpreted code with LOTS of iterations in there‚Ä¶"
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#nicos-trick-parallelizing",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#nicos-trick-parallelizing",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "You might know this by now: I like to parallelize stuff over my CPU cores/threads. It‚Äôs just a thing I think about when I see my processing times are‚Ä¶ Long. Even if long is a few seconds. And here, we‚Äôre talking many iterations, which on bigger datasets/longer states means minutes.\n(Actually, before I decided to simplify to 49 bits the states of the MNIST exercise, I was working with 784 bits states, and well, it would NOT converge after several minutes‚Ä¶)\nWell, I have to keep working on this, but the cool thing is, an LCS can be pre-trained, and then re-trained.\nSo the LCS (in supervised learning setting for now) is nothing but a set of classifiers, which are rules learnt from examples. You can improve it by exposing it, simply, to more examples. Or let it run more iterations over the same examples!\nEither are valid approaches.\nNow‚Ä¶ What if I ‚Äúsharded‚Äù my data? Say, if I have 7 CPU cores‚Ä¶\nI could do one shorter run over all the data to generate a first ruleset. I could then use at the same time that pretrained LCS, and what if I trained 7 sub-LCS on 7 subsets of the data for more iterations (quite a few more epochs now, but in parallel, each on smaller data samples)?\nWhat if then I loop over that, with some sort of ‚Äúcompaction‚Äù of the 7 newly re-trained sub-LCS and generate a new single base LCS, to then re-distribute to 7 sub processes.\nWhat if I did that a few times?\nI still have to think (seriously, and hard) through all the implications of the simple ‚Äúalgorithm‚Äù I just proposed above. I came up with it quite independently, mind you. And maybe it‚Äôs a very bad idea. (not showing all the training examples hinders the selection and then generalization of classifiers, for one‚Ä¶ So maybe it‚Äôs a stupid idea‚Ä¶ I just haven‚Äôt thought it through yet. For now I do think maybe it does in fact kinda make sense.)\nBUT, I have done it already! One version of my code allows for parallel processing like just described.\nAnd well, in (quite a few) tests with the MNIST example above, for roughly the same overall runtimes (and ‚Äúcompaction‚Äù is costly!) I get a gain of 1-2% of correctly classified samples.\nFrom 96% to 97+%. (It might sound marginal, and indeed I‚Äôm talking about a LOT more processing, but the same runtimes with marginal gains in such near-100% ranges, sounds like potentially valuable).\nWell, indeed, I‚Äôve done that too:\n\n\n\nCPU-based parallel training of my LCS"
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#next-steps-packaging",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#next-steps-packaging",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "So I‚Äôve got all the pieces of the puzzle. And it works. And I have validated some marvellous (to me) results. All in a few days.\nWell, not a ‚Äúfew days‚Äù. See, I‚Äôve coded for only a few hours maybe, but I‚Äôve been thinking about this for a long time. And I also think it‚Äôd be unfair to say I was fast: Coding time per-se doesn‚Äôt account for waking up in the middle of the night coming up with my own design to approach the MNIST example. Waking up in the middle of the night, when I have a project like this, somehow happens more often. I have a bit of insomnia, but these projects make it worse, although they also make me more‚Ä¶ Productive.\nI guess what I‚Äôm saying is: coding time might not have been too many hours. But thinking time, or ‚Äúobsessing‚Äù (rather) did account for much more than 4 or 5 days (in 4 to 8h/day). I mean, I have designed everything in between the days of coding, and that started long before.\nAnyhow, to the next step: Moving from a few functions and examples to an actual Package!\nThat‚Äôs what I‚Äôll be up to when I next have time to dedicate to this."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#a-few-days-off",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#a-few-days-off",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Well, that is, indeed, when I next have the time. I‚Äôll be disconnecting for a couple of weeks during January. I simply have other personal priorities (all great).\nSo I‚Äôll take this where I left it maybe a bit later. No matter. I have gone through half the book of packages, and I actually am more convinced than ever that this is the perfect opportunity to create a package for me."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#conclusion",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#conclusion",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Well, I am just so‚Ä¶ Proud of this thing already!\nSee I had only ever read about the idea until‚Ä¶ Well, basically a week ago. Thanks to the holiday season, and long week-end hours, I can now say I have a working LCS in R.\nNot only that:\n\nI know it‚Äôs already not too bad in terms of what you could expect about performance. I know (from profiling) where I could try to go RCpp. I also have ideas to run it in more ‚Äúhighly‚Äù parallel setups. With more cores on a machine, or‚Ä¶ More machines! (plumbeR, here I come‚Ä¶ Well, later though, that‚Äôs not urgent)\nBut I also know, if I can ‚Äúencode‚Äù a problem into a binary string for binary classification, it turns out LCS are a very valid idea to consider! Just as good as a Neural Network! Heck, in some cases, better!\nI have done some testthat() already, I have several separate functions, very few (almost none) dependencies, the thing is somewhat organized‚Ä¶ So the next step comes rather naturally.\n\nAnyhow, overall, I am happy to report, I do believe this (future) package can prove very valuable to the community!\nAnd at this stage, I already hope I can explain that in future forums - I will convince people, I‚Äôm sure of it.\nSo this is turning out to be as great a project as I had (unreasonably) imagined already a long time ago. Which, let me tell you, is very satisfying at a very personal level."
  },
  {
    "objectID": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#references",
    "href": "posts/2024-12-29_RLCS_fully_functional_v000/index.html#references",
    "title": "RLCS Day 4.5 - Fully Functional v0 and New Tests",
    "section": "",
    "text": "Again, if some keywords above are confusing, the how of the algorithm is neatly explained in this 12‚Äô video, which I highly recommend."
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "I‚Äôve had my own kaizen-r.com blog for some time, and I relaunched on github.io in November last year.\nOne issue I‚Äôm facing is I need to migrate all the old contents, which were not on the same platform‚Ä¶ And so I need to manually migrate the contents of the older blog.\n\n\n\nSo one thing that happens is I have made many publications about my last MSc project. As the project is published already, it makes little sense for me to re-produce all those entries.\nSome, I might, but the majority will disappear.\nAnd some more of the old contents is probably‚Ä¶ Well, I might be better off if it disappears. Saves me the effort.\n\n\n\nText is just copy-paste, almost.\nThe issue is to re-organize the multimedia (mostly images‚Ä¶ And a few videos). That, unfortunately, will not be fast‚Ä¶\n\n\n\nIn short, I‚Äôll need to slowly move contents from the old blog, with some criteria hopefully."
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#migrating-more-of-the-old-contents",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#migrating-more-of-the-old-contents",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "I‚Äôve had my own kaizen-r.com blog for some time, and I relaunched on github.io in November last year.\nOne issue I‚Äôm facing is I need to migrate all the old contents, which were not on the same platform‚Ä¶ And so I need to manually migrate the contents of the older blog."
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#not-everything-goes",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#not-everything-goes",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "So one thing that happens is I have made many publications about my last MSc project. As the project is published already, it makes little sense for me to re-produce all those entries.\nSome, I might, but the majority will disappear.\nAnd some more of the old contents is probably‚Ä¶ Well, I might be better off if it disappears. Saves me the effort."
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#easy-and-slow",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#easy-and-slow",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "Text is just copy-paste, almost.\nThe issue is to re-organize the multimedia (mostly images‚Ä¶ And a few videos). That, unfortunately, will not be fast‚Ä¶"
  },
  {
    "objectID": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#conclusions",
    "href": "posts/2025-03-14_Migrating_Contents_Slowly/index.html#conclusions",
    "title": "Migrating Contents (slowly)",
    "section": "",
    "text": "In short, I‚Äôll need to slowly move contents from the old blog, with some criteria hopefully."
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html",
    "href": "posts/2024-11-10_entropy_of_zip/index.html",
    "title": "Entropy - Identifying compressed files",
    "section": "",
    "text": "About Shannon‚Äôs Information Entropy, applied to potentially detecting ciphered or compressed text compared to plain text.\n(First entry of the new platform, let‚Äôs see how it goes.)"
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#shannons-information-entropy",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#shannons-information-entropy",
    "title": "Entropy - Identifying compressed files",
    "section": "Shannon‚Äôs Information Entropy",
    "text": "Shannon‚Äôs Information Entropy\n\nWhy try to understand that?\nLong story short, Information Entropy is useful in quite a few machine learning algorithms, and to name only a few, the following two use it directly:\n\nPartitioning Trees (for nodes selection)\nLogistic Regression (through log loss)\n\nDoesn‚Äôt seem like much, said like that, but the Logistic Regression in turn can be used for‚Ä¶ Neural Networks :)\n\n\nHow it is defined?\nThe best way I personally managed to try and understand information entropy is through the concept of compression and surprise.\nA few helpful descriptions:\n‚Äú[‚Ä¶] the expected amount of information needed to describe the state of the variable [‚Ä¶]‚Äù\n‚ÄúEntropy is the measure of uncertainty of a variable. The more uncertain it is, the higher the entropy is.‚Äù\nHere is the mathematical expression of it:\n\\[\nH(X) = - \\sum_{x \\in X} p(x) log(p(x))\n\\]\nFrom the Wikipedia (I mean, why not?), this is the part that somehow can make sense for an intuitive understanding of the concept:\n‚ÄúThe information content, also called the surprisal or self-information, of an event \\(E\\) is a function which increases as the probability \\(p(E)\\) of an event decreases. When \\(p(E)\\) is close to 1, the surprisal of the event is low, but if \\(p(E)\\) is close to 0, the surprisal of the event is high. This relationship is described by the function\n\\[\nlog({1 \\over p(E)})\n\\]\nwhere \\(log()\\) is the logarithm, which gives 0 surprise when the probability of the event is 1. In fact, log is the only function that satisfies –∞ specific set of conditions [‚Ä¶]‚Äú"
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#application-detecting-cipherzip-on-data-streams",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#application-detecting-cipherzip-on-data-streams",
    "title": "Entropy - Identifying compressed files",
    "section": "Application: Detecting cipher/zip on data streams",
    "text": "Application: Detecting cipher/zip on data streams\nWe‚Äôre aiming for this today:\n\n\n\nCharacters distribution in Plain vs Zip text for a few Wiki entries\n\n\n\nThe code\nThe code will be on my Github soon enough (if not already).\nBut for now, a few blocks of it:\n\nmake_freq_df &lt;- function(filename) {\n    test1 &lt;- file(filename, open=\"rb\", raw = TRUE)\n    t1_bytes &lt;- t1_chars &lt;- c()\n    while(TRUE) {\n        temp &lt;- readBin(test1, what = \"raw\")\n        if(length(temp) == 0) break;\n        t1_bytes &lt;- c(t1_bytes, temp)\n        t1_chars &lt;- c(t1_chars, rawToChar(temp))\n    }\n    close(test1)\n    t1_df &lt;- data.frame(sort(table(as.character.hexmode(t1_bytes)), decreasing = TRUE))\n    t1_df$char &lt;- names(sort(table(t1_chars), decreasing = TRUE))\n    names(t1_df) &lt;- c(\"x\", \"probs\", \"char\")\n    # Instead of counts (table output), make it probability:\n    t1_df$probs &lt;- t1_df$probs/sum(t1_df$probs)\n    # Alternative could have been:\n    #t1_df$probs &lt;- as.numeric(prop.table(sort(table(t1_chars), decreasing = TRUE)))\n    \n    t1_df\n}\n\nThe above function is a (bad, but functional) way of taking a file, reading it in ‚Äúraw‚Äù format, and output byte-by-byte into a dataframe.\n\nThe first output column will be the ‚Äúraw byte‚Äù (for text, the ASCII code, say ‚Äú20‚Äù for space character).\nThe second column contains the Probability of appearance of a character, compared to the whole text being analysed (so, the frequency of it‚Äôs appearance).\nThe third column is for reference only, to ‚Äúsee‚Äù what the character would look like in plain text. Note that ‚Äù ‚Äù (space) and null would look similar‚Ä¶ And so would other encoded bytes, but that‚Äôs not to worry for today.\n\nWith the above in mind, here is an output of plain and zip‚Äôed text, along with the Shannon‚Äôs Entropy of it, correspondingly:\n&gt; firewall_wiki &lt;- compare_clear_zip(1, wiki_pages_df)\nupdating: posts/2024-11-10_entropy_of_zip/firewall_wiki.txt (deflated 63%)\n   x      probs char\n1 20 0.14766670     \n2 65 0.09267745    e\n3 69 0.07790143    i\n4 74 0.06658562    t\n5 6e 0.06621154    n\n6 61 0.06050687    a\n   x       probs char\n1  0 0.012244898     \n2 39 0.006722689    9\n3 72 0.006722689    r\n4 5f 0.006482593    _\n5 34 0.006242497    4\n6 e4 0.006242497 \\xe4\n[1] \"Entropy Plain text: 4.29839806234458\"\n[1] \"Entropy Zip text: 7.94701914818039\"\n\nIn Plain text, the space character appears quite a bit. So do the letters e, i, t, n, a... (That‚Äôs for English, and remember these are small sample texts extracted from some Wikipedia pages‚Ä¶). Plain text has repetition on some characters (higher probability of appearance), with varying distributions (and uses fewer different bytes).\nIn Zip, the probabilities are each MUCH lower, and more even across all possible bytes. And that‚Äôs our KEY concept for today. Zip is compression, so all its characters have as few repetition as possible (i.e.¬†low probability).\nInterestingly, with the above approach, ciphered data would look like zip data.\n\nOK, so let‚Äôs go back to our definitions of the first part:\n‚Äú[‚Ä¶] When \\(p(E)\\) is close to 1, the surprisal of the event is low, but if \\(p(E)\\) is close to 0, the surprisal of the event is high[‚Ä¶]‚Äú\nHopefully we‚Äôre getting somewhere with understanding the concept. Uncommon characters will have higher ‚Äúsurprisal‚Äù, and lower probability of appearing.\nOh: And we should not be afraid of the math, it wasn‚Äôt that bad. Here is what Shannon‚Äôs Entropy actually looks like for varying values of probability of appearance of a given character:\n\n\n\nShannon Entropy across possible probabilities\n\n\n\n\nWhat does it mean in practice?\nWell, it means that if you sample some bytes sniffed on a network, if you see seemingly random characters and no particular prevalence of any given one over the rest, you know it‚Äôs not clear-text.\nAnd yes, if you have the file extension, maybe this is all useless. So why you would care?\nFirst, this is pretty cool. If you sample data (from a network stream, or bytes on a disk‚Ä¶), you can distinguish ‚Äúautomagically‚Äù what‚Äôs plain text and what‚Äôs ciphered/zip.\nSecond: Maybe you can use that to detect covert channels out of packet capture? Or maybe let your computer on its own decide to use one set of algorithm to analyse things when there is plain text, and use another set of characters when there is ciphered/compressed text (or images, etc.)."
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#conclusions",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#conclusions",
    "title": "Entropy - Identifying compressed files",
    "section": "Conclusions",
    "text": "Conclusions\nAll this took me quite a while to really understand it. Or think I do, anyway :D\nToday we‚Äôve tried to explain the concept of information entropy through a simple application. If at this point my readers have gotten somewhat of an intuition about the concept, I‚Äôll be very happy.\nAnd the concept is quite relevant for Machine Learning, as we shall see in future posts."
  },
  {
    "objectID": "posts/2024-11-10_entropy_of_zip/index.html#references",
    "href": "posts/2024-11-10_entropy_of_zip/index.html#references",
    "title": "Entropy - Identifying compressed files",
    "section": "References",
    "text": "References\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)\nThe original idea about this post I read a few years back in ‚ÄúNetwork Security Through Data Analysis‚Äù, 2nd Ed, by M. Collins (O`Reilly)"
  },
  {
    "objectID": "posts/2025-03-21_RLCS_DataMining_Visuals/index.html",
    "href": "posts/2025-03-21_RLCS_DataMining_Visuals/index.html",
    "title": "RLCS for Data Mining: Visuals",
    "section": "",
    "text": "I‚Äôve shown quite a few ‚ÄúClassifiers‚Äù and detailed how data mining can be cool with an LCS.\nBut looking at text-like classifiers populations is not fun (easier for the computer, not fun for the human).\nHere is an alternative:\n\n\n\nWhat you need to decide on classification\n\n\nThat‚Äôs the 3D version of it.\nInterpretation: It basically says ‚Äúmost rules will require to know about 3 bits only (out of 6 in this case), and for these, the first two bits are much more important‚Äù.\nIndeed. That was for the MUX6bits of past examples, the very same proposed in the book about LCS I have referenced in past entries.\nI had inspiration from a video by Dr.¬†Will Browne on how one could visualize the attention of an LCS:\n\n\n\nProposed alternative in Dr.¬†Browne‚Äôs youtube video\n\n\nCompared to the past results I have been showing:\n\n\n\nFormer/old presentation‚Ä¶\n\n\nI think the visual is better looking :D\nAs I am not a fan of forcing 3D on what could be 2D and a palette, here goes an alternative visualization. Less fancy, but just as practical, I think:\n\n\n\n2D alternative. Might be the best one"
  },
  {
    "objectID": "posts/2025-03-21_RLCS_DataMining_Visuals/index.html#i-keep-at-it-explainable-findings",
    "href": "posts/2025-03-21_RLCS_DataMining_Visuals/index.html#i-keep-at-it-explainable-findings",
    "title": "RLCS for Data Mining: Visuals",
    "section": "",
    "text": "I‚Äôve shown quite a few ‚ÄúClassifiers‚Äù and detailed how data mining can be cool with an LCS.\nBut looking at text-like classifiers populations is not fun (easier for the computer, not fun for the human).\nHere is an alternative:\n\n\n\nWhat you need to decide on classification\n\n\nThat‚Äôs the 3D version of it.\nInterpretation: It basically says ‚Äúmost rules will require to know about 3 bits only (out of 6 in this case), and for these, the first two bits are much more important‚Äù.\nIndeed. That was for the MUX6bits of past examples, the very same proposed in the book about LCS I have referenced in past entries.\nI had inspiration from a video by Dr.¬†Will Browne on how one could visualize the attention of an LCS:\n\n\n\nProposed alternative in Dr.¬†Browne‚Äôs youtube video\n\n\nCompared to the past results I have been showing:\n\n\n\nFormer/old presentation‚Ä¶\n\n\nI think the visual is better looking :D\nAs I am not a fan of forcing 3D on what could be 2D and a palette, here goes an alternative visualization. Less fancy, but just as practical, I think:\n\n\n\n2D alternative. Might be the best one"
  },
  {
    "objectID": "posts/2025-03-21_RLCS_DataMining_Visuals/index.html#code-improvement",
    "href": "posts/2025-03-21_RLCS_DataMining_Visuals/index.html#code-improvement",
    "title": "RLCS for Data Mining: Visuals",
    "section": "Code improvement",
    "text": "Code improvement\nDid you notice the code above?\nplot(lcs_classifier2)\nThat‚Äôs because I am working on doing things a bit better, using in this case S3 objects. I create a class ‚Äúrlcs_population‚Äù and with that I can have dedicated print, plot, etc. functions that are called for these objects!\nStill not great, still work ongoing, but‚Ä¶ Looking better every day :)"
  },
  {
    "objectID": "posts/2025-03-21_RLCS_DataMining_Visuals/index.html#conclusions",
    "href": "posts/2025-03-21_RLCS_DataMining_Visuals/index.html#conclusions",
    "title": "RLCS for Data Mining: Visuals",
    "section": "Conclusions",
    "text": "Conclusions\nI‚Äôm not always a fan of 3D visuals.\nBut I have to admit, it beats looking at strings of binary numbers in our tertiary alphabet‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "At least the XCS version of LCS algorithm is a ‚Äúwell-known‚Äù (in the rather small sub-world of LCS enthusiasts, that is) approach to do Reinforcement Learning.\nThen again, all step-by-steps for RL in LCS seem to be hidden in more or less archane papers which are pay-per-access, and I don‚Äôt know yet if things make enough sense for me to pay for reading said papers. (Until recently, I had access with my University‚Äôs MSc Student account‚Ä¶ No more, it would seem. Fair enough.) We‚Äôll see how I go about that later‚Ä¶\nBut for now‚Ä¶\n\n\n\nWell, what I can do, is try to come up with my own personal version of an RL implementation with my RLCS ‚Äúfuture-package‚Äù, see if I can make it work, without worrying about past work too much for now.\nI do have a general understanding of ideas of LCS and RL, I just haven‚Äôt found a detailed step-by-step for me to copy from pseudo-code. I also have been reading (but I‚Äôm far from done) what I understand is the bible of Reinforcement Learning, i.e.¬†‚ÄúReinforcement Learning - An Introduction‚Äù by R. S. Sutton & A. G. Bardo. The general ideas of TD and Q-Learning I think I somewhat grasp, and I have my own (limited, sure) experience with Monte-Carlo.\nMixing these concepts with what I have implemented thus far of an LCS, adapting my code with concepts of Action Sets instead of Correct Sets, detect/action/effect/reward with an environment, and a split between exploit and explore, I came up with this ‚Äúdesign‚Äù this morning (Saturday and Sunday mornings are often a bit quieter than the rest of the week):\n\n\n\nthinking, thinking‚Ä¶\n\n\nThis is all on my small home-office wall right now, not at all implemented, but it seems it should be feasible, and I am curious as to whether it could work (or be a complete bust).\n\n\n\nWell, truth being told‚Ä¶ I don‚Äôt know.\nIt ‚Äúsounds‚Äù like a cool thing to be able to do. In the above drawing, I suggest I could, using an LCS, implement an ‚Äúagent‚Äù (yes, one of those, but please, I‚Äôm not talking about LLMs) that would learn on its own how to navigate a simple grid-world and look for food.\nI will have to code the whole logic and rules of that world, all aspects of movements in it, rewards, etc. It‚Äôs going to be a bit of work (somehow I think I‚Äôll manage).\nBut yeah, I don‚Äôt know a few things at this point: whether my ‚Äúdesign‚Äù will work (with whatever adjustments I can come up with), or why even program an RL algorithm.\nI just think it would be cool to have this future package be able to do all of data mining, supervised learning and reinforcement learning, with examples of each.\nAs to when I could use it, I know the answer for data mining and SL ‚Äì not so much for RL, for now.\n\n\n\nJust another fun exercise, really. It will take a few weeks, probably.\nBut if (big if at this stage) I manage to make it work, that would mean I actually understand enough of LCS as a concept to implement it from scratch and do data mining, Supervised Learning and Reinforcement learning. In turn that would also imply I know a bit more of what Reinforcement Learning entails.\nWhich in itself ‚Äì for me ‚Äì would already be pretty cool.\n\n\n\nTo be fair, there are a few papers out there I was able to access for free. In no special order and particularly valuable to inspire my future exercise I found these:\nhttps://arxiv.org/abs/2305.09945\nhttps://dl.acm.org/doi/pdf/10.1145/206944.206955\nhttps://www2.cs.uh.edu/~ceick/6367/bull-lcs.pdf"
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#although-i-know-in-theory",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#although-i-know-in-theory",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "At least the XCS version of LCS algorithm is a ‚Äúwell-known‚Äù (in the rather small sub-world of LCS enthusiasts, that is) approach to do Reinforcement Learning.\nThen again, all step-by-steps for RL in LCS seem to be hidden in more or less archane papers which are pay-per-access, and I don‚Äôt know yet if things make enough sense for me to pay for reading said papers. (Until recently, I had access with my University‚Äôs MSc Student account‚Ä¶ No more, it would seem. Fair enough.) We‚Äôll see how I go about that later‚Ä¶\nBut for now‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#general-understanding-of-rl-and-lcs-my-own-mix",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#general-understanding-of-rl-and-lcs-my-own-mix",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "Well, what I can do, is try to come up with my own personal version of an RL implementation with my RLCS ‚Äúfuture-package‚Äù, see if I can make it work, without worrying about past work too much for now.\nI do have a general understanding of ideas of LCS and RL, I just haven‚Äôt found a detailed step-by-step for me to copy from pseudo-code. I also have been reading (but I‚Äôm far from done) what I understand is the bible of Reinforcement Learning, i.e.¬†‚ÄúReinforcement Learning - An Introduction‚Äù by R. S. Sutton & A. G. Bardo. The general ideas of TD and Q-Learning I think I somewhat grasp, and I have my own (limited, sure) experience with Monte-Carlo.\nMixing these concepts with what I have implemented thus far of an LCS, adapting my code with concepts of Action Sets instead of Correct Sets, detect/action/effect/reward with an environment, and a split between exploit and explore, I came up with this ‚Äúdesign‚Äù this morning (Saturday and Sunday mornings are often a bit quieter than the rest of the week):\n\n\n\nthinking, thinking‚Ä¶\n\n\nThis is all on my small home-office wall right now, not at all implemented, but it seems it should be feasible, and I am curious as to whether it could work (or be a complete bust)."
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#why-even-consider-rl",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#why-even-consider-rl",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "Well, truth being told‚Ä¶ I don‚Äôt know.\nIt ‚Äúsounds‚Äù like a cool thing to be able to do. In the above drawing, I suggest I could, using an LCS, implement an ‚Äúagent‚Äù (yes, one of those, but please, I‚Äôm not talking about LLMs) that would learn on its own how to navigate a simple grid-world and look for food.\nI will have to code the whole logic and rules of that world, all aspects of movements in it, rewards, etc. It‚Äôs going to be a bit of work (somehow I think I‚Äôll manage).\nBut yeah, I don‚Äôt know a few things at this point: whether my ‚Äúdesign‚Äù will work (with whatever adjustments I can come up with), or why even program an RL algorithm.\nI just think it would be cool to have this future package be able to do all of data mining, supervised learning and reinforcement learning, with examples of each.\nAs to when I could use it, I know the answer for data mining and SL ‚Äì not so much for RL, for now."
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#conclusions",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#conclusions",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "Just another fun exercise, really. It will take a few weeks, probably.\nBut if (big if at this stage) I manage to make it work, that would mean I actually understand enough of LCS as a concept to implement it from scratch and do data mining, Supervised Learning and Reinforcement learning. In turn that would also imply I know a bit more of what Reinforcement Learning entails.\nWhich in itself ‚Äì for me ‚Äì would already be pretty cool."
  },
  {
    "objectID": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#references",
    "href": "posts/2025-02-09_About_Understanding_RL_with_LCS/index.html#references",
    "title": "RLCS: Reinforcement Learning?",
    "section": "",
    "text": "To be fair, there are a few papers out there I was able to access for free. In no special order and particularly valuable to inspire my future exercise I found these:\nhttps://arxiv.org/abs/2305.09945\nhttps://dl.acm.org/doi/pdf/10.1145/206944.206955\nhttps://www2.cs.uh.edu/~ceick/6367/bull-lcs.pdf"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "I have yet another day off (that‚Äôs the thing with Holiday season), and I really wanted to keep working on this.\nSo I added the Genetic Algorithm part to my Supervised Learning basic Michigan-style LCS.\nThat means, after covering (done last Sunday) I am done with the (basic) implementation of the Rule Discovery features of the LCS algorithm.\nAnd, heck, it works.\n\n\n\nSo we had covering, whereby new rules were created when none in the classifiers matched the a new-coming environment instance.\nBut this is only part of the discovery of new rules. The second part in the Michigan-style LCS is that of a Genetic Algorithm.\nI implemented now a simple version, with tournament parents selection. That is, from the Correct set (see past entries) ‚Äì but taking numerosity of classifiers into account ‚Äì I choose N (a parameter) parents randomly. Then I take the best 2 parents by fitness. Which is ellitist.\nThen I run a one-point cross-over, to generate 2 children.\nI apply mutation to the children genes in turn, with a mutation pressure set as yet another parameter (default is 5% in my implementation).\nAnd finally I add the children to the population.\nNote that children mutations ensure only valid rules are created (i.e.¬†condition is compatible with the training instance). Mutations either generalize (add a wildcard) or specify (replaces a wildcard by a 0 or 1, depending on training instance).\nFully general (‚Äúall wildcard‚Äù) individuals are of course discarded. If a child actually matches by condition an individual in the correct set, it is discarded and the repeated classifier instead get a numerosity increase.\nAnd that‚Äôs IT!\n\n\n\nIn my current dummy example, whereby the goal is for the LCS to discover the rule ‚ÄúClass is NOT(bit 4 of input state)‚Äù, well‚Ä¶ In manual runs (several of them), I often (not always, mind you) find the two best rules are perfect rules:\n\n\n\nPerfect Rules are often discovered!\n\n\n\n\n\nThe first two rules (ordered by accuracy and numerosity, both decreasing) are the exact rules that perfectly will always predict the correct class/action, and are perfectly balanced in this case between general and specific.\nThese are the kind of things that an LCS provides that a Neural Network won‚Äôt:\nIt tells you WHY it makes a decision.\nHere, it is saying:\n\nRule 1: If bit 4 is 0, then class is 1. I am 100% accurate, whenever that happens.\nRule 2: If bit 4 is 1, then class is 0. I am 100% accurate, whenever that happens.\n\nWhich is exactly what this dummy training data set was trying to say in the first place, with 70% of the training possibilities shown to train the algorithm.\nObviously this is‚Ä¶ A somewhat silly example. But the thing is, I did NOT tell it how to get there, and it got the IF THEN ELSE correctly on its own.\n\n\n\nEvery now and then (after a certain number of Epochs) I run a Deletion process, whereby rules that have Numerosity 1 are discarded, thereby containing the size of the set of classifiers.\nThis is important for interpretability and performance.\n\n\n\nI am quite happy that after only a couple of days of coding, I have a basic, perfectly functional LCS!\nI do miss two components: Subsumption and Compaction.\nPlus of course the Prediction, that comes after training, on a test subset.\nIt‚Äôs all on my to-do, but this thing is already promising!\nGranted, it‚Äôs not real fast. And the code is far from perfect, or duly organized. But it works!"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#intro",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#intro",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "I have yet another day off (that‚Äôs the thing with Holiday season), and I really wanted to keep working on this.\nSo I added the Genetic Algorithm part to my Supervised Learning basic Michigan-style LCS.\nThat means, after covering (done last Sunday) I am done with the (basic) implementation of the Rule Discovery features of the LCS algorithm.\nAnd, heck, it works."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#what-the-ga-does",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#what-the-ga-does",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "So we had covering, whereby new rules were created when none in the classifiers matched the a new-coming environment instance.\nBut this is only part of the discovery of new rules. The second part in the Michigan-style LCS is that of a Genetic Algorithm.\nI implemented now a simple version, with tournament parents selection. That is, from the Correct set (see past entries) ‚Äì but taking numerosity of classifiers into account ‚Äì I choose N (a parameter) parents randomly. Then I take the best 2 parents by fitness. Which is ellitist.\nThen I run a one-point cross-over, to generate 2 children.\nI apply mutation to the children genes in turn, with a mutation pressure set as yet another parameter (default is 5% in my implementation).\nAnd finally I add the children to the population.\nNote that children mutations ensure only valid rules are created (i.e.¬†condition is compatible with the training instance). Mutations either generalize (add a wildcard) or specify (replaces a wildcard by a 0 or 1, depending on training instance).\nFully general (‚Äúall wildcard‚Äù) individuals are of course discarded. If a child actually matches by condition an individual in the correct set, it is discarded and the repeated classifier instead get a numerosity increase.\nAnd that‚Äôs IT!"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#results-with-fully-functional-rule-discovery",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#results-with-fully-functional-rule-discovery",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "In my current dummy example, whereby the goal is for the LCS to discover the rule ‚ÄúClass is NOT(bit 4 of input state)‚Äù, well‚Ä¶ In manual runs (several of them), I often (not always, mind you) find the two best rules are perfect rules:\n\n\n\nPerfect Rules are often discovered!"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#important-what-the-rules-set-tells-us",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#important-what-the-rules-set-tells-us",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "The first two rules (ordered by accuracy and numerosity, both decreasing) are the exact rules that perfectly will always predict the correct class/action, and are perfectly balanced in this case between general and specific.\nThese are the kind of things that an LCS provides that a Neural Network won‚Äôt:\nIt tells you WHY it makes a decision.\nHere, it is saying:\n\nRule 1: If bit 4 is 0, then class is 1. I am 100% accurate, whenever that happens.\nRule 2: If bit 4 is 1, then class is 0. I am 100% accurate, whenever that happens.\n\nWhich is exactly what this dummy training data set was trying to say in the first place, with 70% of the training possibilities shown to train the algorithm.\nObviously this is‚Ä¶ A somewhat silly example. But the thing is, I did NOT tell it how to get there, and it got the IF THEN ELSE correctly on its own."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#deletion-is-also-implemented",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#deletion-is-also-implemented",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "Every now and then (after a certain number of Epochs) I run a Deletion process, whereby rules that have Numerosity 1 are discarded, thereby containing the size of the set of classifiers.\nThis is important for interpretability and performance."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#conclusion",
    "href": "posts/2024-12-22_RLCS_Day1_PartII_Basic_Coverage/index.html#conclusion",
    "title": "RLCS Day 2 - Full Rule Discovery",
    "section": "",
    "text": "I am quite happy that after only a couple of days of coding, I have a basic, perfectly functional LCS!\nI do miss two components: Subsumption and Compaction.\nPlus of course the Prediction, that comes after training, on a test subset.\nIt‚Äôs all on my to-do, but this thing is already promising!\nGranted, it‚Äôs not real fast. And the code is far from perfect, or duly organized. But it works!"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I‚Äôve already tried to parallelize a bit the training of a supervised-learning LCS. It kinda‚Äô works‚Ä¶ But the more I think about it, the more I‚Äôm convinced it‚Äôs not as easy as I thought.\nI initially thought of one way of parallel processing: Break the training samples in N (as many as CPU cores you wanna use) subsets, train N LCS.\nThen ‚Äúmerge‚Äù, subsume, remove duplicates, and‚Ä¶ Repeat.\nIdeally you would bootstrap the thing with an initial epoch on all the training set, just to have relevant coverage to start with.\nBut there is an issue: What if your dataset is ‚Äúnot perfect‚Äù? Let‚Äôs suppose there isn‚Äôt a clear-cut separation between classes to be found, but rather a fuzzy separation. So there aren‚Äôt perfect rules to be discovered. Instead, in each subset, some rules work, but THEN when these are exposed to the complete dataset, the discovered classes in one subset is correct in that subset, but NOT in the overall training data?\nYou end up with classifiers that‚Ä¶ Don‚Äôt work over test sets.\nAt least that‚Äôs my current conclusion after trying it with some messier-than-perfect datasets.\n\n\n\nSo if breaking the dataset in sub-sets does not work‚Ä¶ What about breaking the data POINTS in different parts?\nIn my earlier example, I have tested (quite successfully, in my view) my RLCS on a simplified MNIST exercise.\nThere, I was training to classify images. The original images where too big, in their equivalent bit-strings representation, for my laptop to work on in acceptable runtimes (for my taste). I ended up compressing said images into 49 bits strings per image. It was then manageable, and it worked quite good, in little over 20 seconds of training (with the correct hyper-parameters).\nBut could I instead break each images in say 7x7 lines, and train on each line separately (and in parallel), and then train one more LCS on the result of that (so 7 outputs would become the input states for a new classifier system, another 7-bits train set, so to speak)?\nI would then in effect, in terms of overall processing time, train approximately twice on 7 bits strings, instead of once on 49 bits strings.\nWould that even work? More on that later.\nBut first‚Ä¶\n\n\n\nEpistasis!\nWell, at least that‚Äôs how I understand it right now: Two (or more) different locations of the ‚Äúgenome‚Äù are required to define the phenotype. In my simple RLCS, in that case two or more bits in a state interact in a more or less complex manner so that the class is decided by said interaction. THEN, the second approach would break the assumptions!\nThink of the ‚ÄúMux6‚Äù example explained in this Blog a few entries ago. Two bits pointed to a position, which value was the class to be predicted. In that example, the first two bits pointed to a position in the last 4 bits.\nIF I were to break the 6 bits states in say two sub-strings of 3 bits each, thinking I can learn from each sub-string and then recompose (see ‚Äúidea version 2‚Äù above)‚Ä¶ I would fail miserably: you need the complete 6-bit strings in that example for the LCS to come up with the relevant rules!\nThat said‚Ä¶\n\n\n\nI‚Äôve shown ad-nauseam by now the ‚Äúnot-bit-4‚Äù example. In the example so far, I have used 5 bits strings. So the ‚Äúsearch space‚Äù was that of 5 bits. And the solution was simple in that one bit would suffice to predict the whole class.\nBut that was arbitrary, and meant to be simple.\nWhat if we were to be faced with the exact same rule to be found in 10-bits strings? The solution would be just as simple (two rules would suffice), but the search space would be comparatively‚Ä¶ Well, way bigger.\nIn 5-bits, the not-bit-4 is fully expressed with 32 states.\nIn 10-bits, the SAME not-bit-4 classifier works with‚Ä¶ 1024 states (total). For a perfect data mining exercise, you need to find a ruleset that perfectly matches all 1024 states, and you need to try combinations of 3 possible values per-bit (our ternary alphabet: {‚Äò#‚Äô, ‚Äò0‚Äô, ‚Äò1‚Äô}). If I‚Äôm correct, that could be up to about 3^10*2 (including class) 59049-1 possible unique classifiers to potentially check (coverage ensures here that not-bit-4 is respected, so that‚Äôs only half the overall 10 bits strings + class, as half are illegal, making it a 3^10 matter‚Ä¶ And the ‚Äò##########‚Äô option is not valid for us‚Ä¶). Well, approximately anyway.\nAnd let‚Äôs remember, an LCS is expected to output a combination subset of these ~59K classifiers as an output System, which could be a very large search space indeed‚Ä¶ (In a real world setup, that is.)\nHowever! Well, let‚Äôs try to apply the logic of my ‚Äúapproach number 2‚Äù above to break down the exercise and then see what would happen in such a simple case‚Ä¶\nImagine I break down the exercise into 3 ‚Äútrainings‚Äù:\n\nThe first 5 bits (which in fact are more than enough) per state, creating \\(RLCS_1\\)\nThe last 5 bits per state (completely irrelevant, but we don‚Äôt know that upfront), creating \\(RLCS_2\\)\nThen the outputs of the above to train a third LCS, \\(RLCS_{combined}\\)\n\nThe hope is that we can train the first two in parallel, and use the predictions of these two in combination to then train a third classifier (which incidentally here would be exposed to 2-bits states). So I‚Äôd expect to reduce the search space quite a bit. And hence the training times.\nWell, with no further ado, here come the initial results, and heck!\nParallel runtime using the approach described here to break the problem in just two 5-bit string sub-problems, less than 5 seconds:\n\n\n\nbreaking original problem in two makes it fast!\n\n\nCompared to original sequential time on the 10-bit string, more than 4 minutes:\n\n\n\nThat worked, just‚Ä¶ Very slow, comparatively\n\n\nThe result however is slightly more difficult to interpret (that is, in the current version of this exercise, which I just wanted to see working, but it is not clean‚Ä¶). In particular, each subset I have to keep (for now) better than chance results (anything &gt; 0.5), but that‚Äôs just because it‚Äôs a dirty example, it can be made cleaner for sure:\n\n\n\nFast results, but requires a bit more work to interpret\n\n\nStill, from ‚Äú&gt; 4 minutes‚Äù to ‚Äú&lt; 5 seconds‚Äù! But then again, this will only works for such a problem where the solution did exist in a subset of original bit-strings.\n\n\n\nWell, for one, I am NOT in fact browsing in parallel through 1024 states!\nSee, each sub-string of 5 bits and its class, as a subset can be de-duplicated! There are a lot of repeated instances, say in the first subset, whereby all entries that say ‚Äú00000 -&gt; class 1‚Äù appear 32 times (one for each of the continuing 5-bit strings for the rest of the original state).\nThat kind of deduplication can reduce processing efforts indeed :)\n\n\n\nI haven‚Äôt tested it yet, but I would wager this approach should work just fine on images too, as described above, breaking images in lines and then combining classifiers‚Ä¶\nParallelizing training is an obvious idea (to me) in concept: For some examples, training is slow, and so running it in N parallel processes must be faster (for, you know, complicated examples).\nBut as explained in some detail today, it‚Äôs not that simple! It might work in some cases, as explained, but in the ‚Äúreal world‚Äù, we wouldn‚Äôt know upfront whether it would in fact work, would we? And so we might have to do some trial-and-error‚Ä¶\nBut even just running tests on new datasets, as explained above, might allow us to:\n\nEither see how we can run fast in parallel on a given dataset and still work out a solution\nOr find out that the dataset cannot be broken the way we have initially chosen, indicating possible ‚Äúepistasis‚Äù, which in itself would be interesting and would have just supposed a few seconds lost here.\nOr ‚Äì and that could happen ‚Äì that the chosen dataset cannot be broken down as we would like it to, for an LCS to come up with a proposed classification solution as there is not a very clear-cut solution (think Yahoo finance historical data‚Ä¶ Yes, I‚Äôve had a quick look. Suffice to say, I won‚Äôt beat the markets just yet :D:D:D But that‚Äôs not relevant‚Ä¶ )\n\nI‚Äôll work on that parallelizing stuff some more‚Ä¶ Even if that‚Äôs not part of the package itself, it‚Äôs more about data preparation and how to use the package, I am aware. But demonstrating how the future RLCS package can hopefully be used and competitive compared to a Neural Network (for instance) is part of my overall objective, I guess.\nFun stuff (to me) all of it :)"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I‚Äôve already tried to parallelize a bit the training of a supervised-learning LCS. It kinda‚Äô works‚Ä¶ But the more I think about it, the more I‚Äôm convinced it‚Äôs not as easy as I thought.\nI initially thought of one way of parallel processing: Break the training samples in N (as many as CPU cores you wanna use) subsets, train N LCS.\nThen ‚Äúmerge‚Äù, subsume, remove duplicates, and‚Ä¶ Repeat.\nIdeally you would bootstrap the thing with an initial epoch on all the training set, just to have relevant coverage to start with.\nBut there is an issue: What if your dataset is ‚Äúnot perfect‚Äù? Let‚Äôs suppose there isn‚Äôt a clear-cut separation between classes to be found, but rather a fuzzy separation. So there aren‚Äôt perfect rules to be discovered. Instead, in each subset, some rules work, but THEN when these are exposed to the complete dataset, the discovered classes in one subset is correct in that subset, but NOT in the overall training data?\nYou end up with classifiers that‚Ä¶ Don‚Äôt work over test sets.\nAt least that‚Äôs my current conclusion after trying it with some messier-than-perfect datasets."
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea-version-2",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-idea-version-2",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "So if breaking the dataset in sub-sets does not work‚Ä¶ What about breaking the data POINTS in different parts?\nIn my earlier example, I have tested (quite successfully, in my view) my RLCS on a simplified MNIST exercise.\nThere, I was training to classify images. The original images where too big, in their equivalent bit-strings representation, for my laptop to work on in acceptable runtimes (for my taste). I ended up compressing said images into 49 bits strings per image. It was then manageable, and it worked quite good, in little over 20 seconds of training (with the correct hyper-parameters).\nBut could I instead break each images in say 7x7 lines, and train on each line separately (and in parallel), and then train one more LCS on the result of that (so 7 outputs would become the input states for a new classifier system, another 7-bits train set, so to speak)?\nI would then in effect, in terms of overall processing time, train approximately twice on 7 bits strings, instead of once on 49 bits strings.\nWould that even work? More on that later.\nBut first‚Ä¶"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-caveat",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#the-caveat",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "Epistasis!\nWell, at least that‚Äôs how I understand it right now: Two (or more) different locations of the ‚Äúgenome‚Äù are required to define the phenotype. In my simple RLCS, in that case two or more bits in a state interact in a more or less complex manner so that the class is decided by said interaction. THEN, the second approach would break the assumptions!\nThink of the ‚ÄúMux6‚Äù example explained in this Blog a few entries ago. Two bits pointed to a position, which value was the class to be predicted. In that example, the first two bits pointed to a position in the last 4 bits.\nIF I were to break the 6 bits states in say two sub-strings of 3 bits each, thinking I can learn from each sub-string and then recompose (see ‚Äúidea version 2‚Äù above)‚Ä¶ I would fail miserably: you need the complete 6-bit strings in that example for the LCS to come up with the relevant rules!\nThat said‚Ä¶"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#counter-caveat-not-bit-4-in-10-bits-states",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#counter-caveat-not-bit-4-in-10-bits-states",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I‚Äôve shown ad-nauseam by now the ‚Äúnot-bit-4‚Äù example. In the example so far, I have used 5 bits strings. So the ‚Äúsearch space‚Äù was that of 5 bits. And the solution was simple in that one bit would suffice to predict the whole class.\nBut that was arbitrary, and meant to be simple.\nWhat if we were to be faced with the exact same rule to be found in 10-bits strings? The solution would be just as simple (two rules would suffice), but the search space would be comparatively‚Ä¶ Well, way bigger.\nIn 5-bits, the not-bit-4 is fully expressed with 32 states.\nIn 10-bits, the SAME not-bit-4 classifier works with‚Ä¶ 1024 states (total). For a perfect data mining exercise, you need to find a ruleset that perfectly matches all 1024 states, and you need to try combinations of 3 possible values per-bit (our ternary alphabet: {‚Äò#‚Äô, ‚Äò0‚Äô, ‚Äò1‚Äô}). If I‚Äôm correct, that could be up to about 3^10*2 (including class) 59049-1 possible unique classifiers to potentially check (coverage ensures here that not-bit-4 is respected, so that‚Äôs only half the overall 10 bits strings + class, as half are illegal, making it a 3^10 matter‚Ä¶ And the ‚Äò##########‚Äô option is not valid for us‚Ä¶). Well, approximately anyway.\nAnd let‚Äôs remember, an LCS is expected to output a combination subset of these ~59K classifiers as an output System, which could be a very large search space indeed‚Ä¶ (In a real world setup, that is.)\nHowever! Well, let‚Äôs try to apply the logic of my ‚Äúapproach number 2‚Äù above to break down the exercise and then see what would happen in such a simple case‚Ä¶\nImagine I break down the exercise into 3 ‚Äútrainings‚Äù:\n\nThe first 5 bits (which in fact are more than enough) per state, creating \\(RLCS_1\\)\nThe last 5 bits per state (completely irrelevant, but we don‚Äôt know that upfront), creating \\(RLCS_2\\)\nThen the outputs of the above to train a third LCS, \\(RLCS_{combined}\\)\n\nThe hope is that we can train the first two in parallel, and use the predictions of these two in combination to then train a third classifier (which incidentally here would be exposed to 2-bits states). So I‚Äôd expect to reduce the search space quite a bit. And hence the training times.\nWell, with no further ado, here come the initial results, and heck!\nParallel runtime using the approach described here to break the problem in just two 5-bit string sub-problems, less than 5 seconds:\n\n\n\nbreaking original problem in two makes it fast!\n\n\nCompared to original sequential time on the 10-bit string, more than 4 minutes:\n\n\n\nThat worked, just‚Ä¶ Very slow, comparatively\n\n\nThe result however is slightly more difficult to interpret (that is, in the current version of this exercise, which I just wanted to see working, but it is not clean‚Ä¶). In particular, each subset I have to keep (for now) better than chance results (anything &gt; 0.5), but that‚Äôs just because it‚Äôs a dirty example, it can be made cleaner for sure:\n\n\n\nFast results, but requires a bit more work to interpret\n\n\nStill, from ‚Äú&gt; 4 minutes‚Äù to ‚Äú&lt; 5 seconds‚Äù! But then again, this will only works for such a problem where the solution did exist in a subset of original bit-strings."
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#trick-why-so-fast",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#trick-why-so-fast",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "Well, for one, I am NOT in fact browsing in parallel through 1024 states!\nSee, each sub-string of 5 bits and its class, as a subset can be de-duplicated! There are a lot of repeated instances, say in the first subset, whereby all entries that say ‚Äú00000 -&gt; class 1‚Äù appear 32 times (one for each of the continuing 5-bit strings for the rest of the original state).\nThat kind of deduplication can reduce processing efforts indeed :)"
  },
  {
    "objectID": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#conclusion",
    "href": "posts/2025-01-26_About_Parallelizing_RLCS_Training/index.html#conclusion",
    "title": "RLCS: training in parallel?",
    "section": "",
    "text": "I haven‚Äôt tested it yet, but I would wager this approach should work just fine on images too, as described above, breaking images in lines and then combining classifiers‚Ä¶\nParallelizing training is an obvious idea (to me) in concept: For some examples, training is slow, and so running it in N parallel processes must be faster (for, you know, complicated examples).\nBut as explained in some detail today, it‚Äôs not that simple! It might work in some cases, as explained, but in the ‚Äúreal world‚Äù, we wouldn‚Äôt know upfront whether it would in fact work, would we? And so we might have to do some trial-and-error‚Ä¶\nBut even just running tests on new datasets, as explained above, might allow us to:\n\nEither see how we can run fast in parallel on a given dataset and still work out a solution\nOr find out that the dataset cannot be broken the way we have initially chosen, indicating possible ‚Äúepistasis‚Äù, which in itself would be interesting and would have just supposed a few seconds lost here.\nOr ‚Äì and that could happen ‚Äì that the chosen dataset cannot be broken down as we would like it to, for an LCS to come up with a proposed classification solution as there is not a very clear-cut solution (think Yahoo finance historical data‚Ä¶ Yes, I‚Äôve had a quick look. Suffice to say, I won‚Äôt beat the markets just yet :D:D:D But that‚Äôs not relevant‚Ä¶ )\n\nI‚Äôll work on that parallelizing stuff some more‚Ä¶ Even if that‚Äôs not part of the package itself, it‚Äôs more about data preparation and how to use the package, I am aware. But demonstrating how the future RLCS package can hopefully be used and competitive compared to a Neural Network (for instance) is part of my overall objective, I guess.\nFun stuff (to me) all of it :)"
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html",
    "href": "posts/2024-10-20_Interpolation_Example/index.html",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "",
    "text": "As I want to make sure I keep posting here, I keep looking for things to write about in my own personal ‚Äúlibrary‚Äù (book shelves).\nOne topic I want to make sure I don‚Äôt forget about is ‚ÄúNumerical Methods‚Äù. (Another one is graph theory, another one is anomaly detection‚Ä¶ There are a few topics like that :D).\nAnyhow. Today: Interpolation.\nMore specifically, Lagrange polynomials. Which is just one of many possible approaches (and has its own drawbacks)."
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#intro",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#intro",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "",
    "text": "As I want to make sure I keep posting here, I keep looking for things to write about in my own personal ‚Äúlibrary‚Äù (book shelves).\nOne topic I want to make sure I don‚Äôt forget about is ‚ÄúNumerical Methods‚Äù. (Another one is graph theory, another one is anomaly detection‚Ä¶ There are a few topics like that :D).\nAnyhow. Today: Interpolation.\nMore specifically, Lagrange polynomials. Which is just one of many possible approaches (and has its own drawbacks)."
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#concept-of-interpolation",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#concept-of-interpolation",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Concept of Interpolation",
    "text": "Concept of Interpolation\nSo first, what is ‚ÄúInterpolation‚Äù. Most people interested in Statistics and Machine Learning (mostly wrt time series and predicting the future of stock markets, say‚Ä¶) will have heard about Extrapolation. And that‚Äôs mostly it: Take a distribution or timeline and ‚Äúpredict‚Äù what the future of it should look like. (To keep things simple, that is).\nInterpolation is about finding a distribution in between known points. So not about the ‚Äúfuture‚Äù (in the above comparison) but instead about the ‚Äúpresent‚Äù, if you will. It‚Äôs about ‚Äúfilling the gaps‚Äù. Or trying to üôÇ"
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#lagrange-polynomials",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#lagrange-polynomials",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Lagrange Polynomials",
    "text": "Lagrange Polynomials\n‚ÄúFor a given set of N+1 data points, we want to find the coefficients of an Nth-degree polynomial function to match them [‚Ä¶]‚Äù\n\nLagrange polynomials are somewhat intuitive because each term x_m can rather easily be shown to correspond exactly to y_m (by definition of L_{N, m} above).\nAlright, and the math above leads me to the following R code (my own, but not saying ‚Äúperfect‚Äù, of course :D):\n\n(The source code can be found here)\nAnd to validate that, I take the same example points as that of the reference book, and we can plot the results:\n\nSo we see a POSSIBLE approach here, which ‚Äúlooks‚Äù sensible (but MIGHT VERY WELL be wrong)."
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#conclusions",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#conclusions",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Conclusions",
    "text": "Conclusions\nToday was about one of many approaches, a short introduction to the topic of Interpolation (as ‚Äúopposed‚Äù, though not really, to extrapolation).\nThere are several alternatives, namely Newton Polynomials, for which improvements can be obtained by better choosing sample points (Chebyshev nodes). Or the well appreciated Cubic Splines.\nNewton Polynomials for instance work with recursion, and so might come in handy if one expects to ADD new reference data points in the future, because with Lagrange Polynomials, all calculations need to be redone from scratch.\nAll of which might make for a nice few future entries of this blog üôÇ"
  },
  {
    "objectID": "posts/2024-10-20_Interpolation_Example/index.html#resources",
    "href": "posts/2024-10-20_Interpolation_Example/index.html#resources",
    "title": "Random Exercises: Interpolation with Lagrange Polynomials",
    "section": "Resources",
    "text": "Resources\n‚ÄúApplied Numerical Methods Using MatLab, 2nd Edition‚Äù, Ed. Wiley, by W. Y. Yang, W. Cao, J. Kim & al."
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html",
    "title": "While on the Train: Cellular Automata",
    "section": "",
    "text": "I decidedly LOVE riding on a train: For whatever reason, it‚Äôs one particular moment where I somehow focus a bit more than usual. It‚Äôs usually just a few hours, reasonably quiet environment, a bit more spacious than say airplanes (airplanes are just not the same, even with earplugs), no TV to distract my attention, the excuse of limited/imperfect mobile coverage‚Ä¶ You name it. I just don‚Äôt know, but it works, I feel motivated to code & run experiments while on a train‚Ä¶"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#intro",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#intro",
    "title": "While on the Train: Cellular Automata",
    "section": "",
    "text": "I decidedly LOVE riding on a train: For whatever reason, it‚Äôs one particular moment where I somehow focus a bit more than usual. It‚Äôs usually just a few hours, reasonably quiet environment, a bit more spacious than say airplanes (airplanes are just not the same, even with earplugs), no TV to distract my attention, the excuse of limited/imperfect mobile coverage‚Ä¶ You name it. I just don‚Äôt know, but it works, I feel motivated to code & run experiments while on a train‚Ä¶"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#introducing-the-cellular-automata",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#introducing-the-cellular-automata",
    "title": "While on the Train: Cellular Automata",
    "section": "Introducing the Cellular Automata",
    "text": "Introducing the Cellular Automata\nOne thing that I liked about the book I recommended a couple of days back (see last entry before this one) is that it provides nice and easy examples (but in Python), and then reasonable exercises. I loved to read through most of them (and some of the math sections), but reading is not that fun in this case, I wanted to try it out (of course!).\nThe following is the result of a simple R implementation of ‚ÄúCellular Automata‚Äù for simulation of a ‚Äúfire spread‚Äù in a theoretical forest setting. The identification of the ‚Äúneighbour‚Äù trees in fire follows the Von Neumann definition of ‚Äúneighbours‚Äù in this 2D configuration.\nHopefully the video is quite self-explanatory. You start with ONE burning tree, and then let time pass‚Ä¶ üôÇ"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#visualization-trick",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#visualization-trick",
    "title": "While on the Train: Cellular Automata",
    "section": "Visualization trick",
    "text": "Visualization trick\nOne thing I learnt today is how to quickly draw a matrix into a picture. I hereby recommend you look into the ‚ÄúMBCbook‚Äù R package, for its function ‚Äúimshow()‚Äù. Which incidentally I found while looking for alternatives to the Python function‚Ä¶ imshow! (So yeah, it was a fast search that one‚Ä¶)"
  },
  {
    "objectID": "posts/2023-11-08_WhileOnTheTrain/index.html#conclusions",
    "href": "posts/2023-11-08_WhileOnTheTrain/index.html#conclusions",
    "title": "While on the Train: Cellular Automata",
    "section": "Conclusions",
    "text": "Conclusions\nI hope you enjoyed it (I could definitely make it faster, and more or less dense a forest, and change the neighbours identification, and what not‚Ä¶ I‚Äôve tested this a few times with a few parameters).\nAt least to me, this was for no-good-reason quite‚Ä¶ Satisfying üôÇ\nAnd I‚Äôm not sharing the code (not yet anyway) just because this was a first, horrible pasted-together step by step implementation, full of slow and nested ‚Äúfor loops‚Äù, to-be-improved matrix indexing, not enough functions‚Ä¶ And well, just not quite ‚Äúpresentable‚Äù. Find the code linked below. But it does work just fine though üôÇ\nCode on my GitHub account"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "",
    "text": "So as I keep working on an upcoming presentation I shall give in a few weeks, I thought I‚Äôd prepare something a bit more ‚Äúlive‚Äù than just a PPT with Memes (although who doesn‚Äôt like a few Memes in a PPT?).\nMy current idea is to talk about some ‚ÄúMachine Learning Key Concepts‚Äù, and then bring it back to Cybersecurity applications.\nSo part of it could be about two key things:\n\nSupervised vs Unsupervised Learning\nSymbolic vs Sub-Symbolic ‚ÄúAI‚Äù\n\nWhich I feel help frame a bit what we‚Äôre talking about when we talk about Machine Learning.\nToday, we‚Äôll do some review, and then talk about Clustering. Here an output of one such algorithm, in 3D:"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#section",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#section",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "",
    "text": "So as I keep working on an upcoming presentation I shall give in a few weeks, I thought I‚Äôd prepare something a bit more ‚Äúlive‚Äù than just a PPT with Memes (although who doesn‚Äôt like a few Memes in a PPT?).\nMy current idea is to talk about some ‚ÄúMachine Learning Key Concepts‚Äù, and then bring it back to Cybersecurity applications.\nSo part of it could be about two key things:\n\nSupervised vs Unsupervised Learning\nSymbolic vs Sub-Symbolic ‚ÄúAI‚Äù\n\nWhich I feel help frame a bit what we‚Äôre talking about when we talk about Machine Learning.\nToday, we‚Äôll do some review, and then talk about Clustering. Here an output of one such algorithm, in 3D:"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#simplified-definition-of-machine-learning",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#simplified-definition-of-machine-learning",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Simplified definition of Machine Learning",
    "text": "Simplified definition of Machine Learning\nNow I‚Äôm probably NOT the right source for you to learn this, so I really suggest you read about that somewhere else. The wiki puts it a bit like so:\n‚ÄúThe study of statistical algorithms¬†that can learn from¬†data¬†and¬†generalize¬†to unseen data, and thus perform¬†tasks¬†without explicit¬†instructions.‚Äù\nThere are quite a FEW THINGS in that sentence right there. But for today:\n\nIn traditional programming, a person WRITES A PROGRAM, that receives INPUT (say a picture), and generates an OUTPUT (say‚Ä¶ ‚ÄúDog‚Äù or ‚ÄúCat‚Äù)\nIn a Machine Learning approach, a person PROVIDES (LOTS OF) INPUTS AND CORRESPONDING OUTPUTS, and the COMPUTER CREATES THE PROGRAM (usually then called a ‚Äúmodel‚Äù).\n\nThe above is specifically applicable to ‚ÄúSupervised‚Äù learning, but nevermind that, the key here is: The computer CREATES the MODEL that we (humans) can then apply to new data."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-unsupervised-learning",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-unsupervised-learning",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Why Unsupervised Learning?",
    "text": "Why Unsupervised Learning?\nIn Cybersecurity, a relevant part of the job for ML can be about detecting anomalies.\nOften times, you don‚Äôt get pre-trained neural networks applicable to your scenario. Or more simple: you don‚Äôt have access to relevant ‚Äúbig data‚Äù (which would help with training your models, indeed), i.e.¬†people (companies) rarely share detailed data (network packets, logs, configurations, etc.) of their breaches. Understandable.\nAnd so ‚Äúunsupervised‚Äù learning might make sense in that scenario. Unsupervised Learning is about discovering structure in your data, that you might not have known about upfront."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Warning",
    "text": "Warning\nAlso there are lots of potential issues with leveraging ML, but two possibly relevant ones would be:\n\nImbalance between classes (hopefully you have little data as examples of real attacks on your network, and a LOT of ‚Äúnormal traffic‚Äù data, for instance),\nBase-rate fallacy (a SOC analyst might, in certain settings, work most of their time on false-positives)\n\nThat said, let‚Äôs keep it simple for today, we will keep it clean, no complications (yet, anyway)."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#symbolic-vs-sub-symbolic",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#symbolic-vs-sub-symbolic",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Symbolic vs Sub-symbolic",
    "text": "Symbolic vs Sub-symbolic\nSimplifying A LOT, let‚Äôs just say for today that ‚ÄúSymbolic‚Äù can be read by a Human, and so it could look like sets of rules of the type:\nIF (A & NOT B) THEN (ACTION X)\nWhere a person could read A (‚Äúnumber of Errors in 1‚Ä≤ log file &gt; 100‚Äù), B (‚Äúless than 10 errors are of type ‚Äòlogin failed‚Äô‚Äù), X (‚ÄúBlock originating IP on Firewall‚Äù). (Note the negation of B in the expression above ;))\nPutting together many of these rules a person COULD setup a reactive security configuration for a firewall based on monitoring logs. I mean, conceptually, why not?! That would be an ‚ÄúExpert System‚Äù, as they called them in the 80‚Äôs.\nOh: And you COULD have ‚ÄúMachine Learning‚Äù on top of Rule-based systems. For example one interesting field (to me) that somehow has received little attention so far is that of the ‚ÄúLearning Classifier Systems‚Äù (LCS)‚Ä¶ But that‚Äôs for another day.\nWhen you enter the realm of Neural Networks, Dimensionality Reduction (say PCA on TF-IDF), BackPropagation, non-linearity, differentiable functions, etc., you quickly leave the realm of ‚Äúhuman readable‚Äù, and you enter the world of vectors, matrices, tensors‚Ä¶¬†In these settings, you use numbers, linear algebra, and the concept of distance.\nFor instance, distances between the ‚Äúcalculated class‚Äù and ‚Äúreal class‚Äù for a set of entries (say, images of cats and dogs, or log files, or‚Ä¶), trying to reduce these distances would mean trying to reduce the prediction error. Said like that, it is probably a bit confusing. But to be perfectly clear: That last sentence is a BIG part of what supervised machine learning with Neural Networks is all about! (More exactly in this case the goal is to minimize the difference between predicted and real class, or predicted and real value)\nLet‚Äôs just make a note at this point, then: Sub-symbolic is the domain of neural networks, a world of algebra and calculus, weights and thresholds, which often are hard to translate into ‚Äúhuman-readable rules‚Äù. And more specifically in the case of the current trend with deep neural networks (which are truly an impressive thing!), it‚Äôs a big issue, because there is a problem with how we can UNDERSTAND what the algorithm does. And that introduces things like lack of trust, issues with responsibility, and not being able to explain why something works (or, usually more to the point, why something suddenly DOES NOT work).\nBut the goal here and today is not to explain the details (‚Äúwhy backpropagation expects differentiable activation functions, for gradient descent, and the Chain Rule‚Äù ‚Äì or ‚Äúwhy ReLU works so good for training a NNet, but it‚Äôs not a differentiable function, and so people use approximations like leaky ReLU‚Äù‚Ä¶ All that might be a bit much ‚Äì Me at least, I still often need to come back to my books each time I want to explain these things correctly‚Ä¶ So some other time :D).\nToday I‚Äôll focus on the concept of distance between points, and leverage that to identify ‚Äúclusters‚Äù of points (and we‚Äôll mention multi-dimensional spaces real quick)."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#clustering",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#clustering",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Clustering",
    "text": "Clustering\nOne type of ‚ÄúUnsupervised Learning‚Äù is what is called clustering. The main idea is to look at data and to try and create ‚Äúgroups of similar data points‚Äù. That‚Äôs it. That‚Äôs what Clustering is all about.\nRight, but‚Ä¶ How?\nSo let‚Äôs see:\nIf a = 20, b = 21, c = 99 and d = 100‚Ä¶ Would you agree you could possibly say ‚Äúa is nearer from b than from d‚Äù. And iterating, you might conclude:\n\nGroup 1: a, b\nGroup 2: c, d\n\nDoes that make sense? Hopefully YES üôÇ\n\n\n\n1d and 3 groups of points\n\n\nLet‚Äôs move on to two dimensions. You get a set of points (imagine, for Cybersecurity, I don‚Äôt know: for each point representing a machine on a network, the x coordinate represents the number of TCP Packets sent by the machine from its TCP Port 80 in the last minute, and the y coordinate represents the number of TCP Packets sent by the same machine from its port TCP 443 in the last minute).\nSo now you might have two sets of points that ‚Äúcluster together‚Äù, some with very little activity on both axis, that is: (x, y) = 0, and others (maybe only a few), that have a range of numbers but overall have maybe lots of activity as per both axis, so say for example (x &gt; 1000, y &gt; 1000).\nLet‚Äôs apply the same logic as above. There will probably be two clusters, one of which might have many points but overlapping, and the other a cloud of points on the top right‚Ä¶ Representing Web Servers.\nThat‚Äôs probably a very dumb example, but it serves a purpose: You COULD identify groups of machines in these two dimensions. Distances here would probably use Euclidean Distance, but if you understand it visually, good enough for today.\n\n\n\n2d and 2 groups of points\n\n\nAnd with more (very similar) dimensions, you might be able to discover groups of machines that are similar to one another, but a bit different from those of another group‚Ä¶\nHere, I just gave you an algorithm to group machines and separate Windows from Linux, or Clients from Servers, or Web from Mail from LDAP from NTP from DNS servers‚Ä¶ Obviously, the above categories are a bit‚Ä¶ not great, well because most of the time you will KNOW what the machines are to begin with. But what if all you have to work from is a tcpdump file?\nLet‚Äôs visualize this, shall we?"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#dbscan",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#dbscan",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "DBScan",
    "text": "DBScan\nOne (of MANY) algorithms out there to do clustering is DBScan. Its very name says most of it: ‚ÄúDensity based Spatial Clustering‚Äù.\nI‚Äôm not going to reinvent the wheel today, and we‚Äôll go right ahead and leverage the dbscan R package and its documentation. The code is here.\nSo first, we‚Äôll generate a set of seemingly almost-random points in a 2-dimensional space.\n\nVisually, a person can already tell there seems to be some structure in there, some groups. How many might be a bit of a judgement call, but still.\nLeveraging a number of neighbours (say, 4 nearest points to identify a group)¬† to identify an ‚Äúelbow‚Äù of the separation of the groups, we can set a ‚Äúnoise threshold‚Äù to the above whereby if a node is too removed from a group, it could be considered as SEPARATE.\nLet‚Äôs see:\n\n\n\nidentifying noise in clustering\n\n\nIn the above, there is a ‚Äúclear‚Äù change in trend in the line that finds said distances, at 0.85 approximately (red line) that identifies regions of LOW DENSITY of points, that the DBScan algorithm would then propose as a limit to separate OUTLIERS from the rest of clustered points.\nIt‚Äôs a bit of a mess to write down, but hopefully the results are self explanatory:\n\nHere we color the groups of points by cluster, or what the algorithm has proposed as such. Again, the only concept in use was the distance to other points. A detailed look in the last picture would show that maybe something is amiss, at least one point had x &lt; 0 before, and it doesn‚Äôt show up here.\nThat‚Äôs an identified outlier.\n\nLet‚Äôs take a minute here: We‚Äôve identified stuff that goes together, so ‚Äúclusters‚Äù.\nBut one key aspect (value) of DBScan over some alternative algorithms for clustering is, it can help with ANOMALY DETECTION. Indeed, that‚Äôs why I have chosen this algorithm today (the typical intro to clustering would have probably focused on KMeans first :D).\nSo now, we have an ‚Äúautomated ML algorithm‚Äù that receives coordinates of points, and is capable of identifying groups of points, AND points that seem to not quite fit anywhere.\nRemember earlier when we mentioned ‚Äúimbalance‚Äù of prevalence of ‚Äúnormal traffic‚Äù vs ‚Äúattack traffic‚Äù on a corporate network? Well, this is why I mentioned it. With a little luck, what a DBScan output tells us are ‚Äúoutliers‚Äù is something that is UNUSUAL, and HOPEFULLY that‚Äôs actually identifying attack-related data for us!\nOK, back to the demos."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-bother",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#why-bother",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Why bother?",
    "text": "Why bother?\nOK, so beyond finding things out about your data, the data you have‚Ä¶ What if you could get information from new data (of the same kind, that is)?\nAfter all, you‚Äôve identified groups. And that‚Äôs cool, and maybe you‚Äôve learned that somehow ten computers seem to behave similarly, and quite differently from another set of 50 computers, on the same network. And maybe that leads you to do some digging and conclude that all 10 of the first group were DB servers, and the other were front-end stuff (Idk, Apache). All from network dump files. Not too bad.\nBut now you receive a new dump file, which you‚Äôre told contains network traces from other computers. Wouldn‚Äôt it be cool to then just feed that to your ‚Äútrained‚Äù model (which, remember, was actually unsupervised to begin with), and get it to tell you ‚Äì like a supervised algorithm would: That new machine is a ‚ÄúGroup 1‚Äù machine (and so you can deduce it‚Äôs a DB server).\nI know, I know. Just look at ports, and you would know, fair enough. Plus, it‚Äôs not clear the example above would even work (there are MANY considerations in there). Anyhow, let‚Äôs take your ‚Äúpre-trained‚Äù model from above, and see what would happen with say 12 new points:\n That‚Äôs it: New data, and without you having to look at it, the machine will tell you to which group each entry belongs. That‚Äôs the cool thing about Machine Learning üôÇ"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#going-3d",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#going-3d",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Going 3D",
    "text": "Going 3D\nAn almost identical exercise, but in 3 Dimensions. I just want to show it so that we can all agree on one thing: We human can conceptualize up to three dimensions. But with this next visualization, I hope to show one important idea: There is nothing precluding an algorithm from going and work into ‚Äúhigher dimensions‚Äù. We can easily visualize groups in 1-D, 2-D, now 3-D (maybe, on a screen, with the help of some animation). But 4-D, or 1000-D, is NOT a problem for a computer!\nOK, so in 3D, same algorithm, similarly random-generated data points. What DBScan can do is shown at the top of this Blog post üôÇ\n(I just know people are more impressed by 3-D animations than 2D visuals for some reason, and so I put it at the top to keep you interested :D)"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#its-not-magic",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#its-not-magic",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "It‚Äôs NOT magic",
    "text": "It‚Äôs NOT magic\nLet‚Äôs see a very classical example, and how DBScan kinda‚Äô fails. Not really, but still.\nWhen applied to the ‚ÄúIris‚Äù dataset (if you‚Äôve ever studied a bit of data science, you know what it is), DBScan identifies two clusters, while we all know there are 3 types of flowers represented in the dataset.\nThat does NOT mean that DBScan FAILED. It just means that the information it can tell us about that dataset is that one group of flowers is clearly different from the rest. And that‚Äôs OK, although we know it‚Äôs insufficient. BUT YOU NEED TO KNOW IT‚ÄôS NOT MAGIC. From a few data points / coordinates, it‚Äôs still only working with so much information‚Ä¶\nReal groups: 3\n\n\n\nreal iris groups\n\n\nDBScan (with selected parameters)groups: 2 (and a few outlier points)\n\n\n\ndbscan identified iris groups"
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning-2",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#warning-2",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Warning 2",
    "text": "Warning 2\nNOT ALL numbers are ordinals/cardinals. Although 20, 21, 22, 23 might seem nearer from one another than say from 80, 123, or 443‚Ä¶ That doesn‚Äôt mean you can use THAT ‚Äúdistance‚Äù.\nIn Cybersecurity (but in any other field), PLEASE remember DOMAIN KNOWLEDGE can ‚Äúmake or break‚Äù a data scientist. And not knowing why things don‚Äôt work as you expect then is a bad thing. And it‚Äôs not always the algorithm fault.\nWeb is different from NTP, while HTTPS uses cryptography and so does SSH, but‚Ä¶ In context, port TCP 80 is NOT nearer TCP port 22 than it is from TCP Port 443.\nYou‚Äôve been warned."
  },
  {
    "objectID": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#conclusions",
    "href": "posts/2024-10-26_UnsupervisedLearning_DBScan/index.html#conclusions",
    "title": "ML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan",
    "section": "Conclusions",
    "text": "Conclusions\nUnsupervised Machine Learning has potential for applications to Cybersecurity data. Maybe used on network traffic captures or logs, one can identify structure and propose groupings of machines, users (by their activity, accesses, hours, who knows‚Ä¶).\nAlthough we‚Äôve seen one algorithm and how to visualize its decisions with 2- and 3-dimensional data, the ‚Äúsky is the limit‚Äù, and if one can come up with 100 (or 1000) such dimensions (that‚Äôs the concept behind ‚Äúfeature engineering‚Äù), there is nothing precluding our machines to work with that and propose groupings for us (although not all algorithms deal nicely with ‚Äúcurse of dimensionality‚Äù, but that‚Äôs a different topic). In ML, more (quality) data is often a good thing. Also, if one of the 100 dimensions helps us separate perfectly some groups, some ML algorithm will find that and use it for us. Would you visualize manually and study 100 dimensions? 1000?\nAnd that‚Äôs where it‚Äôs powerful: A person might have a hard time grouping hundreds of machines or users while considering several aspects at once, much less when the number of groups or ‚Äúkinds of groups‚Äù to be found are not known upfront‚Ä¶ But that‚Äôd be no issue for a computer üôÇ\nHopefully I can walk some of my colleagues through the above concepts (organised in a PPT) and show them (in R :P) how all the concepts ‚Äúwork‚Äù, and then translate into ‚Äúreal world applications‚Äù.\nMaybe next week I‚Äôll move on to making text into multi-dimensional data points. And then, we‚Äôd be set to apply all the above to text data. Which is quite prevalent in Cybersecurity (CVE descriptions, logs, code‚Ä¶ it‚Äôs all text :D).\n\nResources\nWikipedia as linked above, and dbscan R Package documentation, mostly.\nAlso this about making a video from 3D scatter plot with RGL"
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "The exact same code to do supervised learning can be used for data mining. In fact, I‚Äôve already shown that, with the ‚Äúnot-bit-4‚Äù example.\nThe approach is simple: Instead of training (and then testing with new data), suppose what you want to understand something about your data. Then you can show it all to the LCS for it to come up with rules. If you‚Äôre lucky, your data has hidden rules that you didn‚Äôt know about.\n\n\n\nSo we‚Äôve seen it already, the ‚Äúnot-bit-4‚Äù example. Well, the LCS can try to find out for you:\n\n\n\nit looks easier like this, doesn‚Äôt it?\n\n\nBut what if what you have is this? Not so easy to find the rule manually, right?\n\n\n\nWhat if your data looks like so?\n\n\nMaybe too easy? But it‚Äôs the concept that matters! Let‚Äôs try a different example. See if you can spot it:\n\n\n\nThis one is just slightly trickier‚Ä¶\n\n\nThe LCS only takes a few second‚Ä¶\n\n\n\nprocessing for simple cases is rather fast\n\n\nAnd finds the rules (4 in this case):\n\n\n\nBit 4 XOR bit 5\n\n\nThat‚Äôs how the LCS here expresses ‚Äúbit 4 XOR bit 5‚Äù. And yes, that was it.\nEnough with the examples. But conceptually, the ‚ÄúMux6‚Äù example for the last entry? Same-same.\nAnd more to the point, I had 6 bits inputs here, but what if there are 10 bits? 20 bits? Would you be able to manually find similar rules? It would take me some time :D\nSure, the algorithm is slower with 20 bits instead of 6 (can you blame it?). But it‚Äôs machine time. :)\n\n\n\nI actually have a use-case at work I want to test this on. As soon as I upload a first version of my project to GitHub, I‚Äôll be able to test it there. I have inventories and some things about them I want to know whether I can explain with short, readable rules. I think the LCS might help. Although‚Ä¶ Maybe there is no simple rule to be found, but that‚Äôs worth trying.\nThe trick there will be: How to encode the data into binary strings, as this is what my code accepts‚Ä¶? But I actually know how to go about that.\n\n\n\nWell, I just like the idea that the same Supervised Learning algorithm can be used to do Data mining. And provide interpretable information about some datasets (if there is something to be found, that is).\nTake that, Neural Networks! :D\nAlright, I guess next week I‚Äôll return to working on the package itself."
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#lcs-for-data-mining",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#lcs-for-data-mining",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "The exact same code to do supervised learning can be used for data mining. In fact, I‚Äôve already shown that, with the ‚Äúnot-bit-4‚Äù example.\nThe approach is simple: Instead of training (and then testing with new data), suppose what you want to understand something about your data. Then you can show it all to the LCS for it to come up with rules. If you‚Äôre lucky, your data has hidden rules that you didn‚Äôt know about."
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#demonstration",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#demonstration",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "So we‚Äôve seen it already, the ‚Äúnot-bit-4‚Äù example. Well, the LCS can try to find out for you:\n\n\n\nit looks easier like this, doesn‚Äôt it?\n\n\nBut what if what you have is this? Not so easy to find the rule manually, right?\n\n\n\nWhat if your data looks like so?\n\n\nMaybe too easy? But it‚Äôs the concept that matters! Let‚Äôs try a different example. See if you can spot it:\n\n\n\nThis one is just slightly trickier‚Ä¶\n\n\nThe LCS only takes a few second‚Ä¶\n\n\n\nprocessing for simple cases is rather fast\n\n\nAnd finds the rules (4 in this case):\n\n\n\nBit 4 XOR bit 5\n\n\nThat‚Äôs how the LCS here expresses ‚Äúbit 4 XOR bit 5‚Äù. And yes, that was it.\nEnough with the examples. But conceptually, the ‚ÄúMux6‚Äù example for the last entry? Same-same.\nAnd more to the point, I had 6 bits inputs here, but what if there are 10 bits? 20 bits? Would you be able to manually find similar rules? It would take me some time :D\nSure, the algorithm is slower with 20 bits instead of 6 (can you blame it?). But it‚Äôs machine time. :)"
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#so-what",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#so-what",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "I actually have a use-case at work I want to test this on. As soon as I upload a first version of my project to GitHub, I‚Äôll be able to test it there. I have inventories and some things about them I want to know whether I can explain with short, readable rules. I think the LCS might help. Although‚Ä¶ Maybe there is no simple rule to be found, but that‚Äôs worth trying.\nThe trick there will be: How to encode the data into binary strings, as this is what my code accepts‚Ä¶? But I actually know how to go about that."
  },
  {
    "objectID": "posts/2025-01-25_RLCS_4_DataMining/index.html#conclusion",
    "href": "posts/2025-01-25_RLCS_4_DataMining/index.html#conclusion",
    "title": "RLCS: for Data Mining",
    "section": "",
    "text": "Well, I just like the idea that the same Supervised Learning algorithm can be used to do Data mining. And provide interpretable information about some datasets (if there is something to be found, that is).\nTake that, Neural Networks! :D\nAlright, I guess next week I‚Äôll return to working on the package itself."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html",
    "title": "Classifying URLs",
    "section": "",
    "text": "I‚Äôm still working my way through a ‚Äúpractical demos‚Äù session on ML background for Cybersecurity.\nSee I have a ‚Äúplan‚Äù in my head about how to introduce Machine Learning, including demystifying a bit the math behind it. The ‚Äúplan‚Äù goes a bit like this:\n\nWe‚Äôve seen recently a bit of what unsupervised learning can be about. For Cybersecurity, anomaly detection is a clear use-case. We‚Äôve used the concept of distance between points. But there is more to this category (clustering per-se, but also dimensionality reduction is quite useful, for instance).\nML can be about making a line go through points (nicely). That‚Äôs regression. With sufficient data, you‚Äôre compressing information of relationships between dimensions to predict an outcome. In the simplest form, given x, you can estimate y. And the math is quite simple:\n\\[\ny = \\beta_0 + \\beta_1x\n\\]\nSuch a linear regression is then really of the realm of statistics. As such, there is little of practical application of that thing for us in this simplest form, as in Cybersecurity, many things are expected to be black or white. But maybe it‚Äôs not completely crazy to maybe estimate that the more time a machine is connected to the Internet, the higher the risk of infection (all other things being equal, that is). That could be a growing line. Sometimes things can be approximated with the appropriate transformations on the dimensions (say ‚Äúindependent variables‚Äù) and then used in a linear regression (so the ‚Äúmodel is linear on its parameters‚Äù).\n\n\nI‚Äôll make an entry about linear regression soon(-ish), when I have time.\n\n\nML can also be about making a line separate groups of things nicely, i.e.¬†classifying stuff. In its simplest form, we will have input data, and will want to know in which one of two categories a given input should belong too. That‚Äôs our ‚Äúrun of the mill‚Äù Spam classifier.\n\n\nBoth regression (linear or not) and classification are usually considered to be the two most common supervised learning use-cases.\n\n\nThen you have Reinforcement Learning (aka RL). So far, I have yet to personally come across a real-world cybersecurity application of that, although it is pretty cool a concept! (Actually a friend has demonstrated something with Honeypots and such‚Ä¶) But I‚Äôll admit, I have played little with RL myself for now, and it‚Äôs a pending task of mine. It does sound appealing, and there are some papers out there to use RL for optimizing pentesting processes and such.\nAnd then you have Self-Supervised Learning. The first time I read that I was like ‚ÄúWhaaaat?‚Äù. But that‚Äôs LLMs, as noted at the end of one of my most recent posts. You predict next word in a text, and check what actually comes in the‚Ä¶ Rest of the text. So you only need‚Ä¶ The text. Nifty. But I‚Äôm not too fond of it and have yet to think it through enough to come up with something practical beyond LLMs‚Ä¶\n\nThere is plenty more, but this is one way of putting categories to what ML is about.\n\n\n\nAlright, so I‚Äôll skip some of the above, and today jump right onto this:\nHow does Supervised Machine Learning work, as applied to one cybersecurity-relevant classification use-case?\nSomehow I feel it‚Äôs more fun to frame things a little into practical use-cases, instead of pure math (which I suppose is boring/threatening to quite a few).\nEven then, be warned: This entry is pretty long. But hopefully it contains some important things and my wish is, it‚Äôs also understandable."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#i-have-a-plan",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#i-have-a-plan",
    "title": "Classifying URLs",
    "section": "",
    "text": "I‚Äôm still working my way through a ‚Äúpractical demos‚Äù session on ML background for Cybersecurity.\nSee I have a ‚Äúplan‚Äù in my head about how to introduce Machine Learning, including demystifying a bit the math behind it. The ‚Äúplan‚Äù goes a bit like this:\n\nWe‚Äôve seen recently a bit of what unsupervised learning can be about. For Cybersecurity, anomaly detection is a clear use-case. We‚Äôve used the concept of distance between points. But there is more to this category (clustering per-se, but also dimensionality reduction is quite useful, for instance).\nML can be about making a line go through points (nicely). That‚Äôs regression. With sufficient data, you‚Äôre compressing information of relationships between dimensions to predict an outcome. In the simplest form, given x, you can estimate y. And the math is quite simple:\n\\[\ny = \\beta_0 + \\beta_1x\n\\]\nSuch a linear regression is then really of the realm of statistics. As such, there is little of practical application of that thing for us in this simplest form, as in Cybersecurity, many things are expected to be black or white. But maybe it‚Äôs not completely crazy to maybe estimate that the more time a machine is connected to the Internet, the higher the risk of infection (all other things being equal, that is). That could be a growing line. Sometimes things can be approximated with the appropriate transformations on the dimensions (say ‚Äúindependent variables‚Äù) and then used in a linear regression (so the ‚Äúmodel is linear on its parameters‚Äù).\n\n\nI‚Äôll make an entry about linear regression soon(-ish), when I have time.\n\n\nML can also be about making a line separate groups of things nicely, i.e.¬†classifying stuff. In its simplest form, we will have input data, and will want to know in which one of two categories a given input should belong too. That‚Äôs our ‚Äúrun of the mill‚Äù Spam classifier.\n\n\nBoth regression (linear or not) and classification are usually considered to be the two most common supervised learning use-cases.\n\n\nThen you have Reinforcement Learning (aka RL). So far, I have yet to personally come across a real-world cybersecurity application of that, although it is pretty cool a concept! (Actually a friend has demonstrated something with Honeypots and such‚Ä¶) But I‚Äôll admit, I have played little with RL myself for now, and it‚Äôs a pending task of mine. It does sound appealing, and there are some papers out there to use RL for optimizing pentesting processes and such.\nAnd then you have Self-Supervised Learning. The first time I read that I was like ‚ÄúWhaaaat?‚Äù. But that‚Äôs LLMs, as noted at the end of one of my most recent posts. You predict next word in a text, and check what actually comes in the‚Ä¶ Rest of the text. So you only need‚Ä¶ The text. Nifty. But I‚Äôm not too fond of it and have yet to think it through enough to come up with something practical beyond LLMs‚Ä¶\n\nThere is plenty more, but this is one way of putting categories to what ML is about."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#todays-intro",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#todays-intro",
    "title": "Classifying URLs",
    "section": "",
    "text": "Alright, so I‚Äôll skip some of the above, and today jump right onto this:\nHow does Supervised Machine Learning work, as applied to one cybersecurity-relevant classification use-case?\nSomehow I feel it‚Äôs more fun to frame things a little into practical use-cases, instead of pure math (which I suppose is boring/threatening to quite a few).\nEven then, be warned: This entry is pretty long. But hopefully it contains some important things and my wish is, it‚Äôs also understandable."
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#classifying-urls-into-two-categories",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#classifying-urls-into-two-categories",
    "title": "Classifying URLs",
    "section": "Classifying URLs into TWO categories",
    "text": "Classifying URLs into TWO categories\n\nThe data\nIn Cybersecurity sometimes getting to interesting datasets can be a bit challenging. After all, certain things are usually done behind closed doors. You can probably understand why. Which is why I like for instance this resource, secrepo.com.\nToday, we‚Äôre gathering a seemingly simple dataset: A list of web URLs, very simply tagged as either good or bad. Nothing else. But, mind you, 420 thousand of‚Äôem.\n\nPoint number one: to do ML, it can help to have lots of data. (It‚Äôs not always necessary, but it‚Äôs usually a good idea.)\n\n\n\nThe objective\nToday is about trying to distinguish (really just trying!) to classify URLs (‚Äúwebpages‚Äù, for most) in two categories: Good or Bad. Why? Applications are for your protection, and can be used to recommend you to avoid certain websites, which in turn can be maybe used as a supplementary control for other security measures, such as detecting Phishing emails.\nCan we make our computer tell us if a given URL is good or bad?\nThat‚Äôs it. That‚Äôs our goal for today. Using Machine Learning, of course. So we‚Äôre aiming to implement one (or more) model(s) to classify URLs. Based on data we already have. That‚Äôs supervised learning, more precisely a classifier.\n\nJust to be very clear: This is all part of a preparation to an introduction on ML background for Cybersecurity. ‚ÄúIntroduction‚Äù is the key here: I‚Äôm not aiming for complete, perfect, not even good, as long as I can convey certain concepts that I believe are relevant to grasp an idea at best of how ML works.\nEven the code I put together is‚Ä¶ Well, lacking. It‚Äôs not meant to be production grade.\n\nThere is nothing in the way of test-driven anything\nSome of the regular expressions are simplistic\nSome stuff will be badly filtered\nThe trained models are not good\nThe data is what it is, and I‚Äôm not trying to complement it\n‚Ä¶\n\nPlease don‚Äôt come saying ‚Äúthis is not great‚Äù. I know. I only have so much spare time. This post is only my way to support with contents an interactive session I plan to give soon. There is a lot of good training on ML, Cybersecurity & al.¬†out there. Go find it, if you want formal and/or good, detailed training.\nIf you‚Äôre fine with simply trying to wrap our heads around concepts, do keep reading.\n\n\n\nThe code\nThe code will be on my Github eventually. But for now, as usual, a few blocks of it:\n\nurls_df &lt;- read.csv(\"https://raw.githubusercontent.com/faizann24/Using-machine-learning-to-detect-malicious-URLs/refs/heads/master/data/data.csv\")\n\nurls_df$url &lt;- tolower(urls_df$url)\n## Let's have a quick look, a.k.a. \"EDA\"\ndim(urls_df)\n&gt; [1] 420464      2\ntable(urls_df$label) ## Imbalance, we might want e.g. to under-sample \"good\"\n&gt;  bad   good \n 75643 344821 \n\n\nPoint number two: Imbalance is often bad. Here we have 4.5 times more good entries than bad entries. Now, why could that be bad? Here we‚Äôre going to try to learn from our data. If we keep the imbalance in the dataset, to make things simple, our model could learn that there is more good than bad. And maybe that‚Äôs what we want, but then that imbalance could affect the outcome of our model. Unless we want to use that imbalance for some reason, it‚Äôs probably best to manage it upfront.\n\nHow to remove imbalance? Well, one way (of surely many out there, only I only know a few), is to ‚Äúunder-sample‚Äù some of the over-represented class. Today we‚Äôre going to take proportionally less entries from good to train our model, making then sure that we have roughly half and half, of each class.\nAs per the class, it‚Äôs a binary choice, good or bad. We‚Äôll create a variable to encode that as 0 or 1 (or the other way around, it‚Äôs irrelevant). That‚Äôs just to make things compatible with certain models, as most will expect numerical data.\nurls_df$label_num &lt;- ifelse(urls_df$label == \"good\", 0, 1)\nurls_df$url_length &lt;- nchar(urls_df$url)\n\n## A bit of domain knowledge helps:\nnrow(urls_df[urls_df$url_length &gt; 300,]) / nrow(urls_df)\n[1] 0.001203432\n\nPoint number 3: Domain Knowledge is important. We‚Äôre going to leverage that today quite a bit. To begin with, we have 0.1% of the entries with URL length superior to 300 characters, and to make things cleaner, we‚Äôll assume today these are‚Ä¶ Irrelevant. So we remove them. Our classifier will hence not be trained with such data. And maybe that‚Äôs a bad idea, depending on your goals. For today, everything is fair game, we want to keep things simple.\n\n\n\nFeature Engineering\nHeck. We only have URLs. And a class. How is a machine suppose to go from there?\nLet‚Äôs try to extract something akin‚Äô to a signal out of that. So we‚Äôve got already the length of each URL. And maybe that‚Äôs helpful. Are longer URLs more often bad than good? Well, for real long URLs, maybe a bit. But it‚Äôs not really definitive, is it?\n\n\n\nComparing densities of URL lengths per class\n\n\n\nPoint number 4: Always look at the data. Don‚Äôt just run into the modelling, it‚Äôs not a good idea. Get a feeling of the data you want to work with. I can‚Äôt stress this enough.\n\nLet‚Äôs keep going then. Again, domain knowledge is key. The good news is, most of us have seen thousands of URLs in our lifetime, so maybe we have a little understanding of what we could look for.\nToo many slashes ‚Äú/‚Äù? Too many ‚Äúdots‚Äù? Maybe. So those could be two new ‚Äúdimensions‚Äù. Although maybe these two are already somewhat expressed through the length of the URL? In other words, it might make sense that the longer the URL, the more dots and slashes.\n\nPoint number 5: That‚Äôs a correlation right there, and depending on how much two variables are correlated, maybe you‚Äôre better off with fewer variables. There is a lot of background statistics on this topic. And for ML algorithms, sometimes too many variables is a bad thing, more so if they don‚Äôt add any useful information.\n\nFor today, we‚Äôll keep it. After all, we have for now only what, 3 variables to work with? We need more. I‚Äôm going to save you the pain of going through it all one by one, and propose my own few variables I thought we might consider for training our model, ALL extracted from the URLs themselves.\n\nIP as host: Humans use ‚ÄúDomain Names‚Äù that are readable. You need a DNS entry for that, and you need to register things as the owner for the DNS entry, for legal reasons. So if you skip the DNS step, you can still have an IP address, but it will look like‚Ä¶ An IP. It‚Äôs a bit far-fetched, but I‚Äôd argue if a URL reflects a Public IP, it‚Äôs either known good (ours or provided by some trusted third party), or - more often than not - it‚Äôs a bad indicator.\nURL readability: So it‚Äôs not direct. A URL can of course contain stuff that‚Äôs purely technical. But we usually make an effort to make things readable: variable names, folder names, etc. Bad actors might want to obfuscate stuff or generate random folder or what-not. And so if a URL is mostly unreadable gibberish, I‚Äôd guess it‚Äôs a bad sign. Which we can ‚Äúencode‚Äù as: How many vowels has the URL relative to its length? Does the URL contain things with 4 consecutive consonants (not usual in written english, although not good an indicator in some other languages‚Ä¶)? Again, both things are probably somewhat related, but not necessarily/completely. So I take both.\nIs a port expressly identified in the URL? After the host, a column and number is usually not required for a normal website, it‚Äôs usually a default (443 or 80). So if you see ‚Äúsomehost.somedomain.com:12345‚Äù, something exotic is going on. Exotic for normal web browsing is weird (well, it‚Äôs exotic :D), and so not expected for good stuff.\nWe can keep going: Domain extension, file extension (a URL ending in .exe is a red flag, for sure :D), or more simply how common is either of these, is probably helpful too.\n\nIt‚Äôs not exhaustive (not in the least) but hopefully it makes some sense. From a URL, we‚Äôve put together 14 different variables that way. All chosen from experience, from ‚Äúdomain knowledge‚Äù. (See point number 3 above if it wasn‚Äôt clear before.)\n\n\n\nWe should keep looking at our data‚Ä¶\n\n\nFrom no variables (except considering the URL itself‚Ä¶) to 14. Not too shabby.\n&gt; names(urls_df)\n [1] \"url\"                       \"label\"                     \"url_length\"               \n [4] \"label_num\"                 \"slashes_count\"             \"dots_count\"               \n [7] \"host_is_ip\"                \"vowels_prev\"               \"ends_in_slash\"            \n[10] \"contains_port\"             \"n_params\"                  \"domain_ext\"               \n[13] \"file_ext\"                  \"is_common_domain_ext\"      \"is_uncommon_domain_ext\"   \n[16] \"is_uncommon_file_ext\"      \"has_4_consonants_straight\"\nThere is sooo much more to consider.\nFor instance if you check out the code (if/when I make it available on my GitHub), you‚Äôll see at one point I ‚Äúscale‚Äù the data. That is, I try to put all the variable in comparable orders of magnitude. This is to avoid one variable overshadowing all the others. Something that varies from 0.5 to 0.6 might otherwise be considered less important than something that varies from 3 to 4000. Which is not always true.\nI also make a BAD thing: I transform extensions to ‚Äúfactors‚Äù, and then I encode the levels of the factors as numerical data. This is not great, I know :D\nNamely, factors are not ordered, while two numbers could be, providing ordinal value at least, and distances could be considered, when here there is clearly no such thing. BAD! BAD Nico!\nLook, this is no excuse, but hopefully, if you order things upfront, and then encode to numerical value, say bad entries as factors first, then good, you end up with ordered levels where by lower ones are for bad, and higher for good (or vice-versa). It will turn out wrong for today. This is tricky and let me insist, NOT good practice. As it turns out, I have so many possible extensions (values) in there, that a better approach - such as one-hot-encoding - makes my dataset explode in size and not fit my RAM memory‚Ä¶ And I am just too lazy to work through this for what was meant to be a simple demo. So‚Ä¶ My apologies, I know, it hurts the eyes to see this. Moving on.\n\n\nTraining Models\nOne last concept, and we‚Äôll dive in actual ‚ÄúLearning‚Äù.\n\nPoint number 6: Save some entries for testing you trained model. So say we have 10K entries, of which 5000 are good and 5000 are bad entries. How do you know your trained model ‚Äúgeneralizes‚Äù correctly? If you were to try and evaluate your model on data you used to train it, you couldn‚Äôt know whether it just learnt exactly that case, or if it would work on future data. To verify how it would work on future data, you‚Ä¶ Validate using data not seen during training. There is more to that, too, but that‚Äôll be enough for conceptual understanding today.\n\nOK. At last. As today has been dense (I know, sorry), I‚Äôll train just ONE model on our dataset.\ngood_urls &lt;- urls_df[urls_df$label == \"good\",]\nbad_urls &lt;- urls_df[urls_df$label == \"bad\",]\n## Undersampling \"good\" vs \"bad\"\nsample_urls_df &lt;- rbind(bad_urls[sample(1:nrow(bad_urls), size = 10000,replace = FALSE),],\n                        good_urls[sample(1:nrow(good_urls), size = 10000,replace = FALSE),])\n\n## ...\n\nseparate_sets &lt;- sample(c(TRUE, FALSE), nrow(sample_urls_df), replace=TRUE, prob=c(0.7,0.3))\nt_train &lt;- sample_urls_df[separate_sets, ]\nt_test &lt;- sample_urls_df[!separate_sets, ] # i.e. Not train set...\n\n\nPartitioning Tree, train and test\nHere is how you train a Partitioning Tree in R:\n## A Partitioning tree but WITHOUT the bad trick of extensions encoding\n## And low depth:\ntree_model &lt;- rpart(label ~ url_length + slashes_count + dots_count +\n                        host_is_ip + vowels_prev + ends_in_slash + contains_port +\n                        n_params + is_common_domain_ext + is_uncommon_domain_ext +\n                        is_uncommon_file_ext + has_4_consonants_straight,\n                    data = t_train,\n                    method = \"class\",\n                    control = rpart.control(cp = 0.05))\nAnd here how you visualize, and ‚Äútest‚Äù it:\n&gt; tree_model ; plot(tree_model); text(tree_model)\nn= 13929 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 13929 6933 good (0.4977385 0.5022615)  \n  2) dots_count&gt;=0.3877992 2846  729 bad (0.7438510 0.2561490) *\n  3) dots_count&lt; 0.3877992 11083 4816 good (0.4345394 0.5654606)  \n    6) vowels_prev&lt; -0.6992319 1877  658 bad (0.6494406 0.3505594) *\n    7) vowels_prev&gt;=-0.6992319 9206 3597 good (0.3907234 0.6092766) *\n&gt; t_test$predicted &lt;- predict(tree_model, t_test, type=\"class\")\n&gt; table(t_test[, c(\"label\", \"predicted\")])\n      predicted\nlabel   bad good\n  bad  1431 1636\n  good  611 2393\nNow to the important part: We‚Äôve tested on 30% of the data our model trained on the other 70% of the data. In the above, we‚Äôve also excluded the factor-level-encoded variables because they‚Äôre a bad thing (but as we‚Äôll see in a second, they contain useful information, unfortunately). And we got some results, as such:\nBased on the data, we have trained a partitioning tree that makes mistakes about 37% of the time. As we have balanced our dataset, we know that randomly choosing one class of the other would have led us to 50% error, approximately. Still, not great.\nLet‚Äôs have a look at this ‚Äútree‚Äù:\n\n\n\nA simplistic partitioning tree\n\n\nLow depth, and still, with only two choices, we get a 63% correct classification on unseen data.\n\nOne thing to note, I‚Äôm not sure that this particular implementation of the model in fact uses Shannon‚Äôs information entropy to select nodes (it could use Gini impurity, typically). But suffice to say it could, and that‚Äôs one way a Partitioning Tree could decide which variable to choose first to make a separation in two branches, and then iterate. And I only mention it because that was the topic of last week‚Äôs entry.\n\nIt does look like the number of ‚Äúdots‚Äù in the URL, and our prevalence of vowels (which I explained a bit earlier) are important to help classify our URLs. Take note! Actually, this is a fair point, Trees are nice because they‚Äôre readable by a human. That is, the decisions of this algorithm are explainable, and that‚Äôs a good thing.\nNow without further ado, what better models I have managed to produce, just increasing depth and/or adding the (badly encoded) extension variables:\n\nWith just more decisions (more branches in the tree, i.e.¬†more depth), I got my classification to a 75% correct classification rate.\nAdding the (incorrectly) encoded extension variables, I go up to 80%.\n\n\n\n\nA somewhat better tree, albeit using bad practices"
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#conclusions",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#conclusions",
    "title": "Classifying URLs",
    "section": "Conclusions",
    "text": "Conclusions\nLots of theory covered. And only a bit of practical outcome, as today we just have a (first) model that is ‚Äúbetter than chance‚Äù, although well, far from perfect.\nIn a future post, we‚Äôll probably circle back to this exercise, to see potentially things related to other classification algorithms such as logistic regression, random forests, neural nets, and maybe SVM. Now that most of the theory is covered, it should be shorter, more to the point. (I have them all working already, I just don‚Äôt want to add content for today, it‚Äôs already too much‚Ä¶)\n\nNote: If I have time, I‚Äôll make a Shiny Application, so that you can test whether or not you can beat this simple (bad) model. Fair warning: I don‚Äôt know how the URLs were originally tagged; but I‚Äôm not much better than my very own simple partitioning tree model :D"
  },
  {
    "objectID": "posts/2024-11-17_URLs_Classification_Example/index.html#references",
    "href": "posts/2024-11-17_URLs_Classification_Example/index.html#references",
    "title": "Classifying URLs",
    "section": "References",
    "text": "References\nFor today, only the recommended list of potential datasets for Cybersecurity.\nThe rest is of my own doing. Of course, the Internet, Stack Overflow, Wikipedia, etc. as usual."
  },
  {
    "objectID": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html",
    "href": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html",
    "title": "RLCS Day 1 - Part II: Basic Covering",
    "section": "",
    "text": "So I had a bit of time. And I chose to use it to keep going a bit. I moved a couple of things, created a few more supporting functions, modified them somewhat, and added the covering part.\nCovering needs to happen when for a given training instance (state, class), there is no match in the correct set.\nCorrect set is the subset of the Matching set for which, on top of the condition matching, the predicted class/action is also correct.\nAnd well, it‚Äôs working:\n\n\n\ntesting basic covering functionality\n\n\n\n\n\nNext up is ‚ÄúRule Discovery‚Äù, whereby, in Michigan-style LCS, a GA-like process will take place to create offspring from two parents taken in the Correct set.\nThat will include cross-over (in one way or another) and mutation (with a preset pressure/probability). With these, on top of coverage, the LCS will hence create new valid rules, that will become part of the population, to be evaluated on further training instances.\nOther parts of the algorithm ‚Äì meant to contain the population size ‚Äì will come later. :)"
  },
  {
    "objectID": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#its-sunday",
    "href": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#its-sunday",
    "title": "RLCS Day 1 - Part II: Basic Covering",
    "section": "",
    "text": "So I had a bit of time. And I chose to use it to keep going a bit. I moved a couple of things, created a few more supporting functions, modified them somewhat, and added the covering part.\nCovering needs to happen when for a given training instance (state, class), there is no match in the correct set.\nCorrect set is the subset of the Matching set for which, on top of the condition matching, the predicted class/action is also correct.\nAnd well, it‚Äôs working:\n\n\n\ntesting basic covering functionality"
  },
  {
    "objectID": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#conclusion",
    "href": "posts/2024-12-24_RLCS_Day_2_Almost_Done_v001/index.html#conclusion",
    "title": "RLCS Day 1 - Part II: Basic Covering",
    "section": "",
    "text": "Next up is ‚ÄúRule Discovery‚Äù, whereby, in Michigan-style LCS, a GA-like process will take place to create offspring from two parents taken in the Correct set.\nThat will include cross-over (in one way or another) and mutation (with a preset pressure/probability). With these, on top of coverage, the LCS will hence create new valid rules, that will become part of the population, to be evaluated on further training instances.\nOther parts of the algorithm ‚Äì meant to contain the population size ‚Äì will come later. :)"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html",
    "href": "posts/2024-12-08_A_new_project/index.html",
    "title": "A new project",
    "section": "",
    "text": "Ever since I read about the concept in M. Mitchell‚Äôs book ‚ÄúComplexity The emerging science at the edge of order and chaos‚Äù some time last year (I think around this time of the year‚Ä¶), I have been thinking about this, in the background of my head.\n\n\n\nWhat piqued me curiosity\n\n\nThe weird strings above, the 11##10### thing, might mean nothing to most right now (you have to look a bit further into it all, and I‚Äôll probably try and explain some of it in the future), but it was a revelation to me when I first read it.\nSo yes, I‚Äôve had other fish to fry for some time, but now it feels like I might just have the mental space to shift my focus a bit‚Ä¶\n\n\nSo here is my upcoming project, explained in one picture:\n\n\n\nMy new project in one picture\n\n\n(By the way, a personal note here: buying that left book in physical format from Springer was one ‚Äútough cookie‚Äù, thank goodness I‚Äôm motivated, it‚Äôs quite possibly the priciest book I ever bought from my own pocket, in ‚Ç¨-per-page‚Ä¶ Oh well: It looks it still was the right call for me :D)\n\n\n\nAs I said, I‚Äôve been a bit busy. Full-time job, MSc paper (success!), then summer, learning other stuff, (did I mention full-time job? :D)‚Ä¶ But it is time.\nI also have been thinking for quite some time I should try my hand eventually at creating my own R Package. This was all the more true after the last R conferences I attended recently, where it looks like ‚Äúeveryone is doing it‚Äù in the R (mostly academic) community‚Ä¶ So why not me?\nPlus, I have an idea or two. So what if I‚Äôm not the greatest coder out there? I still do have some tricks in my sleeves ;) I have proved (at least to myself) I can do highly distributed processing using (among other things) R and create my own thing. (My very own algorithm, that was.)\nPlus, I‚Äôm into ML just as much as the next person, but I‚Äôve also played quite a bit with Genetic Algorithms recently. And I believe I have all it takes to put that to good use here. (I‚Äôm just a bit sad this will not involve Network Graphs, the way I have it designed (yet, anyway :D).) I believe I understand how the LCS algorithms might be particularly useful in their own specific way. I can see in my head how they must work, what makes them different, all in spite of not having written one line of code yet.\nNow, it might take a while, indeed. Why, with the Holiday season coming, the day job and all. But I have a sort of a plan in mind, already an architecture (in my head, for now), examples of how I would want to validate the implementation (that last thing, one example with application to cybersecurity and logs classification, in particular, woke me up in the middle of the night a couple of weeks back).\nAnd R packages, from what I have read thus far, are not ‚Äúmagic‚Äù. It‚Äôs going to be work, for sure, but I don‚Äôt see anything I can‚Äôt manage. If anything, it‚Äôs less complicated than I thought. I‚Äôve already played with testthat, microbenchmark, I‚Äôve always separated my code in functions, separated R and C++ code, all that‚Ä¶ So yeah, I think I‚Äôm up to the task.\nAnd finally, a note about the ‚Äúwhy now‚Äù: I can‚Äôt find it done yet!! Which to me is quite incredible. I have found this niche, that nobody (in the R community) has deemed relevant enough thus far, and I am willing to put the work towards filling that niche!\n\n\n\nThis here today is me announcing, basically, my intention to create and (hopefully) get my future new RLCS package onto CRAN. I know, I know, I‚Äôm getting WAAAAY ahead of myself here.\nAnd I realize me publishing this entry might very well mean someone beats me to it. If that happens, well‚Ä¶ I‚Äôm already convinced my implementation will be ‚Äúcompetitive‚Äù, so‚Ä¶ let‚Äôem!\n(Plus, why would anyone decide to go for this now? It would just make no sense whatsoever - if not to bother me. If that‚Äôs you: Just give me a few months, and you‚Äôll save yourself the trouble :D)\n\n\n\nIt turns out, for some reason, I know I will. Obviously, I don‚Äôt ‚Äúknow know‚Äù, it‚Äôs more of a feeling.\nI am enthused, to begin with. I have the knowledge. Time is not an issue, although it will take me probably a few months (my best estimate right now, given my current known agenda, is 3-4 months for a functional package, plus whatever time it takes for submission to CRAN of a ‚Äúv001‚Äù).\nMoreover, I somehow needed a new, rather difficult, interesting long-term personal objective. I feel this will help me get in the ‚Äúflow‚Äù, which is a great feeling (but hard to come by).\nAlso, I have this crazy idea that, once the thing works, I can dive further into it and study it towards a bigger goal, maybe research, ideally even (who knows) a PhD‚Ä¶?\n\n\n\nI‚Äôve done my first (of two) presentation about ‚ÄúBackground of ML for Cybersecurity‚Äù. I actually had trouble with some of the demos during the first session :S But not because of the demos themselves, just, ‚Äúheat of the moment‚Äù, computer resources, demo effect, and so on. Nothing major.\nThankfully, I get a second chance, as there is a second session coming. And I have time to prepare some more examples, so that‚Äôs good too, the second session will be all-the-more interesting.\nBut because I have some time, I‚Äôm making a pause there and re-focusing on this new project of mine as a personal interest thing.\n\n\n\nI realize nobody cares, but heck: I do. I need to ‚Äúmigrate‚Äù, move everything (while reviewing each thing) from the old blog to this new setup, because.. Well, because this platform is more convenient, and ‚Äì let‚Äôs be honest ‚Äì cheaper‚Ä¶ After all, it‚Äôs one thing to write for one own‚Äôs pleasure, and quite another to spend money with no return year after year just to maintain it.\nSo yes, I need to organize myself to minimize future costs a bit (it just is the sensible thing to do, economically).\n\n\n\n**This is important: I still don‚Äôt really know why this family of algorithms have received sooo little attention**, beyond the fact that ‚ÄúConnectionism‚Äù has received a lot of said attention (deservingly, for sure).\nBut I‚Äôm curious about whether LCS can be made (ideally equally) valuable. And I have the personal conviction that I am in the right position to dig further into it.\nAnd so it is time, I‚Äôm in an ideal position, and I‚Äôm motivated.\nNow all I have to do is‚Ä¶ Start!\n(To start hear means also put thoughts to paper, design, code, test, document‚Ä¶ Don‚Äôt expect lots of news too soon, either! I‚Äôm not in a hurry, nobody should be: this thing has been known since the 70‚Äôs and it looks like few - aside from noted exceptions - have cared about it thus far‚Ä¶)\n\n\n\nThis is the best intro explaining the algorithm that I found thus far - and if anything it confirmed I was on the right path. It just makes a lot of sense to me!"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#the-new-project",
    "href": "posts/2024-12-08_A_new_project/index.html#the-new-project",
    "title": "A new project",
    "section": "",
    "text": "So here is my upcoming project, explained in one picture:\n\n\n\nMy new project in one picture\n\n\n(By the way, a personal note here: buying that left book in physical format from Springer was one ‚Äútough cookie‚Äù, thank goodness I‚Äôm motivated, it‚Äôs quite possibly the priciest book I ever bought from my own pocket, in ‚Ç¨-per-page‚Ä¶ Oh well: It looks it still was the right call for me :D)"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#why-now-why-me",
    "href": "posts/2024-12-08_A_new_project/index.html#why-now-why-me",
    "title": "A new project",
    "section": "",
    "text": "As I said, I‚Äôve been a bit busy. Full-time job, MSc paper (success!), then summer, learning other stuff, (did I mention full-time job? :D)‚Ä¶ But it is time.\nI also have been thinking for quite some time I should try my hand eventually at creating my own R Package. This was all the more true after the last R conferences I attended recently, where it looks like ‚Äúeveryone is doing it‚Äù in the R (mostly academic) community‚Ä¶ So why not me?\nPlus, I have an idea or two. So what if I‚Äôm not the greatest coder out there? I still do have some tricks in my sleeves ;) I have proved (at least to myself) I can do highly distributed processing using (among other things) R and create my own thing. (My very own algorithm, that was.)\nPlus, I‚Äôm into ML just as much as the next person, but I‚Äôve also played quite a bit with Genetic Algorithms recently. And I believe I have all it takes to put that to good use here. (I‚Äôm just a bit sad this will not involve Network Graphs, the way I have it designed (yet, anyway :D).) I believe I understand how the LCS algorithms might be particularly useful in their own specific way. I can see in my head how they must work, what makes them different, all in spite of not having written one line of code yet.\nNow, it might take a while, indeed. Why, with the Holiday season coming, the day job and all. But I have a sort of a plan in mind, already an architecture (in my head, for now), examples of how I would want to validate the implementation (that last thing, one example with application to cybersecurity and logs classification, in particular, woke me up in the middle of the night a couple of weeks back).\nAnd R packages, from what I have read thus far, are not ‚Äúmagic‚Äù. It‚Äôs going to be work, for sure, but I don‚Äôt see anything I can‚Äôt manage. If anything, it‚Äôs less complicated than I thought. I‚Äôve already played with testthat, microbenchmark, I‚Äôve always separated my code in functions, separated R and C++ code, all that‚Ä¶ So yeah, I think I‚Äôm up to the task.\nAnd finally, a note about the ‚Äúwhy now‚Äù: I can‚Äôt find it done yet!! Which to me is quite incredible. I have found this niche, that nobody (in the R community) has deemed relevant enough thus far, and I am willing to put the work towards filling that niche!"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#the-outcome",
    "href": "posts/2024-12-08_A_new_project/index.html#the-outcome",
    "title": "A new project",
    "section": "",
    "text": "This here today is me announcing, basically, my intention to create and (hopefully) get my future new RLCS package onto CRAN. I know, I know, I‚Äôm getting WAAAAY ahead of myself here.\nAnd I realize me publishing this entry might very well mean someone beats me to it. If that happens, well‚Ä¶ I‚Äôm already convinced my implementation will be ‚Äúcompetitive‚Äù, so‚Ä¶ let‚Äôem!\n(Plus, why would anyone decide to go for this now? It would just make no sense whatsoever - if not to bother me. If that‚Äôs you: Just give me a few months, and you‚Äôll save yourself the trouble :D)"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#will-i-manage",
    "href": "posts/2024-12-08_A_new_project/index.html#will-i-manage",
    "title": "A new project",
    "section": "",
    "text": "It turns out, for some reason, I know I will. Obviously, I don‚Äôt ‚Äúknow know‚Äù, it‚Äôs more of a feeling.\nI am enthused, to begin with. I have the knowledge. Time is not an issue, although it will take me probably a few months (my best estimate right now, given my current known agenda, is 3-4 months for a functional package, plus whatever time it takes for submission to CRAN of a ‚Äúv001‚Äù).\nMoreover, I somehow needed a new, rather difficult, interesting long-term personal objective. I feel this will help me get in the ‚Äúflow‚Äù, which is a great feeling (but hard to come by).\nAlso, I have this crazy idea that, once the thing works, I can dive further into it and study it towards a bigger goal, maybe research, ideally even (who knows) a PhD‚Ä¶?"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#what-about-the-sessions-on-ml",
    "href": "posts/2024-12-08_A_new_project/index.html#what-about-the-sessions-on-ml",
    "title": "A new project",
    "section": "",
    "text": "I‚Äôve done my first (of two) presentation about ‚ÄúBackground of ML for Cybersecurity‚Äù. I actually had trouble with some of the demos during the first session :S But not because of the demos themselves, just, ‚Äúheat of the moment‚Äù, computer resources, demo effect, and so on. Nothing major.\nThankfully, I get a second chance, as there is a second session coming. And I have time to prepare some more examples, so that‚Äôs good too, the second session will be all-the-more interesting.\nBut because I have some time, I‚Äôm making a pause there and re-focusing on this new project of mine as a personal interest thing."
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#and-what-about-the-migration-of-the-old-blog",
    "href": "posts/2024-12-08_A_new_project/index.html#and-what-about-the-migration-of-the-old-blog",
    "title": "A new project",
    "section": "",
    "text": "I realize nobody cares, but heck: I do. I need to ‚Äúmigrate‚Äù, move everything (while reviewing each thing) from the old blog to this new setup, because.. Well, because this platform is more convenient, and ‚Äì let‚Äôs be honest ‚Äì cheaper‚Ä¶ After all, it‚Äôs one thing to write for one own‚Äôs pleasure, and quite another to spend money with no return year after year just to maintain it.\nSo yes, I need to organize myself to minimize future costs a bit (it just is the sensible thing to do, economically)."
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#conclusions",
    "href": "posts/2024-12-08_A_new_project/index.html#conclusions",
    "title": "A new project",
    "section": "",
    "text": "**This is important: I still don‚Äôt really know why this family of algorithms have received sooo little attention**, beyond the fact that ‚ÄúConnectionism‚Äù has received a lot of said attention (deservingly, for sure).\nBut I‚Äôm curious about whether LCS can be made (ideally equally) valuable. And I have the personal conviction that I am in the right position to dig further into it.\nAnd so it is time, I‚Äôm in an ideal position, and I‚Äôm motivated.\nNow all I have to do is‚Ä¶ Start!\n(To start hear means also put thoughts to paper, design, code, test, document‚Ä¶ Don‚Äôt expect lots of news too soon, either! I‚Äôm not in a hurry, nobody should be: this thing has been known since the 70‚Äôs and it looks like few - aside from noted exceptions - have cared about it thus far‚Ä¶)"
  },
  {
    "objectID": "posts/2024-12-08_A_new_project/index.html#references",
    "href": "posts/2024-12-08_A_new_project/index.html#references",
    "title": "A new project",
    "section": "",
    "text": "This is the best intro explaining the algorithm that I found thus far - and if anything it confirmed I was on the right path. It just makes a lot of sense to me!"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "This is probably one of the last entries from my preparation to introduce ‚ÄúBackground of ML for Cybersecurity‚Äù. This time, it‚Äôs really about what should probably have been the first entry of the series. Also, it‚Äôs not really put in context of Cybersecurity: I‚Äôm just trying to show one needs not be afraid about the math.\nI‚Äôm having lots of doubts about this one, too: Can I even use the ‚ÄúMachine Learning‚Äù tag? After all, this predates ML by quite a bit. It‚Äôs really of the realm of statistics. Then again, the limit of what qualifies as ML and what doesn‚Äôt is somewhat blurry (at least to me).\nAnd some of it is very very simple, and maybe shouldn‚Äôt warrant writing about it. But I like to write things down, it helps organize my thoughts sometimes, and I believe in the idea that really understanding the basics is helpful to grasp concepts when things get more complicated.\n\n\nIF (and that‚Äôs a reasonably big if) you can find a linear relationship between two numerical vectors, then using a linear regression can help ‚Äúencode‚Äù the whole pair of vectors through ‚Äújust‚Äù two numbers: A slope, and an intercept.\nIn effect, you‚Äôre ‚Äúmodelling‚Äù a relationship - and in doing so, you‚Äôre actually compressing information.\nMore to the point, if you have an intuition that such relationship holds true for other points you were not given, then you can interpolate (if the new points are in the range of original vectors - otherwise it‚Äôs called ‚Äúextrapolation‚Äù) an expected output from a given input.\nIn other words: If you know (or expect) there is a linear (straight line) relationship between a webserver load and number of concurrently opened sessions, you could estimate a load (say % CPU) from knowing only the number of users currently connected (or the other way around, obviously). You‚Äôd expect the load to grow with the number of users.\nThe concept of regression (linear or otherwise) are important in ML. It‚Äôs a core part of most models. You doubt? Let me put it this way: Regression is a core part of how training a neural network works. There. Believe me now? (I guess I just justified myself in tagging this entry as Machine Learning, after all.)\n\n\n\nWe‚Äôll start off very easy today. Imagine to begin with you only have two points. So two pairs\\((x_1, y_1), (x_2, y_2)\\) of coordinates then. IF (again, big IF) you somehow expects a straight line could be used to estimate a third \\(y_3\\) from a third \\(x_3\\) coordinate in the range of the two original points‚Ä¶ How can you estimate said value of \\(y_3\\).\nWell, it‚Äôs basic (elementary? high school? Idk) math, you want to find \\(a, b\\) in the following:\n\\[\ny = ax + b\n\\]\nThere are two numbers to find (a slope, and an intercept), and we have two points. That‚Äôs a system of two equations. But instead of just doing the math, let‚Äôs think about this from a conceptual perspective.\nYou can probably skip this, as it‚Äôs really very basic, but I like to make sure I understand things, not just use the math I can find out there, and so I‚Äôm going to explain it nevertheless.\nWhat is a ‚Äúslope‚Äù?\nThink of a bicycle ride, and you‚Äôre faced with a 8% upwards (so ‚Äúpositive‚Äù) slope; what does that mean? That for every 100 meters you advance horizontally, you should expect to gain 8 meters in elevation. So if you gain 8 units in the y direction, you have moved 100 units in the x direction.\nWith two points: \\(y_2 - y_1 = 8, x_2 - x_1 = 100\\). And you get to the slope by dividing both numbers (which is why we expressed the slope in percentage). So you need then:\n\\[\na = slope = {(y_2 - y_1) \\over (x_2 - x_1)}\n\\]\nNot soooo misterious now, is it? If the slope is downwards, \\(y_2 &lt; y_1\\) and \\(a &lt; 0\\), that‚Äôs all.\nAnd what is the ‚Äúintercept‚Äù?\nIf you show lines on a two dimensional graph, you often will see the \\(y\\) and \\(x\\) axis. The ‚Äúintercept‚Äù is simply the point at which our line crosses (‚Äúintercepts‚Äù) the \\(y\\) axis. Which happens when \\(x = 0\\).\nAnd by now, if we take any of our points, we have the following bits \\(y_1, a, x_1\\) for our equation \\(y = ax + b\\). Can we get to \\(b\\)?\n\\[\nb = y_1 - a x_1\n\\]So that for \\(x = 0\\), we have \\(b\\) as the value of \\(y\\). Which is what we were looking for.\nThis was very very very basic stuff. But hopefully for those that don‚Äôt do math in their day-to-day life, or those that use formulas without thinking (or use Excel to estimate these things, if at all), or simply as a refresher‚Ä¶ I don‚Äôt know. Sorry if that was boring.\n\n## Suppose you have any two points:\nmy_x &lt;- c(1.5, 2.5)\nmy_y &lt;- c(5, 2)\n\n# Line goes through two points\n## By definition of \"slope\":\nmy_line_slope &lt;- (my_y[2] - my_y[1]) / (my_x[2]- my_x[1])\n## Then just replace with one point to get intercept:\nmy_intercept &lt;- my_y[1] - (my_line_slope * my_x[1])\n\ncalculate_resulting_y &lt;- function(x) {\n    my_line_slope * x + my_intercept\n}\n\n## Say then we have any value of x (\"independent variable\"):\nmy_interpolation_x_set &lt;- seq(0, 5, 0.1)\n## In a perfect World, we could then get any dependent value:\nmy_interpolation_y_set &lt;- sapply(my_interpolation_x_set, calculate_resulting_y)\n\nplot(my_interpolation_y_set ~ my_interpolation_x_set, col = \"blue\", type=\"l\")\npoints(my_y ~ my_x, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nIn the above, we took a pretty big leap of faith: We just somehow knew/assumed that two points would represent a linear relationship.\nBut we‚Äôd probably feel more confident if we had more points to confirm our expectations first.\nToday we‚Äôll stay with the ‚ÄúSimple Linear Regression‚Äù, which is nicely explained in the Wikipedia.\nWe want to find \\(\\alpha, \\beta\\) that best describe not two points but instead estimate a line for several of them, so that we solve \\(y = \\alpha + \\beta x\\) that best estimate all \\(x_i, y_i\\) pairs we‚Äôre given (provided they look like a straight line).\nAnd I‚Äôve said ‚Äúestimate‚Äù twice above, as there will be some error, and that‚Äôs key here: We essentially try to minimize the error that our estimated \\(\\alpha, \\beta\\) introduce, for each point. Written ‚Äúfancily‚Äù:\n\\[\n\\widehat{\\epsilon}_i = y_i - (\\alpha + \\beta x_i), \\forall i \\in [1..n], n \\in \\mathbb{N}\n\\]\nWhere n is the number of your available points‚Ä¶\n\nMinimizing the error is an important concept in supervised machine learning.\n\nThen using the ‚ÄúSum of Square‚Äù of the residuals, we‚Äôre going to try to minimize that.\n\n\n\nI will take the time to dive into this a bit, as it is relevant to understand for other cases. Although the math is better explained here, that approach of minimizing sum of squares is useful in other contexts and so here goes.\nSo today we are trying to ‚Äúfit‚Äù a line to a set of points that are not necessarily exactly on said line (see ‚Äúthe result‚Äù below). But where do we start? How do we go about finding one line out of an infinite set of possible lines?\nMore or less, we find the parameters for our line equation that minimizes overall distance from all the points. So we try to minimize the sum of the error of our line from each point. It is a minimization exercise, but sometimes the points will be under the line, and others they‚Äôll be above. To correct for that, we actually minimize the sum of distances, each one squared! That way, all values are positive, and we really try to minimize something.\nThere is another important point hidden right here: The key to understand ‚ÄúGradient Descent‚Äù!\nAs we will see with the ‚Äúresults‚Äù below, the errors, when squared, are shaped in a sort of U. But what‚Äôs important is that the function behind that (parabolic) shape is that it‚Äôs differentiable.\nDifferentiating (that‚Äôs calculus) here will mean that we know in which direction (up, down, horizontal) said curve goes for a given point on the x axis. If we know that, we know that if the curve goes upwards, to minimize the thing, we want to go in the opposite direction.\nAnd that‚Äôs exactly what ‚Äúdescending the gradient‚Äù is all about (although in multivariate settings: A gradient in vector calculus is ‚Äúthe direction and the rate of fastest increase‚Äù, so to ‚Äúdescend a gradient‚Äù, you want to move in the opposite direction).\nFor linear regression, it‚Äôs cool because we can do partial derivatives (as shown on the referenced Wiki page above) to find our \\(\\alpha\\) and \\(\\beta\\) parameters that describe the line that ‚Äúbest fit the data‚Äù.\nIt‚Äôs quite clever, mind you: You minimize the error of your predicted value compared to the actual value. That‚Äôs how your machine can learn! But it‚Äôs also what people have been using to train Neural Networks!\nFor today, let‚Äôs just store the following information: The ‚ÄúLeast Square Minimization‚Äù can help find a minimum error through differentiation.\nAnd for a future conversation: Gradient Descent (hidden in today‚Äôs Sum of Square Minimization) is one of the two mathematical keys for back-propagation. (And back-propagation was the before and after of connectionism in Machine Learning theory).\nAlright, let‚Äôs move on and return to our very simple example.\n\n\n\nRemember how we said at some point that in ML, more data points is often better?\nWe‚Äôre going to find the line that best fits a set of points (I know a simple straight line will work here, because I‚Äôm the one generating said line‚Ä¶ :D)\nWe‚Äôre also going to show the square of errors, how it looks like a U shape, and how it‚Äôs clear there is a minimum to it‚Ä¶\n\n## Some points that can be used:\nmy_regression_x_set &lt;- seq(0, 5, 0.1) + rnorm(51,mean = 0, sd = 0.2)\nmy_regression_y_set &lt;- my_interpolation_y_set + rnorm(51, mean = 0, sd = 0.2)\n\n## SIMPLE linear regression. Using Least Squares distance.\n## Key idea: Minimize differences.\n## Find minimum for error function using differentiation.\n## Final equations here instead of step by step explaining how to get there.\n\n## Just follow the math:\nmean_reg_x &lt;- mean(my_regression_x_set)\nmean_reg_y &lt;- mean(my_regression_y_set)\n\ndev_x &lt;- (my_regression_x_set - mean_reg_x)\ndev_y &lt;- (my_regression_y_set - mean_reg_y)\n\nbeta_est &lt;- sum(dev_x * dev_y) / sum(dev_x^2)\nalpha_est &lt;- mean_reg_y - (beta_est * mean_reg_x)\n\nplot(my_regression_y_set ~ my_regression_x_set)\n## alpha & beta can be confusing, I usually use Beta as multipliers of X...\nabline(a = alpha_est, b = beta_est, col=\"blue\")\n\n\n\n\n\n\n\n## Importantly, squared error is differentiable and has a global minimum\n## \"easily\". Related to \"Gradient Descent\"\nerr_x &lt;- my_regression_y_set - (alpha_est + my_regression_x_set * beta_est)\nerr_y &lt;- sapply(err_x, \\(x) x^2)\n\nplot(err_y ~ err_x)\n\n\n\n\n\n\n\n\nAnd that‚Äôs it! We‚Äôve used the math (skipping some steps), to show one can use it to do a linear regression (although nowadays all that stuff is available with one command, if not directly, then through packages).\nSo here we‚Äôve shown a downward slope: Let‚Äôs put it in context of a potential real-world application: Maybe our x axis represented the number of users connected to our system, and the y axis some number to represent the available resources (spare CPU or something) on a given server. If you know a linear relationship applies, you can estimate at which point you will reach 100% CPU‚Ä¶ (I could have started with that, sorry.)\nSo in the future, if you see a clear straight line between two things, well, you could use the above not-scary-at-all math approach to ‚Äúpredict‚Äù what would happen if you move along the x axis.\nLast note about the math: in this simple case, if you look at the code, we‚Äôve in fact used two statistics concepts to do the calculations: the means of the x and y axis, and the ‚Äúdeviations‚Äù of each point to said means.\nDoes it not make some intuitive sense that the line that best fits the points is related to the means and deviations of the points to said means? :)\n\n\n\nIn the above, we‚Äôve used deviations and means. These are basic concepts from statistics. And in the introduction, we have mentioned something about ‚Äúmodels‚Äù and ‚Äúcompression‚Äù. Which is what it is: Statistics are NOT the real world. And they can be misused. Here the 2 most common examples to illustrate how you can go astray with using statistics without looking at the data.\nLet‚Äôs illustrate just that, with a couple of typical (like, VERY typical) examples:\n\n\n\n\n\n\n\n\n\nAnd if it‚Äôs not clear enough, all these here below have the same mean and standard deviation:\n\n\n\n\n\n\n\n\n\n[1] \"Mean x Dino: 47.8322528169014\"\n\n\n[1] \"Mean x Bullseye: 47.8308231552178\"\n\n\n[1] \"SD x Dino: 16.7651420391168\"\n\n\n[1] \"SD x Bullseye: 16.7692394934267\"\n\n\nSee how they are basically the same numbers?\nStatistics are great, useful and very important today (and have been for quite a while, I‚Äôd argue). They‚Äôre just not always the right solution (neither is ML in a larger sense, mind you).\nAlways look at the data!\n\n\n\nWe‚Äôve gone from really REALLY simple stuff all the way to mentioning differentiation, gradient descent, statistics‚Ä¶\nAnd we‚Äôve skipped steps! (i.e.¬†solving systems of equations, which can be represented as matrices, which is relevant for completely different reasons :D)\nNot too shabby, after all‚Ä¶\n\nI‚Äôd like to go into Logistic Regression and Neural Networks a bit in a future entries. Even discuss a bit the history of ML, maybe. And that would conclude my preparation for my sessions about ‚ÄúML Background for Cybersecurity‚Äù. Surely there is more to it all, but if I manage to convey a few key concepts to my colleagues, I‚Äôll be very very happy. And this here is me trying.\nBut after that, I‚Äôll shift gears and focus on a personal project I have had in mind for some time and that is taking shape: It involves writing an R package, for a set of ML algorithms that for reasons I don‚Äôt quite understand have not received as much attention as I personally think they should. To be continued!\n\n\n\n\nThe Wikipedia for Simple Linear Regression does help.\nHere about the Gradient.\nThe math for the simple linear regression we‚Äôve used today.\nThen the more advanced discussion on Linear Regression hopefully can be followed."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#why-linear-regression-in-the-first-place",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#why-linear-regression-in-the-first-place",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "IF (and that‚Äôs a reasonably big if) you can find a linear relationship between two numerical vectors, then using a linear regression can help ‚Äúencode‚Äù the whole pair of vectors through ‚Äújust‚Äù two numbers: A slope, and an intercept.\nIn effect, you‚Äôre ‚Äúmodelling‚Äù a relationship - and in doing so, you‚Äôre actually compressing information.\nMore to the point, if you have an intuition that such relationship holds true for other points you were not given, then you can interpolate (if the new points are in the range of original vectors - otherwise it‚Äôs called ‚Äúextrapolation‚Äù) an expected output from a given input.\nIn other words: If you know (or expect) there is a linear (straight line) relationship between a webserver load and number of concurrently opened sessions, you could estimate a load (say % CPU) from knowing only the number of users currently connected (or the other way around, obviously). You‚Äôd expect the load to grow with the number of users.\nThe concept of regression (linear or otherwise) are important in ML. It‚Äôs a core part of most models. You doubt? Let me put it this way: Regression is a core part of how training a neural network works. There. Believe me now? (I guess I just justified myself in tagging this entry as Machine Learning, after all.)"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#two-points-interextrapolation",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#two-points-interextrapolation",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "We‚Äôll start off very easy today. Imagine to begin with you only have two points. So two pairs\\((x_1, y_1), (x_2, y_2)\\) of coordinates then. IF (again, big IF) you somehow expects a straight line could be used to estimate a third \\(y_3\\) from a third \\(x_3\\) coordinate in the range of the two original points‚Ä¶ How can you estimate said value of \\(y_3\\).\nWell, it‚Äôs basic (elementary? high school? Idk) math, you want to find \\(a, b\\) in the following:\n\\[\ny = ax + b\n\\]\nThere are two numbers to find (a slope, and an intercept), and we have two points. That‚Äôs a system of two equations. But instead of just doing the math, let‚Äôs think about this from a conceptual perspective.\nYou can probably skip this, as it‚Äôs really very basic, but I like to make sure I understand things, not just use the math I can find out there, and so I‚Äôm going to explain it nevertheless.\nWhat is a ‚Äúslope‚Äù?\nThink of a bicycle ride, and you‚Äôre faced with a 8% upwards (so ‚Äúpositive‚Äù) slope; what does that mean? That for every 100 meters you advance horizontally, you should expect to gain 8 meters in elevation. So if you gain 8 units in the y direction, you have moved 100 units in the x direction.\nWith two points: \\(y_2 - y_1 = 8, x_2 - x_1 = 100\\). And you get to the slope by dividing both numbers (which is why we expressed the slope in percentage). So you need then:\n\\[\na = slope = {(y_2 - y_1) \\over (x_2 - x_1)}\n\\]\nNot soooo misterious now, is it? If the slope is downwards, \\(y_2 &lt; y_1\\) and \\(a &lt; 0\\), that‚Äôs all.\nAnd what is the ‚Äúintercept‚Äù?\nIf you show lines on a two dimensional graph, you often will see the \\(y\\) and \\(x\\) axis. The ‚Äúintercept‚Äù is simply the point at which our line crosses (‚Äúintercepts‚Äù) the \\(y\\) axis. Which happens when \\(x = 0\\).\nAnd by now, if we take any of our points, we have the following bits \\(y_1, a, x_1\\) for our equation \\(y = ax + b\\). Can we get to \\(b\\)?\n\\[\nb = y_1 - a x_1\n\\]So that for \\(x = 0\\), we have \\(b\\) as the value of \\(y\\). Which is what we were looking for.\nThis was very very very basic stuff. But hopefully for those that don‚Äôt do math in their day-to-day life, or those that use formulas without thinking (or use Excel to estimate these things, if at all), or simply as a refresher‚Ä¶ I don‚Äôt know. Sorry if that was boring.\n\n## Suppose you have any two points:\nmy_x &lt;- c(1.5, 2.5)\nmy_y &lt;- c(5, 2)\n\n# Line goes through two points\n## By definition of \"slope\":\nmy_line_slope &lt;- (my_y[2] - my_y[1]) / (my_x[2]- my_x[1])\n## Then just replace with one point to get intercept:\nmy_intercept &lt;- my_y[1] - (my_line_slope * my_x[1])\n\ncalculate_resulting_y &lt;- function(x) {\n    my_line_slope * x + my_intercept\n}\n\n## Say then we have any value of x (\"independent variable\"):\nmy_interpolation_x_set &lt;- seq(0, 5, 0.1)\n## In a perfect World, we could then get any dependent value:\nmy_interpolation_y_set &lt;- sapply(my_interpolation_x_set, calculate_resulting_y)\n\nplot(my_interpolation_y_set ~ my_interpolation_x_set, col = \"blue\", type=\"l\")\npoints(my_y ~ my_x, col = \"red\")"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#multiple-points-with-a-linear-relationship",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#multiple-points-with-a-linear-relationship",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "In the above, we took a pretty big leap of faith: We just somehow knew/assumed that two points would represent a linear relationship.\nBut we‚Äôd probably feel more confident if we had more points to confirm our expectations first.\nToday we‚Äôll stay with the ‚ÄúSimple Linear Regression‚Äù, which is nicely explained in the Wikipedia.\nWe want to find \\(\\alpha, \\beta\\) that best describe not two points but instead estimate a line for several of them, so that we solve \\(y = \\alpha + \\beta x\\) that best estimate all \\(x_i, y_i\\) pairs we‚Äôre given (provided they look like a straight line).\nAnd I‚Äôve said ‚Äúestimate‚Äù twice above, as there will be some error, and that‚Äôs key here: We essentially try to minimize the error that our estimated \\(\\alpha, \\beta\\) introduce, for each point. Written ‚Äúfancily‚Äù:\n\\[\n\\widehat{\\epsilon}_i = y_i - (\\alpha + \\beta x_i), \\forall i \\in [1..n], n \\in \\mathbb{N}\n\\]\nWhere n is the number of your available points‚Ä¶\n\nMinimizing the error is an important concept in supervised machine learning.\n\nThen using the ‚ÄúSum of Square‚Äù of the residuals, we‚Äôre going to try to minimize that."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#sum-of-square-minimization",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#sum-of-square-minimization",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "I will take the time to dive into this a bit, as it is relevant to understand for other cases. Although the math is better explained here, that approach of minimizing sum of squares is useful in other contexts and so here goes.\nSo today we are trying to ‚Äúfit‚Äù a line to a set of points that are not necessarily exactly on said line (see ‚Äúthe result‚Äù below). But where do we start? How do we go about finding one line out of an infinite set of possible lines?\nMore or less, we find the parameters for our line equation that minimizes overall distance from all the points. So we try to minimize the sum of the error of our line from each point. It is a minimization exercise, but sometimes the points will be under the line, and others they‚Äôll be above. To correct for that, we actually minimize the sum of distances, each one squared! That way, all values are positive, and we really try to minimize something.\nThere is another important point hidden right here: The key to understand ‚ÄúGradient Descent‚Äù!\nAs we will see with the ‚Äúresults‚Äù below, the errors, when squared, are shaped in a sort of U. But what‚Äôs important is that the function behind that (parabolic) shape is that it‚Äôs differentiable.\nDifferentiating (that‚Äôs calculus) here will mean that we know in which direction (up, down, horizontal) said curve goes for a given point on the x axis. If we know that, we know that if the curve goes upwards, to minimize the thing, we want to go in the opposite direction.\nAnd that‚Äôs exactly what ‚Äúdescending the gradient‚Äù is all about (although in multivariate settings: A gradient in vector calculus is ‚Äúthe direction and the rate of fastest increase‚Äù, so to ‚Äúdescend a gradient‚Äù, you want to move in the opposite direction).\nFor linear regression, it‚Äôs cool because we can do partial derivatives (as shown on the referenced Wiki page above) to find our \\(\\alpha\\) and \\(\\beta\\) parameters that describe the line that ‚Äúbest fit the data‚Äù.\nIt‚Äôs quite clever, mind you: You minimize the error of your predicted value compared to the actual value. That‚Äôs how your machine can learn! But it‚Äôs also what people have been using to train Neural Networks!\nFor today, let‚Äôs just store the following information: The ‚ÄúLeast Square Minimization‚Äù can help find a minimum error through differentiation.\nAnd for a future conversation: Gradient Descent (hidden in today‚Äôs Sum of Square Minimization) is one of the two mathematical keys for back-propagation. (And back-propagation was the before and after of connectionism in Machine Learning theory).\nAlright, let‚Äôs move on and return to our very simple example."
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#the-result",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#the-result",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "Remember how we said at some point that in ML, more data points is often better?\nWe‚Äôre going to find the line that best fits a set of points (I know a simple straight line will work here, because I‚Äôm the one generating said line‚Ä¶ :D)\nWe‚Äôre also going to show the square of errors, how it looks like a U shape, and how it‚Äôs clear there is a minimum to it‚Ä¶\n\n## Some points that can be used:\nmy_regression_x_set &lt;- seq(0, 5, 0.1) + rnorm(51,mean = 0, sd = 0.2)\nmy_regression_y_set &lt;- my_interpolation_y_set + rnorm(51, mean = 0, sd = 0.2)\n\n## SIMPLE linear regression. Using Least Squares distance.\n## Key idea: Minimize differences.\n## Find minimum for error function using differentiation.\n## Final equations here instead of step by step explaining how to get there.\n\n## Just follow the math:\nmean_reg_x &lt;- mean(my_regression_x_set)\nmean_reg_y &lt;- mean(my_regression_y_set)\n\ndev_x &lt;- (my_regression_x_set - mean_reg_x)\ndev_y &lt;- (my_regression_y_set - mean_reg_y)\n\nbeta_est &lt;- sum(dev_x * dev_y) / sum(dev_x^2)\nalpha_est &lt;- mean_reg_y - (beta_est * mean_reg_x)\n\nplot(my_regression_y_set ~ my_regression_x_set)\n## alpha & beta can be confusing, I usually use Beta as multipliers of X...\nabline(a = alpha_est, b = beta_est, col=\"blue\")\n\n\n\n\n\n\n\n## Importantly, squared error is differentiable and has a global minimum\n## \"easily\". Related to \"Gradient Descent\"\nerr_x &lt;- my_regression_y_set - (alpha_est + my_regression_x_set * beta_est)\nerr_y &lt;- sapply(err_x, \\(x) x^2)\n\nplot(err_y ~ err_x)\n\n\n\n\n\n\n\n\nAnd that‚Äôs it! We‚Äôve used the math (skipping some steps), to show one can use it to do a linear regression (although nowadays all that stuff is available with one command, if not directly, then through packages).\nSo here we‚Äôve shown a downward slope: Let‚Äôs put it in context of a potential real-world application: Maybe our x axis represented the number of users connected to our system, and the y axis some number to represent the available resources (spare CPU or something) on a given server. If you know a linear relationship applies, you can estimate at which point you will reach 100% CPU‚Ä¶ (I could have started with that, sorry.)\nSo in the future, if you see a clear straight line between two things, well, you could use the above not-scary-at-all math approach to ‚Äúpredict‚Äù what would happen if you move along the x axis.\nLast note about the math: in this simple case, if you look at the code, we‚Äôve in fact used two statistics concepts to do the calculations: the means of the x and y axis, and the ‚Äúdeviations‚Äù of each point to said means.\nDoes it not make some intuitive sense that the line that best fits the points is related to the means and deviations of the points to said means? :)"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#look-at-the-data",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#look-at-the-data",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "In the above, we‚Äôve used deviations and means. These are basic concepts from statistics. And in the introduction, we have mentioned something about ‚Äúmodels‚Äù and ‚Äúcompression‚Äù. Which is what it is: Statistics are NOT the real world. And they can be misused. Here the 2 most common examples to illustrate how you can go astray with using statistics without looking at the data.\nLet‚Äôs illustrate just that, with a couple of typical (like, VERY typical) examples:\n\n\n\n\n\n\n\n\n\nAnd if it‚Äôs not clear enough, all these here below have the same mean and standard deviation:\n\n\n\n\n\n\n\n\n\n[1] \"Mean x Dino: 47.8322528169014\"\n\n\n[1] \"Mean x Bullseye: 47.8308231552178\"\n\n\n[1] \"SD x Dino: 16.7651420391168\"\n\n\n[1] \"SD x Bullseye: 16.7692394934267\"\n\n\nSee how they are basically the same numbers?\nStatistics are great, useful and very important today (and have been for quite a while, I‚Äôd argue). They‚Äôre just not always the right solution (neither is ML in a larger sense, mind you).\nAlways look at the data!"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#conclusions",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#conclusions",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "We‚Äôve gone from really REALLY simple stuff all the way to mentioning differentiation, gradient descent, statistics‚Ä¶\nAnd we‚Äôve skipped steps! (i.e.¬†solving systems of equations, which can be represented as matrices, which is relevant for completely different reasons :D)\nNot too shabby, after all‚Ä¶\n\nI‚Äôd like to go into Logistic Regression and Neural Networks a bit in a future entries. Even discuss a bit the history of ML, maybe. And that would conclude my preparation for my sessions about ‚ÄúML Background for Cybersecurity‚Äù. Surely there is more to it all, but if I manage to convey a few key concepts to my colleagues, I‚Äôll be very very happy. And this here is me trying.\nBut after that, I‚Äôll shift gears and focus on a personal project I have had in mind for some time and that is taking shape: It involves writing an R package, for a set of ML algorithms that for reasons I don‚Äôt quite understand have not received as much attention as I personally think they should. To be continued!"
  },
  {
    "objectID": "posts/2024-11-25_Understanding_Linear_Regression/index.html#references",
    "href": "posts/2024-11-25_Understanding_Linear_Regression/index.html#references",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "The Wikipedia for Simple Linear Regression does help.\nHere about the Gradient.\nThe math for the simple linear regression we‚Äôve used today.\nThen the more advanced discussion on Linear Regression hopefully can be followed."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "So I wasn‚Äôt kidding. I really like this ‚ÄúLearning Classifier System‚Äù (aka LCS) idea, and I really want to develop the R package for it, which I will call ‚ÄúRLCS‚Äù (clever, hu?).\nNow I have to start somewhere, and so I will keep it simple for now. I will first focus on an LCS that does supervised learning only (that means, no reinforcement learning for now.)\nAlso, I will focus on the simplest binary environments. In LCS lingo, for supervised classification, environment is equivalent to saying dataset. Each entry to classify is presented as binary strings, and the class (aka action) is, for simplicity, also a simple 0 or 1.\nSo binary classification of binary strings.\nThis might sound simplistic (and it is somewhat, yes) but the feature engineering and codificatioin into GA-compatible individuals is often harder than the actual training of a ML algorithm.\nAnyhow. I will forego some details for now, but suffice to say I‚Äôm working right now towards a Michigan-style LCS.\n\n\n\nNow we will receive an environment/dataset as input that has ‚Äústates‚Äù in binary format, and actions/classes as either 0 or 1.\n## Suppose test data is:\ntest_environment &lt;- data.frame(state = c('00011', '10010', '01110', '00001'),\n                               class = c(0, 0, 0, 1))\n(It‚Äôs not enough info to conclude that, but these examples all agree with a classifier that would say, in human readable format: ‚ÄúThe Class is the negated bit 4 of the input‚Äù.)\nOne of the key aspects of an LCS, is that it will produce rules. A rule is a pair of a condition and an action.\nA condition in turn is written (in our current approach) as a string of a ternary alphabet, namely characters from the set {0, 1, #}, where # means wildcard and matches either 0 or 1.\nNow for each input of the environment (each entry of the dataset), for today‚Äôs purposes, we will want to know which rules it matches (regardless of the action). In other words, we want to match an input to condition(s). That will tell us the ‚ÄúMatch set‚Äù.\nMatching is one key aspect of the LCS overall algorithm, one that will be run a lot, and therefore I worry that it could be slow.\nA couple more keywords: A classifier is a rule and its statistics. (We will skip the statistics part for today.) And a population is our set of classifiers.\nShort of going the way of C++, I have coded one first simplistic function that takes a classifier set as a dataframe, and another one that uses a list of lists instead.\nConditions can easily be written as strings:\nconditions &lt;- c('#001#', '10#10', '##110', '###0#')\nactions &lt;- c(0, 0, 0, 1)\nFor the first approach, it was quite straightforward:\nlcs_classifier1 &lt;- data.frame(id = 1:4,\n                      condition_string = conditions,\n                      action = actions, #simplest is binary for supervised\n                      accuracy = 1,\n                      numerosity = 1,\n                      first_seen = c(1, 2, 3, 4) ## I'm inventing a coverage trigger\n                      )\n\n## Find Matching Rules\nget_match_set_v001 &lt;- function(ti_condition, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- unlist(strsplit(ti_condition, \"\"))\n    pop &lt;- pop[, c(\"id\", \"condition_string\")] # Same\n\n    which(sapply(1:nrow(pop), function(i) {\n\n        t_cond &lt;- unlist(strsplit(pop$condition_string[i], \"\"))\n        relevant_positions &lt;- which(t_cond %in% c(\"0\", \"1\"))\n        \n        all(t_cond[relevant_positions] == ti_cond[relevant_positions])\n    }))\n}\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v001(test_environment$state[1], lcs_classifier1), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v001(test_environment$state[2], lcs_classifier1),\n        c(1, 2)\n    )\n})\n\n(OUTPUT:) Test passed ü•≥\nFor the second approach, I needed some more supporting functions:\n######\n## Option 2: Use list instead of Data.frame and pre-calculate match sets\n\n## Function to create a new rule from a condition string and an action\n## THIS IS VALID FOR CLASSIFICATION, NOT FOR Reinforcement Learning!!!\n## In our case, a new rule has: A condition string, an (correct!) action\n## Since action is correct, this is useful for future \"covering\"\n## AND we set accuracy to 1\nnew_supervised_rule &lt;- function(condition_string, action) {\n    list(id = 1,\n         condition_string = condition_string,\n         condition_length = nchar(condition_string),\n         condition_list = list(\"0\" = which(unlist(strsplit(condition_string, \"\")) == \"0\"),\n                               \"1\" = which(unlist(strsplit(condition_string, \"\")) == \"1\")),\n         action = action,\n         accuracy = 1,\n         numerosity = 1,\n         first_seen = 1)\n}\n\n## Function to return max id from a classifier set in list format\nmax_id_in_classifier &lt;- function(pop) {\n    max(sapply(pop, function(x) x$id))\n}\n\n## Function to add rule to a population.\n## date_rule_born will be useful stat for future, setting as parameter for now.\nadd_valid_rule_to_pop &lt;- function(pop, condition_string, action, date_rule_born = 1) {\n    \n    t_rule &lt;- new_supervised_rule(condition_string, action)\n    t_rule$id &lt;- max_id_in_classifier(pop)+1\n    t_rule$first_seen &lt;- date_rule_born\n    \n    pop[[length(pop)+1]] &lt;- t_rule\n    \n    pop\n}\n\n## Support function for human-compatible printing:\nmake_pop_printable &lt;- function(classifier) {\n    data.frame\n    rbind.fill(lapply(1:length(classifier), function(i) {\n        t_c &lt;- classifier[[i]]\n        data.frame(id = t_c$id,\n                   condition = t_c$condition_string,\n                   action = t_c$action,\n                   accuracy = t_c$accuracy,\n                   numerosity = t_c$numerosity,\n                   first_seen = t_c$first_seen)\n    }))\n}\n\n## Matching with list formatted population.\nget_match_set_v002 &lt;- function(input_state, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- as.integer(unlist(strsplit(input_state, \"\")))\n    \n    which(sapply(1:length(pop), function(i) {\n        rule &lt;- pop[[i]]$condition_list\n        all(ti_cond[rule$'0'] == 0, ti_cond[rule$'1'] == 1)\n    }))\n}\n\nlcs_classifier2 &lt;- list(new_supervised_rule(conditions[1], actions[1]))\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[2], actions[2])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[3], actions[3])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[4], actions[4])\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v002(test_environment$state[1], lcs_classifier2), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v002(test_environment$state[2], lcs_classifier2),\n        c(1, 2)\n    )\n})\n\n\n\nAn example population\n\n\n\n\n\nmicrobenchmark(\n    sapply(test_environment$state, function(x) { get_match_set_v001(x, lcs_classifier1)}),\n    sapply(test_environment$state, function(x) { get_match_set_v002(x, lcs_classifier2)}),\n    times = 10000L\n)\n\n\n\nlists and pre-calculated match positions is faster\n\n\nLists are more flexible and data.frames are not the fastest option out there anyway. Here the pre-calculations in the second format also helps doing faster matching.\nHowever I know this choice might have an impact, particularly when time will come to implement deletion, subsumption and compaction (all useful for better LCS performance/value). And another idea I have in mind will require further lists management (but it‚Äôs too early to discuss that).\nBut that‚Äôs a problem for future me (and nothing that hasn‚Äôt been solved already).\n\n\n\nWell well: Day 1, and we have covered concepts of environment, states, actions, conditions, rules, classifiers, population and matching/match set.\nAlong with developping a functional (basic) list for storing classifiers, and a matching function.\nNot too bad (albeit all too simplistic and not going for ‚Äúfull-power‚Äù flexible LCS implementation).\n\n\n\nI‚Äôll keep referring to this video (although the book is better to get acquainted with the vocabulary and concepts): intro explaining the algorithm ."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#intro",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#intro",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "So I wasn‚Äôt kidding. I really like this ‚ÄúLearning Classifier System‚Äù (aka LCS) idea, and I really want to develop the R package for it, which I will call ‚ÄúRLCS‚Äù (clever, hu?).\nNow I have to start somewhere, and so I will keep it simple for now. I will first focus on an LCS that does supervised learning only (that means, no reinforcement learning for now.)\nAlso, I will focus on the simplest binary environments. In LCS lingo, for supervised classification, environment is equivalent to saying dataset. Each entry to classify is presented as binary strings, and the class (aka action) is, for simplicity, also a simple 0 or 1.\nSo binary classification of binary strings.\nThis might sound simplistic (and it is somewhat, yes) but the feature engineering and codificatioin into GA-compatible individuals is often harder than the actual training of a ML algorithm.\nAnyhow. I will forego some details for now, but suffice to say I‚Äôm working right now towards a Michigan-style LCS."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#the-ternary-alphabet-for-conditions",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#the-ternary-alphabet-for-conditions",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "Now we will receive an environment/dataset as input that has ‚Äústates‚Äù in binary format, and actions/classes as either 0 or 1.\n## Suppose test data is:\ntest_environment &lt;- data.frame(state = c('00011', '10010', '01110', '00001'),\n                               class = c(0, 0, 0, 1))\n(It‚Äôs not enough info to conclude that, but these examples all agree with a classifier that would say, in human readable format: ‚ÄúThe Class is the negated bit 4 of the input‚Äù.)\nOne of the key aspects of an LCS, is that it will produce rules. A rule is a pair of a condition and an action.\nA condition in turn is written (in our current approach) as a string of a ternary alphabet, namely characters from the set {0, 1, #}, where # means wildcard and matches either 0 or 1.\nNow for each input of the environment (each entry of the dataset), for today‚Äôs purposes, we will want to know which rules it matches (regardless of the action). In other words, we want to match an input to condition(s). That will tell us the ‚ÄúMatch set‚Äù.\nMatching is one key aspect of the LCS overall algorithm, one that will be run a lot, and therefore I worry that it could be slow.\nA couple more keywords: A classifier is a rule and its statistics. (We will skip the statistics part for today.) And a population is our set of classifiers.\nShort of going the way of C++, I have coded one first simplistic function that takes a classifier set as a dataframe, and another one that uses a list of lists instead.\nConditions can easily be written as strings:\nconditions &lt;- c('#001#', '10#10', '##110', '###0#')\nactions &lt;- c(0, 0, 0, 1)\nFor the first approach, it was quite straightforward:\nlcs_classifier1 &lt;- data.frame(id = 1:4,\n                      condition_string = conditions,\n                      action = actions, #simplest is binary for supervised\n                      accuracy = 1,\n                      numerosity = 1,\n                      first_seen = c(1, 2, 3, 4) ## I'm inventing a coverage trigger\n                      )\n\n## Find Matching Rules\nget_match_set_v001 &lt;- function(ti_condition, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- unlist(strsplit(ti_condition, \"\"))\n    pop &lt;- pop[, c(\"id\", \"condition_string\")] # Same\n\n    which(sapply(1:nrow(pop), function(i) {\n\n        t_cond &lt;- unlist(strsplit(pop$condition_string[i], \"\"))\n        relevant_positions &lt;- which(t_cond %in% c(\"0\", \"1\"))\n        \n        all(t_cond[relevant_positions] == ti_cond[relevant_positions])\n    }))\n}\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v001(test_environment$state[1], lcs_classifier1), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v001(test_environment$state[2], lcs_classifier1),\n        c(1, 2)\n    )\n})\n\n(OUTPUT:) Test passed ü•≥\nFor the second approach, I needed some more supporting functions:\n######\n## Option 2: Use list instead of Data.frame and pre-calculate match sets\n\n## Function to create a new rule from a condition string and an action\n## THIS IS VALID FOR CLASSIFICATION, NOT FOR Reinforcement Learning!!!\n## In our case, a new rule has: A condition string, an (correct!) action\n## Since action is correct, this is useful for future \"covering\"\n## AND we set accuracy to 1\nnew_supervised_rule &lt;- function(condition_string, action) {\n    list(id = 1,\n         condition_string = condition_string,\n         condition_length = nchar(condition_string),\n         condition_list = list(\"0\" = which(unlist(strsplit(condition_string, \"\")) == \"0\"),\n                               \"1\" = which(unlist(strsplit(condition_string, \"\")) == \"1\")),\n         action = action,\n         accuracy = 1,\n         numerosity = 1,\n         first_seen = 1)\n}\n\n## Function to return max id from a classifier set in list format\nmax_id_in_classifier &lt;- function(pop) {\n    max(sapply(pop, function(x) x$id))\n}\n\n## Function to add rule to a population.\n## date_rule_born will be useful stat for future, setting as parameter for now.\nadd_valid_rule_to_pop &lt;- function(pop, condition_string, action, date_rule_born = 1) {\n    \n    t_rule &lt;- new_supervised_rule(condition_string, action)\n    t_rule$id &lt;- max_id_in_classifier(pop)+1\n    t_rule$first_seen &lt;- date_rule_born\n    \n    pop[[length(pop)+1]] &lt;- t_rule\n    \n    pop\n}\n\n## Support function for human-compatible printing:\nmake_pop_printable &lt;- function(classifier) {\n    data.frame\n    rbind.fill(lapply(1:length(classifier), function(i) {\n        t_c &lt;- classifier[[i]]\n        data.frame(id = t_c$id,\n                   condition = t_c$condition_string,\n                   action = t_c$action,\n                   accuracy = t_c$accuracy,\n                   numerosity = t_c$numerosity,\n                   first_seen = t_c$first_seen)\n    }))\n}\n\n## Matching with list formatted population.\nget_match_set_v002 &lt;- function(input_state, pop) {\n    # Only part relevant for matching\n    ti_cond &lt;- as.integer(unlist(strsplit(input_state, \"\")))\n    \n    which(sapply(1:length(pop), function(i) {\n        rule &lt;- pop[[i]]$condition_list\n        all(ti_cond[rule$'0'] == 0, ti_cond[rule$'1'] == 1)\n    }))\n}\n\nlcs_classifier2 &lt;- list(new_supervised_rule(conditions[1], actions[1]))\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[2], actions[2])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[3], actions[3])\nlcs_classifier2 &lt;- add_valid_rule_to_pop(lcs_classifier2, conditions[4], actions[4])\n\ntest_that(\"validate correct match on test set\", {\n    expect_equal(\n        get_match_set_v002(test_environment$state[1], lcs_classifier2), \n        c(1)\n    )\n    expect_equal(\n        get_match_set_v002(test_environment$state[2], lcs_classifier2),\n        c(1, 2)\n    )\n})\n\n\n\nAn example population"
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#well-keep-the-list-format-for-now",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#well-keep-the-list-format-for-now",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "microbenchmark(\n    sapply(test_environment$state, function(x) { get_match_set_v001(x, lcs_classifier1)}),\n    sapply(test_environment$state, function(x) { get_match_set_v002(x, lcs_classifier2)}),\n    times = 10000L\n)\n\n\n\nlists and pre-calculated match positions is faster\n\n\nLists are more flexible and data.frames are not the fastest option out there anyway. Here the pre-calculations in the second format also helps doing faster matching.\nHowever I know this choice might have an impact, particularly when time will come to implement deletion, subsumption and compaction (all useful for better LCS performance/value). And another idea I have in mind will require further lists management (but it‚Äôs too early to discuss that).\nBut that‚Äôs a problem for future me (and nothing that hasn‚Äôt been solved already)."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#conclusions",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#conclusions",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "Well well: Day 1, and we have covered concepts of environment, states, actions, conditions, rules, classifiers, population and matching/match set.\nAlong with developping a functional (basic) list for storing classifiers, and a matching function.\nNot too bad (albeit all too simplistic and not going for ‚Äúfull-power‚Äù flexible LCS implementation)."
  },
  {
    "objectID": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#references",
    "href": "posts/2024-12-22_RLCS_Day1_Matching_and_Classifier_storage/index.html#references",
    "title": "RLCS Day 1: Rules formatting, storage, and matching",
    "section": "",
    "text": "I‚Äôll keep referring to this video (although the book is better to get acquainted with the vocabulary and concepts): intro explaining the algorithm ."
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html",
    "title": "RLCS: Improving‚Ä¶ And getting worse (at the same time!)",
    "section": "",
    "text": "Well, it‚Äôs been a couple of weeks (I was a bit offline last couple of weekends, I guess). So I‚Äôm back at it and, first, I needed to clean things up. It just was messy code.\nSo I did that, removed some duplicate code with functions, unused variables, found out some of it was in fact quite wrong‚Ä¶\nHeck, I half-wonder how it actually worked so well a couple of weeks back learning, that simple agent in that simple small ‚Äúworld‚Äù moving around looking for ‚Äúfood‚Äù‚Ä¶\nI also studied a bit more on RL (although, I have not read as much as I wanted, I had a couple of other books under way‚Ä¶ Anyhow), and modified things so that the reward distribution would better follow the ideas of temporal difference (TD).\nNow code is a bit cleaner, probably running smoother, less cluttered‚Ä¶\nBUT!\n\n\n\nAs it turns out, in fixing the code, I made the learning worse‚Ä¶ It still kind-of works, but from the last few runs, it‚Ä¶ well, it feels it learns worse.\nAnd I‚Äôm really wondering which of the many parameters I have changed, on top of the fixes, have made it bad.\n\n\n\nThere is still stuff to clean. I‚Äôm not advancing as much as I wanted with setting things up in an actual package yet. And while the Supervised Learning is working fine (maybe even better), the Reinforcement Learning example is worse off after this weekend.\nNo matter, I‚Äôll review again. And I‚Äôll fix it. I‚Äôm just slower than I wanted. Oh well‚Ä¶"
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#better-code",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#better-code",
    "title": "RLCS: Improving‚Ä¶ And getting worse (at the same time!)",
    "section": "",
    "text": "Well, it‚Äôs been a couple of weeks (I was a bit offline last couple of weekends, I guess). So I‚Äôm back at it and, first, I needed to clean things up. It just was messy code.\nSo I did that, removed some duplicate code with functions, unused variables, found out some of it was in fact quite wrong‚Ä¶\nHeck, I half-wonder how it actually worked so well a couple of weeks back learning, that simple agent in that simple small ‚Äúworld‚Äù moving around looking for ‚Äúfood‚Äù‚Ä¶\nI also studied a bit more on RL (although, I have not read as much as I wanted, I had a couple of other books under way‚Ä¶ Anyhow), and modified things so that the reward distribution would better follow the ideas of temporal difference (TD).\nNow code is a bit cleaner, probably running smoother, less cluttered‚Ä¶\nBUT!"
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#worse-results",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#worse-results",
    "title": "RLCS: Improving‚Ä¶ And getting worse (at the same time!)",
    "section": "",
    "text": "As it turns out, in fixing the code, I made the learning worse‚Ä¶ It still kind-of works, but from the last few runs, it‚Ä¶ well, it feels it learns worse.\nAnd I‚Äôm really wondering which of the many parameters I have changed, on top of the fixes, have made it bad."
  },
  {
    "objectID": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#conclusions",
    "href": "posts/2025-03-09_RLCS_RL_BetterAndWorse/index.html#conclusions",
    "title": "RLCS: Improving‚Ä¶ And getting worse (at the same time!)",
    "section": "",
    "text": "There is still stuff to clean. I‚Äôm not advancing as much as I wanted with setting things up in an actual package yet. And while the Supervised Learning is working fine (maybe even better), the Reinforcement Learning example is worse off after this weekend.\nNo matter, I‚Äôll review again. And I‚Äôll fix it. I‚Äôm just slower than I wanted. Oh well‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "This is quite literally my first Reinforcement Learning exercise ever. Coded from scratch. With no Neural Network whatsoever, but instead, using my own implementation of the Learning Classifier System(s) I have been discussing lately.\nAnd the point is this: It works!\nHere I present the first actual working results.\n\n\n\nSo we have an agent (blue dot) that can move in any of {up, down, right, left} direction.\nWe have a simulated world, with walls (black dots), some rewarding food (green dots) and some hurting enemies/poison (red dots).\nIndeed the aestetics of this demo are not marvelous, but I‚Äôve never cared much about look&feel, I care about what happens under the hood here.\n\n\n\nmonitoring the learning process\n\n\nSo our agent moves, and gets a reward as informed by our world. For instance:\n\nNo reward if moving to an empty cell\nSmall negative reward if moving backwards (to the cell it was in at immediately anterior step)\nsomewhat larger negative reward if ‚Äúbumping‚Äù into a wall\nlarge positive reward if finding food (obviously)\nand large negative reward if moving onto an enemy/poison cell (duh!)\n\nThe trick is: We do NOT tell the agent what to do! It must learn how to behave on its own, just by maximizing its reward.\nAnd here, the agent will use my implementation of a Learning Classifier System. No neural network or deep learning or anything fancy like that :D\nOh, and every line of code is in R.\n\n\n\nSo there is a bit more to it. I will not explain today in detail my whole implementation of an LCS algorithm to do RL. But some of the logic available to the agent is important to understand current results:\n\nI chose to implement a ‚Äúsmall memory‚Äù for my agent, so that a reward will inform the current step and be ‚Äúpassed onto‚Äù the last action, divided by 2.\nThe reason to do that is the following: the agent only sees one cell in all 8 directions (inlcuding diagonals), and it can move in 4 main directions. The idea of the small memory was to allow the machine to learn rules to choose to ‚Äúmove diagonally‚Äù. I haven‚Äôt validated exactly if this is the result. But it seems the agent does work better with that little bit of ‚Äúmemory‚Äù.\n\nI‚Äôll discuss the complete algorithm later on, but today let‚Äôs look at the result in a short video!\n\n\n\nHere is a small video I made of the current results‚Ä¶\n\n\n\n\nI am unclear at this point about whether I just implemented a version ‚Äì or a mix ‚Äì of TD, Q-Learning, XCS, CS01‚Ä¶ That‚Äôs the theory part, and I‚Äôll need to dig deeper into that now, to frame better what I just created.\n(Also, the code is very‚Ä¶ messy, right now :D I just wanted to see if I could get it to work‚Ä¶)\nBut from an experimental perspective, this was very satisfying already."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#reinforcement-learning-with-lcs",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#reinforcement-learning-with-lcs",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "This is quite literally my first Reinforcement Learning exercise ever. Coded from scratch. With no Neural Network whatsoever, but instead, using my own implementation of the Learning Classifier System(s) I have been discussing lately.\nAnd the point is this: It works!\nHere I present the first actual working results."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#limited-agent-simple-rules",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#limited-agent-simple-rules",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "So we have an agent (blue dot) that can move in any of {up, down, right, left} direction.\nWe have a simulated world, with walls (black dots), some rewarding food (green dots) and some hurting enemies/poison (red dots).\nIndeed the aestetics of this demo are not marvelous, but I‚Äôve never cared much about look&feel, I care about what happens under the hood here.\n\n\n\nmonitoring the learning process\n\n\nSo our agent moves, and gets a reward as informed by our world. For instance:\n\nNo reward if moving to an empty cell\nSmall negative reward if moving backwards (to the cell it was in at immediately anterior step)\nsomewhat larger negative reward if ‚Äúbumping‚Äù into a wall\nlarge positive reward if finding food (obviously)\nand large negative reward if moving onto an enemy/poison cell (duh!)\n\nThe trick is: We do NOT tell the agent what to do! It must learn how to behave on its own, just by maximizing its reward.\nAnd here, the agent will use my implementation of a Learning Classifier System. No neural network or deep learning or anything fancy like that :D\nOh, and every line of code is in R."
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#a-few-more-details",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#a-few-more-details",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "So there is a bit more to it. I will not explain today in detail my whole implementation of an LCS algorithm to do RL. But some of the logic available to the agent is important to understand current results:\n\nI chose to implement a ‚Äúsmall memory‚Äù for my agent, so that a reward will inform the current step and be ‚Äúpassed onto‚Äù the last action, divided by 2.\nThe reason to do that is the following: the agent only sees one cell in all 8 directions (inlcuding diagonals), and it can move in 4 main directions. The idea of the small memory was to allow the machine to learn rules to choose to ‚Äúmove diagonally‚Äù. I haven‚Äôt validated exactly if this is the result. But it seems the agent does work better with that little bit of ‚Äúmemory‚Äù.\n\nI‚Äôll discuss the complete algorithm later on, but today let‚Äôs look at the result in a short video!"
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#the-result",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#the-result",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "Here is a small video I made of the current results‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#conclusions",
    "href": "posts/2025-02-16_RLCS_World_Explorer_v001/index.html#conclusions",
    "title": "RLCS: Reinforcement Learning: World Explorer v1",
    "section": "",
    "text": "I am unclear at this point about whether I just implemented a version ‚Äì or a mix ‚Äì of TD, Q-Learning, XCS, CS01‚Ä¶ That‚Äôs the theory part, and I‚Äôll need to dig deeper into that now, to frame better what I just created.\n(Also, the code is very‚Ä¶ messy, right now :D I just wanted to see if I could get it to work‚Ä¶)\nBut from an experimental perspective, this was very satisfying already."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html",
    "href": "posts/2024-11-02_WordEmbeddings/index.html",
    "title": "ML Concepts: Word Embeddings",
    "section": "",
    "text": "I discussed an unsupervised clustering algorithm, DBScan. We used multi-dimensional points (coordinates) in space. But what if our data is text? That‚Äôs common in ‚Äúgeneral‚Äù, but also in Cybersecurity. So the question becomes: Can we treat ‚Äútext‚Äù as points on which to apply such (or similar) algorithms?\nEnters the concept of embedding."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#continuing-on-text-for-cybersecurity-ml",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#continuing-on-text-for-cybersecurity-ml",
    "title": "ML Concepts: Word Embeddings",
    "section": "",
    "text": "I discussed an unsupervised clustering algorithm, DBScan. We used multi-dimensional points (coordinates) in space. But what if our data is text? That‚Äôs common in ‚Äúgeneral‚Äù, but also in Cybersecurity. So the question becomes: Can we treat ‚Äútext‚Äù as points on which to apply such (or similar) algorithms?\nEnters the concept of embedding."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#conceptual-understanding",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#conceptual-understanding",
    "title": "ML Concepts: Word Embeddings",
    "section": "Conceptual understanding",
    "text": "Conceptual understanding\nOnce again, who am I to ‚Äúteach‚Äù you what an embedding is, hu? It‚Äôs probably better to go to the definition, which hopefully, thanks to the context provided in the last entry, can help intuit where we‚Äôre going with all this:\n‚ÄúIn¬†natural language processing, a¬†word embedding¬†is a representation of a word. The embedding is used in text analysis. Typically, the representation is a¬†real-valued¬†vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning.‚Äù\nSo two things: Today is actually about Natural Language Processing, or NLP in short. Not a new topic in this Blog, but hey.\nSecond, we‚Äôre looking for a representation of a word as a ‚Äúreal-valued vector‚Äù. So think of it like so: A ‚Äúvector‚Äù can represent many things, but today we‚Äôre going to consider it a set of coordinates.\nSo in 3 dimensions (3D), a word embedding would represent a word as 3 numbers, representing each a coordinate of the (x, y, z) space. Sounds familiar? Check again the entry on Clustering if not, please, it makes more sense to consider both entries together‚Ä¶\nYou‚Äôre back?\nOK. For very large text, maybe the information of a word with only 3 dimensions is not enough to ‚Äúencode‚Äù its relationship to other words. So you would go into higher dimensions, say 15, 30 or 1000 dimensions (again, why not? You and I can‚Äôt visualize 30 dimensions in our heads, but it‚Äôs easy for a computer‚Ä¶ Fun, ain‚Äôt it?)\nAnd so with a set of vectors, each representing a word, we get in fact a set of points in an N-dimensional space. And then‚Ä¶\nWhy not use algorithms on these ‚Äúembeddings‚Äù, say the DBScan algorithm?"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#in-practice-embedding-from-texts",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#in-practice-embedding-from-texts",
    "title": "ML Concepts: Word Embeddings",
    "section": "In practice: Embedding from text(s)",
    "text": "In practice: Embedding from text(s)\nWe‚Äôre not going to discuss the current algorithms for embeddings in detail. They use‚Ä¶ Neural Networks, of all things. The general accepted version of Word2Vec for instance in essence takes pairs of words (out of ‚Äútraining text‚Äù) and transforms the ‚Äúwords‚Äù into vectors of 300 dimensions (if I remember correctly). Better even, you can get the embeddings, so you need not train anything yourself (thanks Google AI!).\nBut for today, we might just want to go ahead and actually train our own embeddings. Let‚Äôs go for it!\nBy the way: Here the code for today.\nNow one important concept for Machine Learning: ‚ÄúGarbage IN? Garbage OUT!‚Äù So IF I use cra*py (pardon my french) text as input, I shouldn‚Äôt expect much of a result as an output.¬†\nLet‚Äôs say I consider the Wikipedia to hold ‚Äúgood‚Äù text about some IT and Cybersecurity concepts. If so, I could use for instance the R Package wikipediR. (At least that‚Äôs the one I used for today)\nlibrary(WikipediR) # Get Wiki data\n## Simple wrapper\nmy_page_content &lt;- function(keywords) {\n  page_content(language = \"en\",\n    project = \"wikipedia\",\n    page_name = keywords,\n    as_wikitext = FALSE,\n    clean_response = TRUE) |&gt;\n  clean_text_set()\n}\n## Explicitly for explanation:\nfirewall_wiki &lt;- my_page_content(\"firewall (computing)\")\nfirewall_wiki &lt;- firewall_wiki[1:82]\nswitch_wiki &lt;- my_page_content(\"network switch\")\nswitch_wiki &lt;- switch_wiki[2:96]\nAnd it goes on, with a few other keywords of interest (say ‚Äúrouter‚Äù, ‚Äúhacker‚Äù‚Ä¶). You have the details of this example in the code.\nWhy I filter and keep only 82 paragraphs of the Firewall entry? Just a matter of cleaner stuff, the way I parse the Wiki entries (which is detailed in ‚Äúclean_text_set()‚Äù function), some lines contain a bit of only pairs of words, others a few references with proper Names, etc.¬† Consider this a manual process in this case because I‚Äôm lazy. In the real world, I would look for the key words that mark the section of the Wiki Entry that do not interest me, and keep the ones above that only. And clean those. Like it or not, in this case, I needed to extract meaningful sentences of some HTML pages. It requires a bit of work (my ‚Äúcleant_text_set()‚Äù function, created for today‚Äôs exercise specifically, hopefully can show how one has to work, it‚Äôs not always as simple as running a function call‚Ä¶). All in all, after pre-processing, I‚Äôll end up with 692 sentences. In traditional ML, a good part of the work is about getting the right data in the right format. And that‚Äôs all I say about that today. Moving on.\nThe next step will be to use our data. Here I‚Äôm not going to implement anything myself, it‚Äôs beyond my point. Suffice to say I‚Äôm going to use the ‚ÄúContinuous Bag of Words‚Äù, that looks at words AROUND one word to assign positions in space. In concept and simplifying a lot, we‚Äôre seeing if ‚Äúblock‚Äù and ‚Äúfirewall‚Äù appear in the training text near one another more often than ‚Äúrestaurant‚Äù and ‚Äúfirewall‚Äù. (I‚Äôd guess that‚Äôs about right :D)\nNow we can use R to train our own Word2Vec Neural Network, with Bag Of Words, on our sample text (692 small blocks of text) and ask it to come up with vectors of 30 dimensions as embeddings for the main keywords found in the text.\n## Lets' move on to something more... Substantial:\nfull_text &lt;- c(\n  switch_wiki,\n  router_wiki,\n  firewall_wiki,\n  hacker_wiki,\n  computer_wiki,\n  cpu_wiki,\n  virus_wiki\n)\n\nmodel &lt;- word2vec(full_text, type=\"cbow\", dim=30, iter = 50)\nembeddings &lt;- as.matrix(model)\nembeddings\nAnd yes, it requires a few things (the ‚Äúword2vec‚Äù R package, for one), a bit of understanding (or trial and error, but understanding is better!). But if all goes as planned you‚Äôll get something like this:\n\n\n\nSo 970 words have been transformed into their corresponding 30-dimensional vectors! Good!\n30 dimensions is going to be hard to ‚Äúlook at‚Äù, but let‚Äôs do it anyway. What we really want is to understand the similarity of things here. So for instance, FROM OUR SELECTED WIKI text (very small sample, if you ask me, and yet‚Ä¶):\n\nNow I wouldn‚Äôt necessarily agree that ‚Äúpix‚Äù is the best nearest word for ‚Äúfirewall‚Äù but‚Ä¶ That‚Äôs what our sample text says, it would seem. Anyhow, it doesn‚Äôt sound completely crazy either. (e.g.¬†‚ÄúRestaurant‚Äù, had it been in the sample text, hopefully wouldn‚Äôt appear in the top 5 ‚Äúnearest‚Äù terms for Firewall‚Ä¶)\n30 dimensions is going to be hard to ‚Äúlook at‚Äù, but let‚Äôs do it anyway. What we really want is to understand the similarity of things here. So for instance, FROM OUR SELECTED WIKI text (very small sample, if you ask me, and yet‚Ä¶):\nNow I wouldn‚Äôt necessarily agree that ‚Äúpix‚Äù is the best nearest word for ‚Äúfirewall‚Äù but‚Ä¶ That‚Äôs what our sample text says, it would seem. Anyhow, it doesn‚Äôt sound completely crazy either. (e.g.¬†‚ÄúRestaurant‚Äù, had it been in the sample text, hopefully wouldn‚Äôt appear in the top 5 ‚Äúnearest‚Äù terms for Firewall‚Ä¶)"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#going-2d-and-hint-for-the-future",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#going-2d-and-hint-for-the-future",
    "title": "ML Concepts: Word Embeddings",
    "section": "Going 2D and hint for the future",
    "text": "Going 2D and hint for the future\nI‚Äôm going to finish this with one visualization, and then hopefully everything will come together. Now I‚Äôll say this first: I know you can bring 30 dimensions into 2, yes. I was going to try ‚Äúmulti-dimensional scaling‚Äù (as PCA is probably too lossy for such a reduction), or look into some algorithm for that. But then I came across examples here, and heck, dimensionality reduction was beyond the point for today, and so I skipped doing it myself. (To this day, I haven‚Äôt looked at how the umap() function works. I know, shame on me.)\nBut here is the key of all the conversation for today:\n\n\n\nProjecting Embeddings onto 2D plot\n\n\nWe‚Äôve done it! We have visualized our words, not without first creating embeddings for them, and then projecting into 2 Dimensions.\nAnd let‚Äôs have a look at what is where‚Ä¶\nNot bad, ‚Äúfirewalls‚Äù is near ‚Äúfirewall‚Äù (pfiu!). So are filter, traffic‚Ä¶ And maybe the rest is not great, but that‚Äôs what we came up with from (again) very little sample text."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#mixing-things-up",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#mixing-things-up",
    "title": "ML Concepts: Word Embeddings",
    "section": "Mixing things up!",
    "text": "Mixing things up!\nLast week, I published a (simplistic) entry about DBScan as an algorithm to cluster things. WHY NOT apply that here?!\n\nCould we have a look at one cluster, maybe one that contains the word ‚ÄúVirus‚Äù?\n\nThings like ‚Äúinfection‚Äù, ‚Äúexecutable‚Äù, ‚Äúscanner‚Äù, ‚Äúmalicious‚Äù are all in the area‚Ä¶ And with the same color!\nWith more text and better cleanup‚Ä¶ I‚Äôm convinced the approach has its merits üôÇ\nAs per 3D visuals, this time using Multi-dimensional Scaling (another algorithm that uses distances!) to project onto 3 dimensions, well it works, but there is a bit too much data, and maybe it‚Äôs not super super useful‚Ä¶ Still:"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#applications",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#applications",
    "title": "ML Concepts: Word Embeddings",
    "section": "Applications",
    "text": "Applications\nWhat if instead of Wiki entries from the internet, we had taken CVE text (maybe along with their CVSS (or whatever scoring system you prefer))? We could probably do some sort of regression once we encode the text (maybe even mixing things up with other algorithms) and maybe estimate the score?\nHow about classifying threat alerts into groups?\nWhat if we had taken logs from a machine. Could we maybe use all this and find specially anomalous logs, from the complete log file? (Imagine thousands of lines reviewed in seconds by your laptop like so‚Ä¶)\nAnd consider this: Over this and the last Blog entry, we‚Äôve discussed enough to do some basic ML. But there is much more than Clustering applied to text. I just hope this helps give a hint of the possibilities. üôÇ"
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#conclusion",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#conclusion",
    "title": "ML Concepts: Word Embeddings",
    "section": "Conclusion",
    "text": "Conclusion\nAt this stage, I truly hope we understand what it means for us to be able to put words (or texts, they could be each files‚Ä¶ why not!) into vectors (i.e.¬†points in N-dimensional space), to be projected (or not) and for which distances can be calculated to other words/texts.\nWith that, we open a world of possibilities: Topic Modelling, Sentiment Analysis, etc. can all be done using distances between points üôÇ (There is more to NLP, though, and NOT only LLMs, please. More simple/traditional stuff is out there! One example I wrote about forever ago was part of speech tagging, for instance. Another time I used TF-IDF to model a classifier of log files with supervised learning‚Ä¶) Maybe in a future post I‚Äôll discuss more of these concepts, but I‚Äôd be very happy for now if somehow I helped someone out there understand a bit better how these things could actually work.\nBy the by: GenAI essentially does self-supervised learning on word embeddings. (WOW! What a bomb to leave at the end of a blog entry, ‚Äúself-supervised‚Äù???)."
  },
  {
    "objectID": "posts/2024-11-02_WordEmbeddings/index.html#bonus-a-word-about-genai",
    "href": "posts/2024-11-02_WordEmbeddings/index.html#bonus-a-word-about-genai",
    "title": "ML Concepts: Word Embeddings",
    "section": "Bonus: A word about GenAI",
    "text": "Bonus: A word about GenAI\nOK, OK, OK‚Ä¶ But real quick then.\nFirst: I don‚Äôt particularly like GenAI as a topic because ‚Äì mostly ‚Äì of the hype, misunderstanding, risks‚Ä¶ but otherwise is undoubtedly incredibly powerful and I‚Äôll admit it must have some cool applications‚Ä¶ For expert users! And although very slowly, I myself am considering using it as an assistant R-coder‚Ä¶ I have done tests and it‚Äôs not bad at all, and it WOULD make me much faster‚Ä¶ But I‚Äôm mostly resisting for now: Coding and thinking how to approach a problem is what I like, so why externalize that‚Ä¶ Unless I really HAVE to‚Ä¶\nThat said‚Ä¶ What the heck is ‚Äúself-supervised‚Äù learning?¬†\nSupervised learning needs to have a means of knowing whether it‚Äôs doing a good job to rectify its own behaviour while in training.\nIf your job is to predict the best next word for a given text‚Ä¶ All you need is to try to predict it, and then read the next word (or ‚Äútoken‚Äù). If you guessed wrong, you rectify your behaviour for your next guess. Then you read the next word‚Ä¶ And iterate. And in the above scenario, nobody needs to ‚Äútag‚Äù anything, the information is self-contained! So you just ingest text one ‚Äútoken‚Äù at a time (don‚Äôt worry, say ‚Äúone word at a time‚Äù, and more or less you‚Äôre good). All you need is text (and ‚Äúattention‚Äù, but that‚Äôs WAY beyond today‚Äôs objectives :D).\nThe more text, the more training examples you get üôÇ\nAnd yes: Your words/tokens, are presented to your GenAI (well, LLMs, really) as‚Ä¶ Embbedings."
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Today I just started something simple: An Object to represent a simulated World in which an Agent can move.\nAs the ‚ÄúAgent‚Äù will need to interact with the World, and the World should return rewards, and understand positions, etc., I decided in this case to go the way of R Objects, more specifically ‚ÄúsetRefClass‚Äù.\nFor today, I just created a basic object to generate a ‚Äúrandom world‚Äù that can have walls, food, even enemy(ies) and in which we can throw an agent at random in any empty space.\nEDIT: Added Agent movements and visualizations\nA few peculiarities maybe: I think it will be simpler in the future to not have to deal with ‚Äúborders‚Äù from the Agent perspective, and so I set my Worlds to necessarily be walls all around.\n\n\n\nLet‚Äôs start today with a simple video showing the results. The agent is the blue thing. Food is green, enemy is red, black are walls.\nAll is initialized randomly, and so are the moves of the current agent. On the left, I show the agent‚Äôs perspective/perception (allocentric) of the World. On the right, the World as it is at each step:\n\nVideos are always more satisfying than matrices, but to understand what happens, let‚Äôs have a look under the hood‚Ä¶\n\n\n\nSo in my matrix, I choose 0 = empty cell, 1 is a wall, 2 is food, 3 is an enemy (defaults to no enemies). And 4 will be an agent. This is all arbitrary, but when in the future I encode into binary there will be a specific order, as this will be relevant for the GA to work with.\nAnyhow, it looks like so:\n\n\n\nA simple world object\n\n\nIn this ‚Äúworld‚Äù, I needed to add methods to move around (from-to positions, or just ‚Äúto‚Äù), control for ilegal moves (can‚Äôt move into a wall), return rewards: good +10 for food, bad -100 if enemy, and empty and walls, well, to be refined.\nAll very simple, but cool at the same time: I can already render ‚Äúanimations‚Äù of said worlds‚Ä¶ Albeit with a very stupid agent for now‚Ä¶\n\n\n\nWell, I‚Äôm afraid now I have most of the pieces. But still:\n\nI will need to implement a variation of my existing RLCS code to account for four possible actions (for now, actions are binary, that‚Äôs not enough).\nI will need to add a sensible binary encoding for the agent to interpret its visible environment\nI will need to account for rewards, change the design for the RL case where I need to add prediction and an Action Set\n‚Ä¶\n\nQuite a bit of work still. But all fun.\nTO BE CONTINUED‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#the-world-should-be-easy-to-interact-with",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#the-world-should-be-easy-to-interact-with",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Today I just started something simple: An Object to represent a simulated World in which an Agent can move.\nAs the ‚ÄúAgent‚Äù will need to interact with the World, and the World should return rewards, and understand positions, etc., I decided in this case to go the way of R Objects, more specifically ‚ÄúsetRefClass‚Äù.\nFor today, I just created a basic object to generate a ‚Äúrandom world‚Äù that can have walls, food, even enemy(ies) and in which we can throw an agent at random in any empty space.\nEDIT: Added Agent movements and visualizations\nA few peculiarities maybe: I think it will be simpler in the future to not have to deal with ‚Äúborders‚Äù from the Agent perspective, and so I set my Worlds to necessarily be walls all around."
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#a-random-policy-agent-in-a-random-world",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#a-random-policy-agent-in-a-random-world",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Let‚Äôs start today with a simple video showing the results. The agent is the blue thing. Food is green, enemy is red, black are walls.\nAll is initialized randomly, and so are the moves of the current agent. On the left, I show the agent‚Äôs perspective/perception (allocentric) of the World. On the right, the World as it is at each step:\n\nVideos are always more satisfying than matrices, but to understand what happens, let‚Äôs have a look under the hood‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#todays-results-in-matrix-format",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#todays-results-in-matrix-format",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "So in my matrix, I choose 0 = empty cell, 1 is a wall, 2 is food, 3 is an enemy (defaults to no enemies). And 4 will be an agent. This is all arbitrary, but when in the future I encode into binary there will be a specific order, as this will be relevant for the GA to work with.\nAnyhow, it looks like so:\n\n\n\nA simple world object\n\n\nIn this ‚Äúworld‚Äù, I needed to add methods to move around (from-to positions, or just ‚Äúto‚Äù), control for ilegal moves (can‚Äôt move into a wall), return rewards: good +10 for food, bad -100 if enemy, and empty and walls, well, to be refined.\nAll very simple, but cool at the same time: I can already render ‚Äúanimations‚Äù of said worlds‚Ä¶ Albeit with a very stupid agent for now‚Ä¶"
  },
  {
    "objectID": "posts/2025-02-15_A_World_To_Play_RL/index.html#what-next",
    "href": "posts/2025-02-15_A_World_To_Play_RL/index.html#what-next",
    "title": "RLCS: A World to Play RL",
    "section": "",
    "text": "Well, I‚Äôm afraid now I have most of the pieces. But still:\n\nI will need to implement a variation of my existing RLCS code to account for four possible actions (for now, actions are binary, that‚Äôs not enough).\nI will need to add a sensible binary encoding for the agent to interpret its visible environment\nI will need to account for rewards, change the design for the RL case where I need to add prediction and an Action Set\n‚Ä¶\n\nQuite a bit of work still. But all fun.\nTO BE CONTINUED‚Ä¶"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kaizen-R.com new home",
    "section": "",
    "text": "RLCS goes to the National R Congress!\n\n\n\n\n\n\nRLCS\n\n\ncode\n\n\n\n\n\n\n\n\n\nSep 16, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nOrganizing a bit for future work/exercises\n\n\n\n\n\n\ncode\n\n\nmath\n\n\nsystems\n\n\nOptimization\n\n\nDecisions\n\n\nABM\n\n\nInformation\n\n\ncomplexity\n\n\n\n\n\n\n\n\n\nAug 24, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nThinking about Systems\n\n\n\n\n\n\ncomplexity\n\n\nABM\n\n\nsystems\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS with the mirai package\n\n\n\n\n\n\nRLCS\n\n\ncode\n\n\n\n\n\n\n\n\n\nAug 10, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Less sequential?\n\n\n\n\n\n\nRLCS\n\n\ncode\n\n\n\n\n\n\n\n\n\nJul 25, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nNonlinear Dynamics: on Bifurcations\n\n\n\n\n\n\nmath\n\n\ncode\n\n\ncomplexity\n\n\n\n\n\n\n\n\n\nJul 20, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nStudying Nonlinear Dynamics: Day 1\n\n\n\n\n\n\nmath\n\n\ncode\n\n\ncomplexity\n\n\n\n\n\n\n\n\n\nJul 13, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS package: Progressing nicely\n\n\n\n\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nJul 6, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS as a package?\n\n\n\n\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS for text? And a new idea‚Ä¶\n\n\n\n\n\n\nNLP\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nJun 21, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nMorning Coffee Thoughts\n\n\n\n\n\n\nDecisions\n\n\n\n\n\n\n\n\n\nJun 9, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nClassic Computational Social Model: Schelling‚Äôs Segregation\n\n\n\n\n\n\nDecisions\n\n\nOptimization\n\n\nvisualization\n\n\ncomplexity\n\n\n\n\n\n\n\n\n\nJun 8, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Agent RL\n\n\n\n\n\n\nRLCS\n\n\nML\n\n\n\n\n\n\n\n\n\nJun 5, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Caring for better performance\n\n\n\n\n\n\nRLCS\n\n\ncode\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nMay 25, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Making things faster where it matters\n\n\n\n\n\n\nRLCS\n\n\ncode\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nMay 18, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: small code rewrites\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\ncode\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nMay 2, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Better code?\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\ncode\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nApr 24, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nSunday Thoughts: About Strategy, Information, Decisions, Simulations, Reinforcement Learning & al.\n\n\n\n\n\n\nInformation\n\n\nML\n\n\nDecisions\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Small addition\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS for Data Mining: Visuals\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS & Explainable AI\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nMar 19, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS for RL: It works again (using books helps)\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nMar 16, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating Contents (slowly)\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 14, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Improving‚Ä¶ And getting worse (at the same time!)\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nMar 9, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Reinforcement Learning: World Explorer v1\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nFeb 16, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: A World to Play RL\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: Reinforcement Learning?\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: training in parallel?\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nJan 26, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS: for Data Mining\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nJan 25, 2025\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 4.5 - Fully Functional v0 and New Tests\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 2 - Full Rule Discovery\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 24, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 1 - Part II: Basic Covering\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRLCS Day 1: Rules formatting, storage, and matching\n\n\n\n\n\n\nML\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nA new project\n\n\n\n\n\n\nML\n\n\nnews\n\n\nRLCS\n\n\n\n\n\n\n\n\n\nDec 8, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Linear Regression\n\n\n\n\n\n\nML\n\n\nmath\n\n\n\n\n\n\n\n\n\nNov 25, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying URLs\n\n\n\n\n\n\nML\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 17, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nEntropy - Identifying compressed files\n\n\n\n\n\n\nmath\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 10, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nML Concepts: Word Embeddings\n\n\n\n\n\n\nML\n\n\nNLP\n\n\n\n\n\n\n\n\n\nNov 2, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nML Concepts: Unsupervised Learning ‚Äì Clustering ‚Äì DBScan\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Exercises: Interpolation with Lagrange Polynomials\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nOct 20, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs locally on basic Apple Silicon\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nOct 12, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nCholesky‚Äôs matrix decomposition and where not to use it\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nSep 29, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nMSC Thesis delivered\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nJun 15, 2024\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nWhile on the Train: Cellular Automata\n\n\n\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nInterlude\n\n\n\n\n\n\ncomplexity\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization (4/n): Genetic Algorithm(s) (2/2)\n\n\n\n\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nAug 6, 2023\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization (4/n): Genetic Algorithm(s) (1/m)\n\n\n\n\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization (3/n): Simulated Annealing\n\n\n\n\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization (2/n): Gradient Descent\n\n\n\n\n\n\nOptimization\n\n\nmath\n\n\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nSummer fun: Testing Optimization Algorithms (1/n)\n\n\n\n\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nA quick look at CISA KEV\n\n\n\n\n\n\ncyber\n\n\n\n\n\n\n\n\n\nJul 10, 2022\n\n\nNico\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Entry: Variables\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nJun 20, 2020\n\n\nNico\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "others/test_new_subsite.html",
    "href": "others/test_new_subsite.html",
    "title": "Test_new_quarto",
    "section": "",
    "text": "Testing subsite with one embedded Quarto file.\n\n\n\n\n1+1\n\n[1] 2"
  },
  {
    "objectID": "others/test_new_subsite.html#sub-content-1",
    "href": "others/test_new_subsite.html#sub-content-1",
    "title": "Test_new_quarto",
    "section": "",
    "text": "Testing subsite with one embedded Quarto file."
  },
  {
    "objectID": "others/test_new_subsite.html#sub-content-2",
    "href": "others/test_new_subsite.html#sub-content-2",
    "title": "Test_new_quarto",
    "section": "",
    "text": "1+1\n\n[1] 2"
  },
  {
    "objectID": "posts/2025-03-30_Small_Addition_RLCS/index.html",
    "href": "posts/2025-03-30_Small_Addition_RLCS/index.html",
    "title": "RLCS: Small addition",
    "section": "",
    "text": "I was in attending a local R conference the other day, and took the opportunity to discuss a bit with some of the (very few) that have a minimal interest (and an understanding) of what I‚Äôm trying to do these days with the RLCS project.\nIt‚Äôs always great to talk with them, they know so much, there is so much experience in there. And of course, that means, they can criticize my work. Which, from them, or anyone with criteria for that matter, I happily welcome!\n\n\n\nAlright, so far I‚Äôve always taken examples where I could translate the problems into binary strings for the input.\nFor images, I took each pixel and set it to 1 if the pixel was ‚Äúused‚Äù (say, a high shade of black), and 0 if it wasn‚Äôt (say, a light shade of grey). See https://kaizen-r.github.io/posts/2025-03-19_ExplainableAI/ for instance, for a visual intuition.\nOther examples were specifically chosen to be already presenting binary ‚Äústates‚Äù. That was the case with ‚Äúnot_bit_4‚Äù or ‚Äúmux_6‚Äù here.\nBut it is very common to have numerical variables as input, not‚Ä¶ Well, not simply binary ones.\nThat said, pause for a second: What is a binary string if not (often times) a numerical value, duly encoded? It‚Äôs quite straightforward, if you think about it. Say you want to use 4 bits, you can encode 16 values, say the natural numbers (0:15). So 0000 is 0, 0010 is 2, and 1111 is 15.\nBut let‚Äôs make a second pause: What if you have more values? Say 32 of them, from 0 to 31? And say, for the sake of example, that we want to limit the number of bits to be used still to 4 bits. Then we could decide to assume a loss of information, and if so, use 0000 as a state for actual values 0 and 1, and 1111 for actual values 30 and 31.\nWhy limit the number of bits, you ask? Well, because the way it is working currently, each added bit to the states increases the search space of the algorithm by a (multiplication) factor of 2. Which is to say, long binary strings as states are costly to process and depending on the problem, with lots of samples for training, for instance, would require a lot of time (Note: I‚Äôm not discussing here ‚Äúa lot of processing power‚Äù, because as it is, paralelising is complex, although not impossible, and with varying efficacy depending on the scenarios). With few samples, or clear separation, for instance, the limitation of forcing a sequential processing can be overcome with nicely chosen hyperparameters. But it‚Äôs not straightforward. Anyhow, back to the topic.\nPause number 3: What if we had real-valued variables. Each variable then fits within its own range. I hereby suggest we could ‚Äúbreak‚Äù it in buckets, so that each bucket can then in turn represent a subset of the chosen variable values, a range.\nHow to break that though? Choosing means kind of makes an assumption about the distribution (normality) of input variable. So for now, and without thinking much about it I prefer to use the median, and quartiles, thereby breaking the input in 4 buckets of similar (or almost identical) sizes. And then, why not, break each bucket in turn in 4 sub-buckets. Voil√†: 16 subsets of the one variable. To be represented each as a 4 bits binary string.\nAnd no, I haven‚Äôt given it much more thought. This is just a simplified approach, one which I do not believe is perfect (maybe considering the input distribution would be better), or even best for the LCS processing; one which my statistician friends will surely criticize the next time I meet them‚Ä¶ But the important thing is the conceptual approach: If you accept loss of information, there is nothing precluding you from encoding your real numbers input into binary strings!\nThereby, of course, making the algorithm more practical to use.\n\n\n\nFinal detail for today: While reading the book ‚ÄúIntroduction to Learning Classifier Systems‚Äù (mentioned in the first post on the RLCS topic), they make a point about the binary encoding.\n‚ÄúNormal binary‚Äù encoding of numbers is not always ‚Äúsequential‚Äù for consecutive numbers, in that the Hamming distance between 0011 and 0100 (3 and 4) is 2.\nThere are alternatives, and one that is proposed in the book is the Gray code. I won‚Äôt delve into the details, but suffice to say, I am not yet fully convinced it is a great approach, but there is some intuition to it that makes it probably better than traditional encoding.\nAnyhow, I make a very very simplistic implementation of all I have discussed thus far today for the (in-)famous iris dataset. (Infamous in that, it‚Äôs almost too common :D But then again, that‚Äôs what makes it a perfect example!)\n\n\n\nSo the iris dataset presents 4 real-valued variables, and 3 classes. Think of 16 buckets per variable, using quartiles and sub-quartiles. And a Gray code.\nCan we make a Supervised Learning model using LCS? Sure we can!\nThis is what it looks like, with the state simply putting together 4 strings of 4 bits:\n\n\n\nencoding iris, binary buckets and grey\n\n\nAnd what about the result?\nWell, it‚Äôs a bit slow: 4‚Äô for training. There are considerations about this, but overall, consider searching for solutions in a 16bits search space, where on top of everything else, classes are overlapping in different dimensions‚Ä¶ But no excuse, and I haven‚Äôt continued tweaking hyperparameters for this particular example.\nAnyhow. And what of the quality of the resulting LCS as a supervised learning classifier?\nI take 80% of the samples for the training (119) samples, leaving only 30 samples for testing (it‚Äôs not a big dataset :D). And the results are like so:\n\n\n\nnot bad a classifier\n\n\nThat‚Äôs about it. Although‚Ä¶\nThen again, other options are proposed in the book‚Ä¶\n\n\n\nWell, in fact, yes. There are alternatives, whereby instead of 0 or 1, you can take real numbers. The way this works (at least the way others have approached it and documented it in the reference book on the topic already mentioned) is like so:\n\neach variable is ‚Äúcovered‚Äù separately by a wide or narrow numerical range\nCoverage (creating new classifiers) would set a range around the variable value. Said range would replace our current bits in the state (bit strings).\nupon calling the Genetic Algorithm, the mutation step can replace said ranges with either generalization (so, # as implemented otherwise), but also it could try to widen or narrow the range.\n\nThe above would allow to cover more or less ground per classifier, making them more or less specific. I feel this seems correct. But I do also believe the computational effort would potentially be higher (I haven‚Äôt thought it through either).\nAlso, we need to decide what ‚Äúnarrow‚Äù or ‚Äúwide‚Äù signifies for each variable, for instance.\nAnd of course, I would need to change the code to account for such inputs, which‚Ä¶ I‚Äôm sorry, but I won‚Äôt do it for now. If someone some day shows a minimum of interest in the possibilities of the algorithm, I shall make sure I implement that.\nActually, I kinda know how to go about it. But there are many pieces of the algorithm, and functionally testing any change (in the functionality) is a slow process (in spite of having run several tests thus far‚Ä¶). Unit testing wouldn‚Äôt be so much of a problem.\n\n\n\nIt is true that ‚Äúonly‚Äù accepting binary input strings for the states (i.e.¬†the input data, in our context) is a difficulty, for sure.\nBut consider that: Neural Networks accept numerical input. And people use them. This is slightly different, but not really if you look at it a certain way.\nSo yes, I should work on a future version that will accept numerical input, not just binary strings. Sure. Some day. Future version is the key here, there is only so much I can cover for now :)"
  },
  {
    "objectID": "posts/2025-03-30_Small_Addition_RLCS/index.html#but-you-only-take-binary-strings-as-input",
    "href": "posts/2025-03-30_Small_Addition_RLCS/index.html#but-you-only-take-binary-strings-as-input",
    "title": "RLCS: Small addition",
    "section": "",
    "text": "I was in attending a local R conference the other day, and took the opportunity to discuss a bit with some of the (very few) that have a minimal interest (and an understanding) of what I‚Äôm trying to do these days with the RLCS project.\nIt‚Äôs always great to talk with them, they know so much, there is so much experience in there. And of course, that means, they can criticize my work. Which, from them, or anyone with criteria for that matter, I happily welcome!"
  },
  {
    "objectID": "posts/2025-03-30_Small_Addition_RLCS/index.html#a-short-term-partial-but-functional-solution",
    "href": "posts/2025-03-30_Small_Addition_RLCS/index.html#a-short-term-partial-but-functional-solution",
    "title": "RLCS: Small addition",
    "section": "",
    "text": "Alright, so far I‚Äôve always taken examples where I could translate the problems into binary strings for the input.\nFor images, I took each pixel and set it to 1 if the pixel was ‚Äúused‚Äù (say, a high shade of black), and 0 if it wasn‚Äôt (say, a light shade of grey). See https://kaizen-r.github.io/posts/2025-03-19_ExplainableAI/ for instance, for a visual intuition.\nOther examples were specifically chosen to be already presenting binary ‚Äústates‚Äù. That was the case with ‚Äúnot_bit_4‚Äù or ‚Äúmux_6‚Äù here.\nBut it is very common to have numerical variables as input, not‚Ä¶ Well, not simply binary ones.\nThat said, pause for a second: What is a binary string if not (often times) a numerical value, duly encoded? It‚Äôs quite straightforward, if you think about it. Say you want to use 4 bits, you can encode 16 values, say the natural numbers (0:15). So 0000 is 0, 0010 is 2, and 1111 is 15.\nBut let‚Äôs make a second pause: What if you have more values? Say 32 of them, from 0 to 31? And say, for the sake of example, that we want to limit the number of bits to be used still to 4 bits. Then we could decide to assume a loss of information, and if so, use 0000 as a state for actual values 0 and 1, and 1111 for actual values 30 and 31.\nWhy limit the number of bits, you ask? Well, because the way it is working currently, each added bit to the states increases the search space of the algorithm by a (multiplication) factor of 2. Which is to say, long binary strings as states are costly to process and depending on the problem, with lots of samples for training, for instance, would require a lot of time (Note: I‚Äôm not discussing here ‚Äúa lot of processing power‚Äù, because as it is, paralelising is complex, although not impossible, and with varying efficacy depending on the scenarios). With few samples, or clear separation, for instance, the limitation of forcing a sequential processing can be overcome with nicely chosen hyperparameters. But it‚Äôs not straightforward. Anyhow, back to the topic.\nPause number 3: What if we had real-valued variables. Each variable then fits within its own range. I hereby suggest we could ‚Äúbreak‚Äù it in buckets, so that each bucket can then in turn represent a subset of the chosen variable values, a range.\nHow to break that though? Choosing means kind of makes an assumption about the distribution (normality) of input variable. So for now, and without thinking much about it I prefer to use the median, and quartiles, thereby breaking the input in 4 buckets of similar (or almost identical) sizes. And then, why not, break each bucket in turn in 4 sub-buckets. Voil√†: 16 subsets of the one variable. To be represented each as a 4 bits binary string.\nAnd no, I haven‚Äôt given it much more thought. This is just a simplified approach, one which I do not believe is perfect (maybe considering the input distribution would be better), or even best for the LCS processing; one which my statistician friends will surely criticize the next time I meet them‚Ä¶ But the important thing is the conceptual approach: If you accept loss of information, there is nothing precluding you from encoding your real numbers input into binary strings!\nThereby, of course, making the algorithm more practical to use."
  },
  {
    "objectID": "posts/2025-03-30_Small_Addition_RLCS/index.html#the-tweak-gray-encoding",
    "href": "posts/2025-03-30_Small_Addition_RLCS/index.html#the-tweak-gray-encoding",
    "title": "RLCS: Small addition",
    "section": "",
    "text": "Final detail for today: While reading the book ‚ÄúIntroduction to Learning Classifier Systems‚Äù (mentioned in the first post on the RLCS topic), they make a point about the binary encoding.\n‚ÄúNormal binary‚Äù encoding of numbers is not always ‚Äúsequential‚Äù for consecutive numbers, in that the Hamming distance between 0011 and 0100 (3 and 4) is 2.\nThere are alternatives, and one that is proposed in the book is the Gray code. I won‚Äôt delve into the details, but suffice to say, I am not yet fully convinced it is a great approach, but there is some intuition to it that makes it probably better than traditional encoding.\nAnyhow, I make a very very simplistic implementation of all I have discussed thus far today for the (in-)famous iris dataset. (Infamous in that, it‚Äôs almost too common :D But then again, that‚Äôs what makes it a perfect example!)"
  },
  {
    "objectID": "posts/2025-03-30_Small_Addition_RLCS/index.html#the-iris-classifier",
    "href": "posts/2025-03-30_Small_Addition_RLCS/index.html#the-iris-classifier",
    "title": "RLCS: Small addition",
    "section": "",
    "text": "So the iris dataset presents 4 real-valued variables, and 3 classes. Think of 16 buckets per variable, using quartiles and sub-quartiles. And a Gray code.\nCan we make a Supervised Learning model using LCS? Sure we can!\nThis is what it looks like, with the state simply putting together 4 strings of 4 bits:\n\n\n\nencoding iris, binary buckets and grey\n\n\nAnd what about the result?\nWell, it‚Äôs a bit slow: 4‚Äô for training. There are considerations about this, but overall, consider searching for solutions in a 16bits search space, where on top of everything else, classes are overlapping in different dimensions‚Ä¶ But no excuse, and I haven‚Äôt continued tweaking hyperparameters for this particular example.\nAnyhow. And what of the quality of the resulting LCS as a supervised learning classifier?\nI take 80% of the samples for the training (119) samples, leaving only 30 samples for testing (it‚Äôs not a big dataset :D). And the results are like so:\n\n\n\nnot bad a classifier\n\n\nThat‚Äôs about it. Although‚Ä¶\nThen again, other options are proposed in the book‚Ä¶"
  },
  {
    "objectID": "posts/2025-03-30_Small_Addition_RLCS/index.html#could-we-accept-numerical-inputs",
    "href": "posts/2025-03-30_Small_Addition_RLCS/index.html#could-we-accept-numerical-inputs",
    "title": "RLCS: Small addition",
    "section": "",
    "text": "Well, in fact, yes. There are alternatives, whereby instead of 0 or 1, you can take real numbers. The way this works (at least the way others have approached it and documented it in the reference book on the topic already mentioned) is like so:\n\neach variable is ‚Äúcovered‚Äù separately by a wide or narrow numerical range\nCoverage (creating new classifiers) would set a range around the variable value. Said range would replace our current bits in the state (bit strings).\nupon calling the Genetic Algorithm, the mutation step can replace said ranges with either generalization (so, # as implemented otherwise), but also it could try to widen or narrow the range.\n\nThe above would allow to cover more or less ground per classifier, making them more or less specific. I feel this seems correct. But I do also believe the computational effort would potentially be higher (I haven‚Äôt thought it through either).\nAlso, we need to decide what ‚Äúnarrow‚Äù or ‚Äúwide‚Äù signifies for each variable, for instance.\nAnd of course, I would need to change the code to account for such inputs, which‚Ä¶ I‚Äôm sorry, but I won‚Äôt do it for now. If someone some day shows a minimum of interest in the possibilities of the algorithm, I shall make sure I implement that.\nActually, I kinda know how to go about it. But there are many pieces of the algorithm, and functionally testing any change (in the functionality) is a slow process (in spite of having run several tests thus far‚Ä¶). Unit testing wouldn‚Äôt be so much of a problem."
  },
  {
    "objectID": "posts/2025-03-30_Small_Addition_RLCS/index.html#conclusions",
    "href": "posts/2025-03-30_Small_Addition_RLCS/index.html#conclusions",
    "title": "RLCS: Small addition",
    "section": "",
    "text": "It is true that ‚Äúonly‚Äù accepting binary input strings for the states (i.e.¬†the input data, in our context) is a difficulty, for sure.\nBut consider that: Neural Networks accept numerical input. And people use them. This is slightly different, but not really if you look at it a certain way.\nSo yes, I should work on a future version that will accept numerical input, not just binary strings. Sure. Some day. Future version is the key here, there is only so much I can cover for now :)"
  },
  {
    "objectID": "posts/2025-04-13_About_Information_Decisions_RL/index.html",
    "href": "posts/2025-04-13_About_Information_Decisions_RL/index.html",
    "title": "Sunday Thoughts: About Strategy, Information, Decisions, Simulations, Reinforcement Learning & al.",
    "section": "",
    "text": "Yesterday I did nothing. Like‚Ä¶ Nothing. I considered it an active choice, and as such I wasn‚Äôt sad at the end of the day for not having ‚Äúproduced‚Äù anything‚Ä¶ And today I was motivated to look for something to do.\nBut what? Well, I have options for sure, but one of them was to choose a topic to read about, and I had that book about Strategy that ‚Äúcalled me‚Äù, half-started (maybe 1/4 read‚Ä¶). Of the different options I had, I chose that book.\nAnd while reading it, these ideas of Tolstoi (see ‚ÄúWar and Peace‚Äù) about some limits of the ‚Äúscience‚Äù (rather art) of Strategy, and how history might filter a few decisions as key, discarding many other things that have happened‚Ä¶\n\n\n\nBook Cover for ‚ÄúStrategy, a History‚Äù (see resources)\n\n\nBut let‚Äôs not get ahead of ourselves, as since this morning I have given it some thought‚Ä¶\n\n\n\nBinary things. Yes or No.¬†True or False. Option A or B. This is the simplest level of information encoding I guess.\nAnd so it goes for Decisions! After all, if you have to choose between 3 (competing) options, you can look at them as 3 different binary choices.\nAnd that ‚Äúsimile‚Äù I think is key. Binary is in the end the minimal level of encoding for information AND for decisions, and maybe then it kinda makes sense that information is hence just a support for decision.\nOr maybe not: some will argue randomness, intuition, etc. I don‚Äôt disagree. Chaos theory sorts of proves that point, that you can‚Äôt predict the future. Not absolutely anyway, not in the long run, in complex systems. And decisions are not only binary in nature: they are also about making a prediction, aren‚Äôt they? Otherwise‚Ä¶ They make no sense.\nAnd because of Chaos theory, maybe no one can make perfect decisions, Bayesian thinking would be limited, and then it‚Äôs all irrelevant‚Ä¶ Well, I don‚Äôt know, I suppose there is some truth to that, but I can‚Äôt do much about that, can I?\nAnd so I prefer to look at information-based decisions as imperfect. But not useless or pointless.\n(Incidentally, binary is the current encoding for the RLCS package input, but that is quite irrelevant to today‚Äôs discussion‚Ä¶)\n\n\n\nComing back to the discussion of Tolstoi‚Äôs perspective (I haven‚Äôt read Tolstoi myself), the book implies that he would consider that the impact of specific decisions by military leaders (say, Bonaparte, etc.) was in fact less important than history would have us think, and that the sum of individual decisions (of say the whole armies) in context are then at the very best influenced.\nThis reminded me of two ‚Äúideas‚Äù somehow. Maybe it‚Äôs a bit far-fetched‚Ä¶ But here goes:\n\nCybernetics. Now this is far from a perfect match here, I know. But in concept, originally, that was inspired by a captain that steers a boat, thereby pushes in one direction, but with the wind, waves and what-not, you can‚Äôt perfectly indicate an exact direction, so in that sense you have a lot of influence, yes, but there is some‚Ä¶ wiggle? uncertainty? And although I understand I am being fuzzy in my juxtaposition of these ideas here, well, it also agrees with the fact that although you can have an impact on the overall direction of things, no decision will here be perfect as you cannot have a perfect impact. Yes, you can build negative-feedback loops, constantly improving‚Ä¶ Anyway. Just crossed my mind, there.\nAnd the second idea, is Asimov‚Äôs ‚ÄúPsycho-History‚Äù. I‚Äôve always loved the original trilogy of the Foundations (not the TV adaptation‚Ä¶). The very concept of psycho-history is resounding when reading these concepts by Tolstoi as introduced above‚Ä¶ Again here, maybe steering is possible, but the ‚Äúmasses‚Äù have a sort of inertia to them‚Ä¶ A topic, an idea I always found interesting.\n\nWhat these things are saying, put together, is I guess this: Say in a large company. One boss can make ‚Äústrategic‚Äù decisions. But how much of that translates into a perfectly aligned (maybe more tactical) action? Does anyone know what such an action could/should be (and that‚Äôs the job of managers and employees, you could argue)? Would context accompany, at all?\nContext is of course taken into account in decision making. As it should be. But context evolves, and some talk about 5-year plans, and I would argue, that might not‚Ä¶ Work, as the context is so uncertain. (And there could be a counter-argument made that part of the context is not as ‚Äúchanging‚Äù, or that strategic plans need constant review, etc. All fair points :))\nWould the whole team of many people ever be perfectly aligned with said action or actions? I guess not, and the ideas of Chaos (in that even if deterministic, one cannot cover perfectly an initial situation in all its detail) and Complex Systems (that pieces influence pieces, and one would need to consider each piece, here an employee, in its own context, as well as the system at large) are supporting my view here. Do they (the whole team) actually need to be ‚Äúperfectly‚Äù aligned, one might wonder‚Ä¶?\nAlso, how much of the decision is important vs how much of the actual good-willingness of the underlying execution team (which is what appears to be what Tolstoi would have argued‚Ä¶)? What then is the relative importance of the understanding and alignment of the team?\nIn that context, how important is the decision itself?\nClearly, a wrong decision will make things worse, and a good decision will make things better, that‚Äôs not my point here :)\nThis is all obviously a bit ‚Äúout-there‚Äù and more of a thought exercise. But as I like the topic of decision making and strategy, well‚Ä¶\nAnd yes, I guess I‚Äôm opening more questions than I am answering‚Ä¶ My bad.\n\n\n\nSomewhat related is the topic of the decision making itself and information, and the topic of simulations. Actually above I kinda of made a point of the limits of any simulation. In that the reality is complex, has several levels of abstraction, and too much detail to ever be included in a simulation. Still‚Ä¶ Whenever I think of simulations these days, I think about Monte-Carlo, Reinforcement Learning, and the likes.\n\n\nThis one point has made my day, really. Reinforcement Learning is cool, no question asked, but here is a common derived conversation about RL: So what?\nThe discussion is about the applications of Reinforcement Learning. Well, I will today argue that beyond playing games, it can be considered in context of simulations for decision making. And that opens a lot of potential value for it‚Ä¶\nBack to the conversation, now‚Ä¶\n\n\n\n\nI‚Äôll admit, I‚Äôm a fan of the Ironman movies (and no, I‚Äôm not ashamed). But I mostly like the ‚ÄúJarvis‚Äù part of it. That helpful AI program that helps Tony Stark. Or other simulations that Tony runs to create something‚Ä¶\n\n\n\nrunning simulations to make decisions, Ironman‚Äôs way\n\n\nIn several scenes of different movies in the series of Ironman or the Avengers, Tony launches a simulation of something (say of different ways of programming something, compiling it, and seeing whether the resulting program would successfully fulfill a given purpose, I guess).\nWith all the considerations above about the limits of decision-making, their actual impact, the importance of chaos theory, impossibility of considering future changing-context, the role of the execution, alignment, etc‚Ä¶ With all that in mind‚Ä¶\nI‚Äôve run in the past simulations of traffic in (medium) cities, which helped identify bottlenecks for different traffic scenarios. I‚Äôve run Monte-Carlo simulations of infectious processes on small networks, to decide better on which nodes to act. I‚Äôve been playing with Reinforcement Learning, whereby an agent takes more or less appropriate decisions with (limited) contextual information.\nAnd although maybe this has all been mostly theoretical, I do believe that, within (today‚Äôs) said limits of decision making, that approach makes a lot of sense.\nSo yes, I think RL has a place, maybe, beyond playing Chess and Go‚Ä¶ And maybe if one can setup a valid ‚Äúsimulation‚Äù of a scenario, then RL could be one approach to test different options and see what a computer could come up with.\nBy the way, this is also what Meta-heuristics is all about (including, obviously, Genetic Algorithms, which my readers will know I like :)). And Operations Research (albeit slightly different in approach, or sure).\nI also believe that the ideas of ‚Äúdigital twins‚Äù for instance in Health Care decision support are interesting, in that sense.\nAll in all, I guess this point is that one can support (ever imperfect) decision making with simulations.\n\n\n\nAnd well, that‚Äôs the gist of my thoughts on the topic today.\nI know, I didn‚Äôt invent fire today. It would seem, I rarely have an actually original thought‚Ä¶ Apologies here. The best I can do, it appears, is agreeing or disagreeing with some view, and then changing my mind with everything I learn along the way‚Ä¶ Oh well.\nThen again, hopefully me connecting different ideas is enough of a ‚Äúvalue-add‚Äù. I read somewhere that this connecting the dots is, in fact, a way of ‚Äúinnovating‚Äù.\nI do hope these thoughts are not completely crazy and that they inspire some more thoughts by others‚Ä¶ That would be great.\nAnd that would justify writing about it in the first place, beyond the fact that writing helps me think‚Ä¶\n\n\n\nStrategy, A History, by L. Freedman"
  },
  {
    "objectID": "posts/2025-04-13_About_Information_Decisions_RL/index.html#random-sunday-thoughts",
    "href": "posts/2025-04-13_About_Information_Decisions_RL/index.html#random-sunday-thoughts",
    "title": "Sunday Thoughts: About Strategy, Information, Decisions, Simulations, Reinforcement Learning & al.",
    "section": "",
    "text": "Yesterday I did nothing. Like‚Ä¶ Nothing. I considered it an active choice, and as such I wasn‚Äôt sad at the end of the day for not having ‚Äúproduced‚Äù anything‚Ä¶ And today I was motivated to look for something to do.\nBut what? Well, I have options for sure, but one of them was to choose a topic to read about, and I had that book about Strategy that ‚Äúcalled me‚Äù, half-started (maybe 1/4 read‚Ä¶). Of the different options I had, I chose that book.\nAnd while reading it, these ideas of Tolstoi (see ‚ÄúWar and Peace‚Äù) about some limits of the ‚Äúscience‚Äù (rather art) of Strategy, and how history might filter a few decisions as key, discarding many other things that have happened‚Ä¶\n\n\n\nBook Cover for ‚ÄúStrategy, a History‚Äù (see resources)\n\n\nBut let‚Äôs not get ahead of ourselves, as since this morning I have given it some thought‚Ä¶"
  },
  {
    "objectID": "posts/2025-04-13_About_Information_Decisions_RL/index.html#most-simplistic-decisions-information",
    "href": "posts/2025-04-13_About_Information_Decisions_RL/index.html#most-simplistic-decisions-information",
    "title": "Sunday Thoughts: About Strategy, Information, Decisions, Simulations, Reinforcement Learning & al.",
    "section": "",
    "text": "Binary things. Yes or No.¬†True or False. Option A or B. This is the simplest level of information encoding I guess.\nAnd so it goes for Decisions! After all, if you have to choose between 3 (competing) options, you can look at them as 3 different binary choices.\nAnd that ‚Äúsimile‚Äù I think is key. Binary is in the end the minimal level of encoding for information AND for decisions, and maybe then it kinda makes sense that information is hence just a support for decision.\nOr maybe not: some will argue randomness, intuition, etc. I don‚Äôt disagree. Chaos theory sorts of proves that point, that you can‚Äôt predict the future. Not absolutely anyway, not in the long run, in complex systems. And decisions are not only binary in nature: they are also about making a prediction, aren‚Äôt they? Otherwise‚Ä¶ They make no sense.\nAnd because of Chaos theory, maybe no one can make perfect decisions, Bayesian thinking would be limited, and then it‚Äôs all irrelevant‚Ä¶ Well, I don‚Äôt know, I suppose there is some truth to that, but I can‚Äôt do much about that, can I?\nAnd so I prefer to look at information-based decisions as imperfect. But not useless or pointless.\n(Incidentally, binary is the current encoding for the RLCS package input, but that is quite irrelevant to today‚Äôs discussion‚Ä¶)"
  },
  {
    "objectID": "posts/2025-04-13_About_Information_Decisions_RL/index.html#about-limits-of-strategy",
    "href": "posts/2025-04-13_About_Information_Decisions_RL/index.html#about-limits-of-strategy",
    "title": "Sunday Thoughts: About Strategy, Information, Decisions, Simulations, Reinforcement Learning & al.",
    "section": "",
    "text": "Coming back to the discussion of Tolstoi‚Äôs perspective (I haven‚Äôt read Tolstoi myself), the book implies that he would consider that the impact of specific decisions by military leaders (say, Bonaparte, etc.) was in fact less important than history would have us think, and that the sum of individual decisions (of say the whole armies) in context are then at the very best influenced.\nThis reminded me of two ‚Äúideas‚Äù somehow. Maybe it‚Äôs a bit far-fetched‚Ä¶ But here goes:\n\nCybernetics. Now this is far from a perfect match here, I know. But in concept, originally, that was inspired by a captain that steers a boat, thereby pushes in one direction, but with the wind, waves and what-not, you can‚Äôt perfectly indicate an exact direction, so in that sense you have a lot of influence, yes, but there is some‚Ä¶ wiggle? uncertainty? And although I understand I am being fuzzy in my juxtaposition of these ideas here, well, it also agrees with the fact that although you can have an impact on the overall direction of things, no decision will here be perfect as you cannot have a perfect impact. Yes, you can build negative-feedback loops, constantly improving‚Ä¶ Anyway. Just crossed my mind, there.\nAnd the second idea, is Asimov‚Äôs ‚ÄúPsycho-History‚Äù. I‚Äôve always loved the original trilogy of the Foundations (not the TV adaptation‚Ä¶). The very concept of psycho-history is resounding when reading these concepts by Tolstoi as introduced above‚Ä¶ Again here, maybe steering is possible, but the ‚Äúmasses‚Äù have a sort of inertia to them‚Ä¶ A topic, an idea I always found interesting.\n\nWhat these things are saying, put together, is I guess this: Say in a large company. One boss can make ‚Äústrategic‚Äù decisions. But how much of that translates into a perfectly aligned (maybe more tactical) action? Does anyone know what such an action could/should be (and that‚Äôs the job of managers and employees, you could argue)? Would context accompany, at all?\nContext is of course taken into account in decision making. As it should be. But context evolves, and some talk about 5-year plans, and I would argue, that might not‚Ä¶ Work, as the context is so uncertain. (And there could be a counter-argument made that part of the context is not as ‚Äúchanging‚Äù, or that strategic plans need constant review, etc. All fair points :))\nWould the whole team of many people ever be perfectly aligned with said action or actions? I guess not, and the ideas of Chaos (in that even if deterministic, one cannot cover perfectly an initial situation in all its detail) and Complex Systems (that pieces influence pieces, and one would need to consider each piece, here an employee, in its own context, as well as the system at large) are supporting my view here. Do they (the whole team) actually need to be ‚Äúperfectly‚Äù aligned, one might wonder‚Ä¶?\nAlso, how much of the decision is important vs how much of the actual good-willingness of the underlying execution team (which is what appears to be what Tolstoi would have argued‚Ä¶)? What then is the relative importance of the understanding and alignment of the team?\nIn that context, how important is the decision itself?\nClearly, a wrong decision will make things worse, and a good decision will make things better, that‚Äôs not my point here :)\nThis is all obviously a bit ‚Äúout-there‚Äù and more of a thought exercise. But as I like the topic of decision making and strategy, well‚Ä¶\nAnd yes, I guess I‚Äôm opening more questions than I am answering‚Ä¶ My bad."
  },
  {
    "objectID": "posts/2025-04-13_About_Information_Decisions_RL/index.html#ironmans-approach-i-feel-is-not-wrong",
    "href": "posts/2025-04-13_About_Information_Decisions_RL/index.html#ironmans-approach-i-feel-is-not-wrong",
    "title": "Sunday Thoughts: About Strategy, Information, Decisions, Simulations, Reinforcement Learning & al.",
    "section": "",
    "text": "Somewhat related is the topic of the decision making itself and information, and the topic of simulations. Actually above I kinda of made a point of the limits of any simulation. In that the reality is complex, has several levels of abstraction, and too much detail to ever be included in a simulation. Still‚Ä¶ Whenever I think of simulations these days, I think about Monte-Carlo, Reinforcement Learning, and the likes.\n\n\nThis one point has made my day, really. Reinforcement Learning is cool, no question asked, but here is a common derived conversation about RL: So what?\nThe discussion is about the applications of Reinforcement Learning. Well, I will today argue that beyond playing games, it can be considered in context of simulations for decision making. And that opens a lot of potential value for it‚Ä¶\nBack to the conversation, now‚Ä¶"
  },
  {
    "objectID": "posts/2025-04-13_About_Information_Decisions_RL/index.html#so-simulations",
    "href": "posts/2025-04-13_About_Information_Decisions_RL/index.html#so-simulations",
    "title": "Sunday Thoughts: About Strategy, Information, Decisions, Simulations, Reinforcement Learning & al.",
    "section": "",
    "text": "I‚Äôll admit, I‚Äôm a fan of the Ironman movies (and no, I‚Äôm not ashamed). But I mostly like the ‚ÄúJarvis‚Äù part of it. That helpful AI program that helps Tony Stark. Or other simulations that Tony runs to create something‚Ä¶\n\n\n\nrunning simulations to make decisions, Ironman‚Äôs way\n\n\nIn several scenes of different movies in the series of Ironman or the Avengers, Tony launches a simulation of something (say of different ways of programming something, compiling it, and seeing whether the resulting program would successfully fulfill a given purpose, I guess).\nWith all the considerations above about the limits of decision-making, their actual impact, the importance of chaos theory, impossibility of considering future changing-context, the role of the execution, alignment, etc‚Ä¶ With all that in mind‚Ä¶\nI‚Äôve run in the past simulations of traffic in (medium) cities, which helped identify bottlenecks for different traffic scenarios. I‚Äôve run Monte-Carlo simulations of infectious processes on small networks, to decide better on which nodes to act. I‚Äôve been playing with Reinforcement Learning, whereby an agent takes more or less appropriate decisions with (limited) contextual information.\nAnd although maybe this has all been mostly theoretical, I do believe that, within (today‚Äôs) said limits of decision making, that approach makes a lot of sense.\nSo yes, I think RL has a place, maybe, beyond playing Chess and Go‚Ä¶ And maybe if one can setup a valid ‚Äúsimulation‚Äù of a scenario, then RL could be one approach to test different options and see what a computer could come up with.\nBy the way, this is also what Meta-heuristics is all about (including, obviously, Genetic Algorithms, which my readers will know I like :)). And Operations Research (albeit slightly different in approach, or sure).\nI also believe that the ideas of ‚Äúdigital twins‚Äù for instance in Health Care decision support are interesting, in that sense.\nAll in all, I guess this point is that one can support (ever imperfect) decision making with simulations."
  },
  {
    "objectID": "posts/2025-04-13_About_Information_Decisions_RL/index.html#conclusions",
    "href": "posts/2025-04-13_About_Information_Decisions_RL/index.html#conclusions",
    "title": "Sunday Thoughts: About Strategy, Information, Decisions, Simulations, Reinforcement Learning & al.",
    "section": "",
    "text": "And well, that‚Äôs the gist of my thoughts on the topic today.\nI know, I didn‚Äôt invent fire today. It would seem, I rarely have an actually original thought‚Ä¶ Apologies here. The best I can do, it appears, is agreeing or disagreeing with some view, and then changing my mind with everything I learn along the way‚Ä¶ Oh well.\nThen again, hopefully me connecting different ideas is enough of a ‚Äúvalue-add‚Äù. I read somewhere that this connecting the dots is, in fact, a way of ‚Äúinnovating‚Äù.\nI do hope these thoughts are not completely crazy and that they inspire some more thoughts by others‚Ä¶ That would be great.\nAnd that would justify writing about it in the first place, beyond the fact that writing helps me think‚Ä¶"
  },
  {
    "objectID": "posts/2025-04-13_About_Information_Decisions_RL/index.html#resources",
    "href": "posts/2025-04-13_About_Information_Decisions_RL/index.html#resources",
    "title": "Sunday Thoughts: About Strategy, Information, Decisions, Simulations, Reinforcement Learning & al.",
    "section": "",
    "text": "Strategy, A History, by L. Freedman"
  },
  {
    "objectID": "posts/2025-04-24_RLCS_Code_Rework/index.html",
    "href": "posts/2025-04-24_RLCS_Code_Rework/index.html",
    "title": "RLCS: Better code?",
    "section": "",
    "text": "Most of the code thus far was about getting it to work, and for it to not be too slow at runtime (with several approaches considered).\nAnd although I do try to not write pure garbage code, well, sometimes I‚Äôm not happy with the results.\nI rescued the (mostly yet unread) books I have from ‚ÄúUncle Bob‚Äù (see References) to get renewed inspiration on that topic."
  },
  {
    "objectID": "posts/2025-04-24_RLCS_Code_Rework/index.html#im-worried-about-my-code-being-too-ugly",
    "href": "posts/2025-04-24_RLCS_Code_Rework/index.html#im-worried-about-my-code-being-too-ugly",
    "title": "RLCS: Better code?",
    "section": "",
    "text": "Most of the code thus far was about getting it to work, and for it to not be too slow at runtime (with several approaches considered).\nAnd although I do try to not write pure garbage code, well, sometimes I‚Äôm not happy with the results.\nI rescued the (mostly yet unread) books I have from ‚ÄúUncle Bob‚Äù (see References) to get renewed inspiration on that topic."
  },
  {
    "objectID": "posts/2025-04-24_RLCS_Code_Rework/index.html#monadic-diadic-functions",
    "href": "posts/2025-04-24_RLCS_Code_Rework/index.html#monadic-diadic-functions",
    "title": "RLCS: Better code?",
    "section": "Monadic & Diadic functions",
    "text": "Monadic & Diadic functions\nWell, I understand the concept, but‚Ä¶ How do you pass one variable when you need to pass several hyperparameters for the algorithm to work?\nI‚Äôm not sure this is correct, but I just decided to move the hyperparameters into an object (which, you know, can be a simple list underneath). That will make the calls cleaner for sure."
  },
  {
    "objectID": "posts/2025-04-24_RLCS_Code_Rework/index.html#very-similar-functions",
    "href": "posts/2025-04-24_RLCS_Code_Rework/index.html#very-similar-functions",
    "title": "RLCS: Better code?",
    "section": "Very similar functions",
    "text": "Very similar functions\nI have several functions that do almost exactly the same thing, just‚Ä¶ They act on a different parameter.\nThings like ‚Äúincrease_match_count()‚Äù, ‚Äúincrease_correct_count()‚Äù and ‚Äúincrease_action_count()‚Äù, as you can probably intuit, are basically the very same function.\nSo I‚Äôm thinking about not re-writing the complete functions (although, all of them are very very simple, in that they just lapply() and +1 a given parameter for a given population), and overload them, using a function factory (as described in ‚ÄúAdvanced R‚Äù). It‚Äôs probably best to do it that way, but I‚Äôll think about it, because if I need to expose said functions, then they‚Äôd have to have their own .R file each, and then it‚Äôs less self-explanatory‚Ä¶"
  },
  {
    "objectID": "posts/2025-04-24_RLCS_Code_Rework/index.html#rl-is-im-still-not-doing-a-package-really.",
    "href": "posts/2025-04-24_RLCS_Code_Rework/index.html#rl-is-im-still-not-doing-a-package-really.",
    "title": "RLCS: Better code?",
    "section": "RL is‚Ä¶ I‚Äôm still not doing a package, really.",
    "text": "RL is‚Ä¶ I‚Äôm still not doing a package, really.\nI‚Äôm facing an issue in concept: How do you package a Reinforcement Learning Algorithm?\nI don‚Äôt see two ways around it, you must make assumptions about the environment for your agent(s).\nSo say the environment will be passed as a parameter.\nYou have no choice but to interact with it, and so it simply can‚Äôt expose just any interfaces. Things like ‚Äúagent_calls_action(agent, action)‚Äù, or ‚Äúget_action_score(agent, action)‚Äù (that will always return a valid (say, numeric) value) must be somehow standardized.\nMaybe you can require the environment object to answer correctly to ‚Äúlist_available_actions()‚Äù, but then, you need to probably ensure that these actions are all well controlled in different states, so that they are all legal actions whenever the agent calls them‚Ä¶ Or instead you might want to be able to call ‚Äúlist_legal_actions(agent)‚Äù out of your environment object at each stage‚Ä¶\nAnd so in ‚Äúwrapping‚Äù the RLCS code to make it into a package, I have to make assumptions about valid worlds/environment being provided, for RL.\nFor supervised learning, that‚Äôs not an issue that much, although in the current state of affairs, you will need a specific environment. BUT the difference is, you can test the complete environment for valid states and actions upon first call, and before you proceed with the algorithm runs, so ideally you won‚Äôt run into invalid stuff upon working with the package in the future."
  },
  {
    "objectID": "posts/2025-04-24_RLCS_Code_Rework/index.html#parallel-processing-or-not",
    "href": "posts/2025-04-24_RLCS_Code_Rework/index.html#parallel-processing-or-not",
    "title": "RLCS: Better code?",
    "section": "Parallel processing or not?",
    "text": "Parallel processing or not?\nOne other key thing is processing time. We‚Äôve seen I have tried different calls to parallelize the thing and see what made sense, when. More or less.\nOne rather big issue is that the LCS algorithm is very sequential in nature. And so for large environments (i.e.¬†many states) or long (strings) states (i.e.¬†big search space), it can be slow, and there are not too many ways around it. But I have shown there are possibilities to consider. And I could make that part of the package itself, really.\nOne drawback however is that I have made some effort to limit as much as possible dependencies.\nI use plyr::rbind.fill(), and for the package code itself, I think that‚Äôs basically all of it. No other required library/function I haven‚Äôt coded myself.\n(For World Plotting or Classifier System Visuals, yes, maybe, but I‚Äôm considering leaving that as supplementary code, not part of the package, although I will say, visuals are important to make the thing more attractive‚Ä¶)\nRegardless, even with uncommon plotting functions, maybe I require (directly) 3 packages overall? And nothing such as ‚Äútidyverse‚Äù or other big metapackages.\nThat‚Äôs very much intentional (again, efficiency was a consideration, and I prefer smaller packages with shorter lists of dependencies‚Ä¶). Also, it makes maintainability better, surely (renv() or not).\nBut that‚Äôs a balancing exercise, isn‚Äôt it? And adding support for parallel processing is one set of potentially rather big dependencies. Plus, making sure that works cross-platforms‚Ä¶"
  },
  {
    "objectID": "posts/2025-04-24_RLCS_Code_Rework/index.html#conclusions",
    "href": "posts/2025-04-24_RLCS_Code_Rework/index.html#conclusions",
    "title": "RLCS: Better code?",
    "section": "Conclusions",
    "text": "Conclusions\nThese and other similar considerations are in my head these days. Although I will also say, I‚Äôm taking it slow. The whole thing works, and it‚Äôs not awfully slow (considering what it could be).\nBut should the code be ‚Äúbetter looking‚Äù‚Ä¶\n\nI have next to no comments, for instance. Mainly because I try to use functions with self-explanatory names.\nI have done plenty of testing, for each function, but‚Ä¶ I haven‚Äôt done quite exactly TDD, namely I haven‚Äôt written down tests for each function in a neat file or set of test files‚Ä¶\nOther ideas, like functional approaches: There are still plenty of looping things happening. Should I use recursion, with TCO? I haven‚Äôt looked much into TCO in R, for instance, but if it worked nicely (i.e.¬†fast enough), maybe it would make sense for at least some of my funcions? Or does it? My experience with recursion in R is very bad, but I wasn‚Äôt aware of the TCO option. Memoizing could be another option, but there is quite a bit of stochastic stuff happening with this algorithm, and so that might not be a good idea at all.\nNot to mention, I haven‚Äôt even looked into RCpp for this package just yet. It kinda feels unnecessary at this stage but‚Ä¶ I also don‚Äôt have a comparison point, so maybe I‚Äôm loosing an opportunity to have things improving greatly right there? After all, I use vectorized operations (lapply() and al.) a lot, but maybe an RCpp loop of loops would run even faster? I haven‚Äôt tried it here.\n\nSo much to do still‚Ä¶\nAlso, I want to make a page dedicated to documenting this code/package, so that the algorithm can be understood, and presented. I‚Äôve pointed to great resources in the past to understand the algorithm. I just want to make something to explain the algorithm, with my own implementation. But that‚Äôs a different topic, I guess."
  },
  {
    "objectID": "posts/2025-04-24_RLCS_Code_Rework/index.html#resources",
    "href": "posts/2025-04-24_RLCS_Code_Rework/index.html#resources",
    "title": "RLCS: Better code?",
    "section": "Resources",
    "text": "Resources\nClean Code, by R. C. Martin (‚ÄúUncle Bob‚Äù)\nFunctional Design, by R. C. Martin (‚ÄúUncle Bob‚Äù)\nAdvanced R, by H. Wickham"
  },
  {
    "objectID": "posts/2025-04-24_RLCS_Code_Rework/index.html#monadic-dyadic-functions",
    "href": "posts/2025-04-24_RLCS_Code_Rework/index.html#monadic-dyadic-functions",
    "title": "RLCS: Better code?",
    "section": "Monadic & Dyadic functions",
    "text": "Monadic & Dyadic functions\nWell, I understand the concept, but‚Ä¶ How do you pass one variable when you need to pass several hyperparameters for the algorithm to work?\nI‚Äôm sure this is not a correct solution, but I just decided to simply move the hyperparameters into an object (which, you know, can be a simple list underneath). That will make the calls cleaner for sure."
  },
  {
    "objectID": "posts/2025-04-24_RLCS_Code_Rework/index.html#rl-is-im-still-not-doing-a-package-really",
    "href": "posts/2025-04-24_RLCS_Code_Rework/index.html#rl-is-im-still-not-doing-a-package-really",
    "title": "RLCS: Better code?",
    "section": "RL is‚Ä¶ I‚Äôm still not doing a package, really‚Ä¶",
    "text": "RL is‚Ä¶ I‚Äôm still not doing a package, really‚Ä¶\nI‚Äôm facing an issue in concept: How do you package a Reinforcement Learning Algorithm?\nI don‚Äôt see two ways around it, you must make assumptions about the environment for your agent(s).\nSo say the environment will be passed as a parameter.\nYou have no choice but to interact with it, and so it simply can‚Äôt expose just any interfaces. Things like ‚Äúagent_calls_action(agent, action)‚Äù, or ‚Äúget_action_score(agent, action)‚Äù (that will always return a valid (say, numeric) value) must be somehow standardized.\nMaybe you can require the environment object to answer correctly to ‚Äúlist_available_actions()‚Äù, but then, you need to probably ensure that these actions are all well controlled in different states, so that they are all legal actions whenever the agent calls them‚Ä¶ Or instead you might want to be able to call ‚Äúlist_legal_actions(agent)‚Äù out of your environment object at each stage‚Ä¶\nAnd so in ‚Äúwrapping‚Äù the RLCS code to make it into a package, I have to make assumptions about valid worlds/environment being provided, for RL.\nFor supervised learning, it not that much of an issue, although in the current state of affairs, you will need a specific environment. BUT the key difference is, you can test the complete environment for valid states and actions upon first call, and before you proceed with the algorithm run(s), so ideally you won‚Äôt run into invalid stuff upon working with the package in the future."
  },
  {
    "objectID": "posts/2025-05-02_Slight_code_rewrites/index.html",
    "href": "posts/2025-05-02_Slight_code_rewrites/index.html",
    "title": "RLCS: small code rewrites",
    "section": "",
    "text": "As announced, I‚Äôm doing some code rewriting, just because‚Ä¶"
  },
  {
    "objectID": "posts/2025-05-02_Slight_code_rewrites/index.html#small-updates-of-the-code",
    "href": "posts/2025-05-02_Slight_code_rewrites/index.html#small-updates-of-the-code",
    "title": "RLCS: small code rewrites",
    "section": "",
    "text": "As announced, I‚Äôm doing some code rewriting, just because‚Ä¶"
  },
  {
    "objectID": "posts/2025-05-02_Slight_code_rewrites/index.html#function-factory",
    "href": "posts/2025-05-02_Slight_code_rewrites/index.html#function-factory",
    "title": "RLCS: small code rewrites",
    "section": "Function Factory",
    "text": "Function Factory\nSo I had 2 or three of these:\ninc_match_count &lt;- function(M_pop) { ## All versions\n  lapply(M_pop, \\(x) {\n    x$match_count &lt;- x$match_count + 1\n    x\n  })\n}\n\ninc_correct_count &lt;- function(C_pop) { ## SL Specific\n  lapply(C_pop, \\(x) {\n    x$correct_count &lt;- x$correct_count + 1\n    x\n  })\n}\n\ninc_action_count &lt;- function(A_pop) { ## RL Specific\n  lapply(A_pop, \\(x) {\n    x$action_count &lt;- x$action_count + 1\n    x\n  })\n}\nNow it looks like so:\n## Function factory to increase parameter counts\ninc_param_count &lt;- function(param) {\n  param &lt;- as.name(param)\n  function(pop) {\n    lapply(pop, \\(x) {\n      x[[param]] &lt;- x[[param]] + 1\n      x\n    })\n  }\n}\n\ninc_match_count &lt;- inc_param_count(\"match_count\")\ninc_correct_count &lt;- inc_param_count(\"correct_count\")\ninc_action_count &lt;- inc_param_count(\"action_count\")\nAs I do not intend to expose these functions in the future ‚Äúpackaged‚Äù version, well, this is pretty safe."
  },
  {
    "objectID": "posts/2025-05-02_Slight_code_rewrites/index.html#object-for-hyperparameters",
    "href": "posts/2025-05-02_Slight_code_rewrites/index.html#object-for-hyperparameters",
    "title": "RLCS: small code rewrites",
    "section": "Object for Hyperparameters",
    "text": "Object for Hyperparameters\nAnd as also announced, I cleaned up a bit the code to ‚Äúcall training‚Äù. Now using an object for the hyperparameters of the algorithm, which has its own defaults, makes things a bit cleaner.\nsource(\"run_params/datamining_examples_recommended_hyperparameters_v001.R\")\n\nbasic_hyperparameters &lt;- RLCS_hyperparameters(\n  wildcard_prob = wildcard_prob,\n  rd_trigger = rd_trigger,\n  mutation_probability = mutation_probability,\n  parents_selection_mode = parents_selection_mode,\n  tournament_pressure = tournament_pressure,\n  n_epochs = n_epochs,\n  deletion_trigger = deletion_trigger,\n  deletion_threshold = deletion_threshold\n)\n\n## It makes it more readable here:\nexample_lcs &lt;- rlcs_train(train_environment, basic_hyperparameters)\nAnd given the number of parameters, it‚Äôs even more readable of course if you take the defaults:\ndefault_lcs_hyperparameters &lt;- RLCS_hyperparameters()\nexample_lcs &lt;- rlcs_train(train_environment, default_lcs_hyperparameters)\nUsing an object might also help with adding error controls‚Ä¶ Probably, that‚Äôs not implemented yet. Also, it‚Äôs a valid approach to clean code, as these variables are all part of the same concept of hyper-parameters for the algorithm, meaning they do belong together.\nAnd as a note here, ‚Äúdefaults‚Äù are seldom all good for an LCS, so you should probably overwrite some of the parameters depending on each problem you look at‚Ä¶\nFor comparison, I saved a version of an old call, and it‚Äôs just that it received too many parameters:\nlcs_res &lt;- rlcs_meta_train(train_environment,\n                           1, ## Warmup with just one epoch\n                           wildcard_prob,\n                           rd_trigger,\n                           parents_selection_mode,\n                           mutation_probability,\n                           tournament_pressure,\n                           deletion_trigger) ## Deletion won't be triggered"
  },
  {
    "objectID": "posts/2025-05-02_Slight_code_rewrites/index.html#resources",
    "href": "posts/2025-05-02_Slight_code_rewrites/index.html#resources",
    "title": "RLCS: small code rewrites",
    "section": "Resources",
    "text": "Resources\nClean Code, by R. C. Martin (‚ÄúUncle Bob‚Äù)\nAdvanced R, by H. Wickham"
  },
  {
    "objectID": "others/RLCS_documentation.html#the-weirdest-thing-happened",
    "href": "others/RLCS_documentation.html#the-weirdest-thing-happened",
    "title": "RLCS: A documentation",
    "section": "The weirdest thing happened",
    "text": "The weirdest thing happened\nHave you ever found something that no one else has done?\nI found such a thing last November 2024. But let‚Äôs go back for a minute."
  },
  {
    "objectID": "others/RLCS_documentation.html#an-issue-with-ai-explainability",
    "href": "others/RLCS_documentation.html#an-issue-with-ai-explainability",
    "title": "RLCS: An Introduction",
    "section": "An issue with ‚ÄúAI‚Äù: explainability",
    "text": "An issue with ‚ÄúAI‚Äù: explainability\nLet‚Äôs call it Machine Learning. Today that‚Äôs mostly Neural networks. And mostly, that means it‚Äôs all black boxes.\nThere are ways around that. (e.g.¬†Trees and other ‚Äúopen book‚Äù algorithms‚Ä¶)\n\nJohn H. Holland proposed an algorithm with the Cognitive System One program (1976). Later, people came up with variations‚Ä¶ Today we focus on Michigan-style LCS.\n\nActually‚Ä¶ That‚Äôs it. No time to dive deeper here.\nToday, we‚Äôll be discussing one such explainable Machine Learning algorithm."
  },
  {
    "objectID": "others/RLCS_documentation.html#a-bit-of-history-john-h.-hollands-ideas",
    "href": "others/RLCS_documentation.html#a-bit-of-history-john-h.-hollands-ideas",
    "title": "RLCS: An Introduction",
    "section": "A bit of history: John H. Holland‚Äôs Ideas",
    "text": "A bit of history: John H. Holland‚Äôs Ideas\n\nHolland is behind the ideas of Genetic Algorithms.\n\nHe proposed an algorithm with the Cognitive System One program (1976). Later, people came up with variations‚Ä¶\nSo today we focus on Michigan-style LCS.\n\nActually‚Ä¶ That‚Äôs it. No time to dive deeper here."
  },
  {
    "objectID": "others/RLCS_documentation.html#why-nobody-has-done-it-yet",
    "href": "others/RLCS_documentation.html#why-nobody-has-done-it-yet",
    "title": "RLCS: An Introduction",
    "section": "Why nobody has done it yet?",
    "text": "Why nobody has done it yet?\nIt‚Äôs not fast\n\n\nThere are many sequential steps, rather unavoidable ones at that\nNot ideal to compete with a world of GPUs and parallel processing (yet ;))\n\n\n\nIt‚Äôs ‚Äúcomplex‚Äù\n\n\nOr so does the Wikipedia entry say‚Ä¶\nWhen it comes to ‚Äúalphabets‚Äù, it does get messy, I‚Äôll admit"
  },
  {
    "objectID": "others/RLCS_documentation.html#the-goal-a-new-r-package",
    "href": "others/RLCS_documentation.html#the-goal-a-new-r-package",
    "title": "RLCS: An Introduction",
    "section": "The goal: A new R package",
    "text": "The goal: A new R package\n\n\nHave you ever found something that no one else has done?\n\nI found such a thing last November 2024. But let‚Äôs go back for a minute."
  },
  {
    "objectID": "others/RLCS_documentation.html#the-goal-a-new-r-package-1",
    "href": "others/RLCS_documentation.html#the-goal-a-new-r-package-1",
    "title": "RLCS: An Introduction",
    "section": "The goal: A new R package",
    "text": "The goal: A new R package\nA package to implement a simple version of the Learning Classifier System algorithm:\n\nBinary Alphabet, tournament/one-point crossover GA, accuracy based, Michigan-style LCS\nWith examples, demonstrating the implementation for:\n\nData Mining\nSupervised Learning\nReinforcement Learning"
  },
  {
    "objectID": "others/RLCS_documentation.html#some-code",
    "href": "others/RLCS_documentation.html#some-code",
    "title": "RLCS: A story",
    "section": "Some code",
    "text": "Some code\nHow did I approach the thing?\n\n\nLists. Lists, everywhere.\nlapply() is then my best friend\nStart small, grow fast (because I get obsessed)\nthen clean it\nthen clean it some more\never postponing the move to a Package, though"
  },
  {
    "objectID": "others/RLCS_documentation.html#examples",
    "href": "others/RLCS_documentation.html#examples",
    "title": "RLCS: An Introduction",
    "section": "Examples",
    "text": "Examples\nBefore\nlcs_res &lt;- rlcs_meta_train(train_environment,\n                           1, ## Warmup with just one epoch\n                           wildcard_prob,\n                           rd_trigger,\n                           parents_selection_mode,\n                           mutation_probability,\n                           tournament_pressure,\n                           deletion_trigger) ## Deletion won't be triggered\n\nToo many parameters! (Uncle Bob wouldn‚Äôt like it)"
  },
  {
    "objectID": "others/RLCS_documentation.html#examples-1",
    "href": "others/RLCS_documentation.html#examples-1",
    "title": "RLCS: An Introduction",
    "section": "Examples",
    "text": "Examples\nAfter, using an object (reference class, ‚ÄúR5‚Äù, in this case)\ndefault_lcs_hyperparameters &lt;- RLCS_hyperparameters()\nexample_lcs &lt;- rlcs_train(train_environment, default_lcs_hyperparameters)"
  },
  {
    "objectID": "others/RLCS_documentation.html#demo-time",
    "href": "others/RLCS_documentation.html#demo-time",
    "title": "RLCS: A story",
    "section": "Demo Time",
    "text": "Demo Time\nData Mining Scenario\n\nA warning, the algorithm is non-deterministic‚Ä¶\n\n\nTo be clear: I have used that on a Corporate CMDB (IT Inventory) and it helped!"
  },
  {
    "objectID": "others/RLCS_documentation.html#demo-time-1",
    "href": "others/RLCS_documentation.html#demo-time-1",
    "title": "RLCS: A story",
    "section": "Demo Time",
    "text": "Demo Time\nExpected result:"
  },
  {
    "objectID": "others/RLCS_documentation.html#examples-2",
    "href": "others/RLCS_documentation.html#examples-2",
    "title": "RLCS: An Introduction",
    "section": "Examples",
    "text": "Examples\nOr, you know‚Ä¶\nsource(\"run_params/datamining_examples_recommended_hyperparameters_v001.R\")\nbasic_hyperparameters &lt;- RLCS_hyperparameters(\n  wildcard_prob = wildcard_prob,\n  ## defaults for rd_trigger, mutation_probability,\n  ## parents_selection_mode && tournament_pressure\n  n_epochs = n_epochs,\n  deletion_trigger = deletion_trigger,\n  deletion_threshold = deletion_threshold\n)\n\n## It makes it more readable here:\nexample_lcs &lt;- rlcs_train(train_environment, basic_hyperparameters)"
  },
  {
    "objectID": "others/RLCS_documentation.html#examples-3",
    "href": "others/RLCS_documentation.html#examples-3",
    "title": "RLCS: An Introduction",
    "section": "Examples",
    "text": "Examples\nBefore\ninc_match_count &lt;- function(M_pop) { ## All versions\n  lapply(M_pop, \\(x) {\n    x$match_count &lt;- x$match_count + 1\n    x\n  })\n}\n\ninc_correct_count &lt;- function(C_pop) { ## SL Specific\n  lapply(C_pop, \\(x) {\n    x$correct_count &lt;- x$correct_count + 1\n    x\n  })\n}\n\ninc_action_count &lt;- function(A_pop) { ## RL Specific\n  lapply(A_pop, \\(x) {\n    x$action_count &lt;- x$action_count + 1\n    x\n  })\n}"
  },
  {
    "objectID": "others/RLCS_documentation.html#examples-4",
    "href": "others/RLCS_documentation.html#examples-4",
    "title": "RLCS: An Introduction",
    "section": "Examples",
    "text": "Examples\nAfter, using a function factory\n## Function factory to increase parameter counts\ninc_param_count &lt;- function(param) {\n  param &lt;- as.name(param)\n  function(pop) {\n    lapply(pop, \\(x) {\n      x[[param]] &lt;- x[[param]] + 1\n      x\n    })\n  }\n}\n\ninc_match_count &lt;- inc_param_count(\"match_count\")\ninc_correct_count &lt;- inc_param_count(\"correct_count\")\ninc_action_count &lt;- inc_param_count(\"action_count\")"
  },
  {
    "objectID": "others/RLCS_documentation.html#epistasis",
    "href": "others/RLCS_documentation.html#epistasis",
    "title": "RLCS: An Introduction",
    "section": "Epistasis",
    "text": "Epistasis\nLCS can somehow recognize how two different parts interact. Aptly‚Ä¶ The term comes from of genetics (genes modified by other genes‚Ä¶). (e.g.¬†XOR‚Ä¶)\n\n  condition action match_count correct_count accuracy numerosity first_seen\n     10##1#      1       15913         15913        1        324        688\n     000###      0       15575         15575        1        357       3394\n     001###      1       14842         14842        1        298       9231\n     11###0      0       13149         13149        1        263      22839\n     ..."
  },
  {
    "objectID": "others/RLCS_documentation.html#examples-5",
    "href": "others/RLCS_documentation.html#examples-5",
    "title": "RLCS: An Introduction",
    "section": "Examples",
    "text": "Examples\nBefore\n## Support function for human-compatible printing:\nmake_pop_printable &lt;- function(classifier) {\n    \n    df &lt;- plyr::rbind.fill(lapply(1:length(classifier), \\(i) {\n        t_c &lt;- classifier[[i]]\n        data.frame(id = t_c$id,\n                   condition = t_c$condition_string,\n                   action = t_c$action,\n                   match_count = t_c$match_count,\n                   correct_count = t_c$correct_count,\n                   accuracy = t_c$accuracy,\n                   numerosity = t_c$numerosity,\n                   first_seen = t_c$first_seen)\n    }))\n    df[order(df$accuracy, df$numerosity, decreasing = T),]\n}\n\n(Even the parameter name is wrong‚Ä¶)"
  },
  {
    "objectID": "others/RLCS_documentation.html#examples-6",
    "href": "others/RLCS_documentation.html#examples-6",
    "title": "RLCS: An Introduction",
    "section": "Examples",
    "text": "Examples\nAfter - S3 object (dependency! plyr::rbind.fill)\nprint.rlcs_population &lt;- function(pop) {\n  if(length(pop) == 0) return(NULL)\n  \n  pop &lt;- lcs_best_sort_sl(pop)\n  pop &lt;- unclass(pop)\n  plyr::rbind.fill(lapply(1:length(pop), \\(i) {\n    t_c &lt;- pop[[i]]\n    data.frame(condition = t_c$condition_string, action = t_c$action,\n               match_count = t_c$match_count, correct_count = t_c$correct_count,\n               accuracy = t_c$accuracy, numerosity = t_c$numerosity,\n               first_seen = t_c$first_seen)\n  }))\n}\n\nprint(example_lcs_population)"
  },
  {
    "objectID": "others/RLCS_documentation.html#how-did-i-approach-the-thing",
    "href": "others/RLCS_documentation.html#how-did-i-approach-the-thing",
    "title": "RLCS: An Introduction",
    "section": "How did I approach the thing?",
    "text": "How did I approach the thing?\n\n\nLists. Lists, everywhere. Which might have been a bad idea‚Ä¶ (data.table?)\nfrom there, lapply() & al.¬†is then my best friend\nStart small, grow fast (because I get obsessed)\nthen clean it\nthen clean it some more\never postponing the move to a Package, though"
  },
  {
    "objectID": "others/RLCS_documentation.html#then-again",
    "href": "others/RLCS_documentation.html#then-again",
    "title": "RLCS: An Introduction",
    "section": "Then again",
    "text": "Then again\nThis is all work in progress.\n\nI plan to make it into a CRAN Package.\nSo: document more, write more tests, reorganize functions‚Ä¶"
  },
  {
    "objectID": "others/RLCS_documentation.html#visuals",
    "href": "others/RLCS_documentation.html#visuals",
    "title": "RLCS: A story",
    "section": "Visuals",
    "text": "Visuals"
  },
  {
    "objectID": "others/RLCS_documentation.html#reinforcement-learning-conundrum",
    "href": "others/RLCS_documentation.html#reinforcement-learning-conundrum",
    "title": "RLCS: An Introduction",
    "section": "Reinforcement Learning Conundrum",
    "text": "Reinforcement Learning Conundrum\nWe‚Äôve seen it works, but‚Ä¶ How do you package an RL algorithm?\n\nYou must make assumptions about the ‚Äúworld‚Äù your agent is going to interact with. This makes things complicated:\n\n\nWhat to include inside the package? What not?\nWhat to expose from the package? What not?\n\n\nAnd a few other such questions slow me down a bit‚Ä¶"
  },
  {
    "objectID": "others/RLCS_documentation.html#execution-speed",
    "href": "others/RLCS_documentation.html#execution-speed",
    "title": "RLCS: An Introduction",
    "section": "Execution Speed",
    "text": "Execution Speed\nFor instance, this is a ‚Äúslow‚Äù algorithm. Option: RCpp for Matching? (under testing right now!)\n\nprofviz"
  },
  {
    "objectID": "others/RLCS_documentation.html#because-text-and-code-wont-make-it-popular",
    "href": "others/RLCS_documentation.html#because-text-and-code-wont-make-it-popular",
    "title": "RLCS: An Introduction",
    "section": "Because text and code won‚Äôt make it popular‚Ä¶",
    "text": "Because text and code won‚Äôt make it popular‚Ä¶\n&lt;TBC&gt;"
  },
  {
    "objectID": "others/RLCS_documentation.html#rl-video",
    "href": "others/RLCS_documentation.html#rl-video",
    "title": "RLCS: An Introduction",
    "section": "RL Video",
    "text": "RL Video"
  },
  {
    "objectID": "others/RLCS_documentation.html#so-how-does-it-work",
    "href": "others/RLCS_documentation.html#so-how-does-it-work",
    "title": "RLCS: An Introduction",
    "section": "So how does it work?",
    "text": "So how does it work?\nImagine you receive samples of data points (aka states) with their corresponding classes (aka actions). All your input entries (aka instances) form an environment.\nIn this case, we will accept binary string states for the environment instances (examples):\n\n\n\nState\nAction/Class\nFrom the‚Ä¶\n\n\n\n\n0010111100110010\n‚Äúsetosa‚Äù\nIris dataset\n\n\n001010\n1\n‚ÄúMux 6‚Äù"
  },
  {
    "objectID": "others/RLCS_documentation.html#generating-a-rule",
    "href": "others/RLCS_documentation.html#generating-a-rule",
    "title": "RLCS: An Introduction",
    "section": "1 - Generating a rule",
    "text": "1 - Generating a rule\n\nThe key: ‚Äú#‚Äù means ‚ÄúI don‚Äôt care‚Äù\n\nCovering a state with a probability of ‚Äú#‚Äù values means making a rule that matches the input state and class/action.\nSomething that could match other (partially) similar input:\n&gt; generate_cover_rule_for_unmatched_instance('010001', 0.2)\n[1] \"0#0001\"\n&gt; generate_cover_rule_for_unmatched_instance('010001', 0.8)\n[1] \"##0###\"\n\nYou receive an instance of the environment (a binary string state and a class).\nThe class here is defined already."
  },
  {
    "objectID": "others/RLCS_documentation.html#wait-what-binary-input",
    "href": "others/RLCS_documentation.html#wait-what-binary-input",
    "title": "RLCS: An Introduction",
    "section": "Wait, what? Binary input?!",
    "text": "Wait, what? Binary input?!\nWell, for now‚Ä¶ Yes. There are alternatives. But for now‚Ä¶\nHere using 4-bits with Gray encoding of ‚Äúdouble quartiles‚Äù per variable, we can create binary string:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width  slb  swb  plb  pwb\n1          5.1         3.5          1.4         0.2 0010 1111 0011 0010\n2          4.9         3.0          1.4         0.2 0011 0101 0011 0010\n3          4.7         3.2          1.3         0.2 0000 1101 0000 0010\n             state  class\n1 0010111100110010 setosa\n2 0011010100110010 setosa\n3 0000110100000010 setosa"
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-generating-a-rule",
    "href": "others/RLCS_documentation.html#key-aspects-generating-a-rule",
    "title": "RLCS: A story",
    "section": "Key Aspects: Generating a rule",
    "text": "Key Aspects: Generating a rule\n\nThe key: ‚Äú#‚Äù means ‚ÄúI don‚Äôt care‚Äù\n\nYou receive an instance of the environment string of binary input (a state and a class). The class here is defined already.\nCovering a state with a probability of ‚Äú#‚Äù values means making a rule that matches the input state and class/action. Something that could match other (partially) similar input:\n&gt; generate_cover_rule_for_unmatched_instance('010001', 0.2)\n[1] \"0#0001\"\n&gt; generate_cover_rule_for_unmatched_instance('010001', 0.8)\n[1] \"##0###\""
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-matching-rule-discovery",
    "href": "others/RLCS_documentation.html#key-aspects-matching-rule-discovery",
    "title": "RLCS: A story",
    "section": "Key Aspects: Matching, Rule Discovery",
    "text": "Key Aspects: Matching, Rule Discovery\nImagine you have by now created a set of rules with their corresponding actions.\nSo you generate new rules when you see new population instances that do not match any"
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-1---generating-a-rule",
    "href": "others/RLCS_documentation.html#key-aspects-1---generating-a-rule",
    "title": "RLCS: An Introduction",
    "section": "Key Aspects 1 - Generating a rule",
    "text": "Key Aspects 1 - Generating a rule\n\nThe key: ‚Äú#‚Äù means ‚ÄúI don‚Äôt care‚Äù\n\nYou receive an instance of the environment (a binary string state and a class). The class here is defined already.\nCovering a state with a probability of ‚Äú#‚Äù values means making a rule that matches the input state and class/action. Something that could match other (partially) similar input:\n&gt; generate_cover_rule_for_unmatched_instance('010001', 0.2)\n[1] \"0#0001\"\n&gt; generate_cover_rule_for_unmatched_instance('010001', 0.8)\n[1] \"##0###\""
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-2---matching",
    "href": "others/RLCS_documentation.html#key-aspects-2---matching",
    "title": "RLCS: An Introduction",
    "section": "Key Aspects 2 - Matching",
    "text": "Key Aspects 2 - Matching\nYou generate new rules when you see new environment instances that do not match any rule yet. Suppose you have a set of rules. That‚Äôs your population (of rules).\n\n\nIf one(+) rule(s) in your population matches your new instance state -&gt; increase the match count of the corresponding classifier.\nIf one(+) rule(s) in your population matches your new instance state && class/action ‚Äì&gt; increase the correct count.\n\n\nincluding their corresponding actions"
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-3---rule-discovery",
    "href": "others/RLCS_documentation.html#key-aspects-3---rule-discovery",
    "title": "RLCS: An Introduction",
    "section": "Key Aspects 3 - Rule Discovery",
    "text": "Key Aspects 3 - Rule Discovery\nAfter a few epochs of exposing the LCS to your full environment, you will have a few rules that match correctly a given instance, the correct set.\nMatch and Correct count are indicators of how good each rule is. But are there other better possibilities?\n\nTake all the correct set, and apply a Genetic Algorithm to that set, to generate new rules!"
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-3---rule-discovery-1",
    "href": "others/RLCS_documentation.html#key-aspects-3---rule-discovery-1",
    "title": "RLCS: An Introduction",
    "section": "Key Aspects 3 - Rule Discovery",
    "text": "Key Aspects 3 - Rule Discovery"
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-4---rule-compaction",
    "href": "others/RLCS_documentation.html#key-aspects-4---rule-compaction",
    "title": "RLCS: A story",
    "section": "Key Aspects 4 - Rule Compaction",
    "text": "Key Aspects 4 - Rule Compaction\nMatching must go through all the population every time an environment instance is presented to the LCS.\n\\[O(match) = epochs*N(environment)*N(population)\\]\nWhere \\(N()\\) means ‚Äúsize of‚Äù.\n\\[e.g. 1.000 * 5.000 * 1.000 = 5.000.000.000\\]\n\nSolution: Reduce the population of rules."
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-5---other-uses",
    "href": "others/RLCS_documentation.html#key-aspects-5---other-uses",
    "title": "RLCS: A story",
    "section": "Key Aspects 5 - Other uses!",
    "text": "Key Aspects 5 - Other uses!\n\nWhy talk about ‚Äúenvironment‚Äù and ‚Äúaction‚Äù? This comes from the world of Reinforcement Learning.\n\n\nAnd because one can read the rules, and ‚Äúunderstand‚Äù the population, you can also use the LCS to interpret the results and thus do data mining!\nAll with the same algorithm!"
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-4---rule-compaction-1",
    "href": "others/RLCS_documentation.html#key-aspects-4---rule-compaction-1",
    "title": "RLCS: A story",
    "section": "Key Aspects 4 - Rule Compaction",
    "text": "Key Aspects 4 - Rule Compaction\n\n\nSubsumption: A ‚Äúperfect‚Äù classifier that has 100% accuracy might be simpler (more # characters) than other classifiers in the population with same classification. Keep only the best classifiers. (Implemented: Accuracy-based subsumption)\nDeletion: You can keep all classifiers that have a 60%+ accuracy after a number of epochs. But you can also cap the population size, keeping only the 10.000 best classifiers.\n‚Ä¶"
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-5---prediction",
    "href": "others/RLCS_documentation.html#key-aspects-5---prediction",
    "title": "RLCS: An Introduction",
    "section": "Key Aspects 5 - Prediction",
    "text": "Key Aspects 5 - Prediction\nImagine a new sample/instance, never seen before. (Test environment)\n\nPrediction is about returning the match set for that new instance.\n\n\nMajority (possibly weighted by numerosity, accuracy‚Ä¶) of proposed class/action will be the prediction.\nThat‚Äôs it! It also means, this is natively an ensemble learning algorithm."
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-6---other-uses",
    "href": "others/RLCS_documentation.html#key-aspects-6---other-uses",
    "title": "RLCS: An Introduction",
    "section": "Key Aspects 6 - Other uses!",
    "text": "Key Aspects 6 - Other uses!\n\nWhy talk about ‚Äúenvironment‚Äù and ‚Äúaction‚Äù? This comes from the world of Reinforcement Learning.\n\n\nAnd because one can read the rules, and ‚Äúunderstand‚Äù the population, you can also use the LCS to interpret the results and thus do data mining!\nAll with the same algorithm!"
  },
  {
    "objectID": "others/RLCS_documentation.html#section-1",
    "href": "others/RLCS_documentation.html#section-1",
    "title": "RLCS: An Introduction",
    "section": "",
    "text": "kaizen-r blog on RLCS"
  },
  {
    "objectID": "others/RLCS_documentation.html#iris",
    "href": "others/RLCS_documentation.html#iris",
    "title": "RLCS: An Introduction",
    "section": "Iris",
    "text": "Iris\n[1] \"Training Runtime: 4.44374799728394\" (seconds)\n[1] \"Training Set Size: 127\"\n[1] \"Confusion 'Matrix' for Class setosa:\"\nsetosa \n     5 \n[1] \"Confusion 'Matrix' for Class versicolor:\"\nversicolor  virginica \n         6          1 \n[1] \"Confusion 'Matrix' for Class virginica:\"\nvirginica \n       10"
  },
  {
    "objectID": "others/RLCS_documentation.html#images-classifier",
    "href": "others/RLCS_documentation.html#images-classifier",
    "title": "RLCS: An Introduction",
    "section": "Images Classifier",
    "text": "Images Classifier"
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-4---controlling-population-size",
    "href": "others/RLCS_documentation.html#key-aspects-4---controlling-population-size",
    "title": "RLCS: An Introduction",
    "section": "Key Aspects 4 - Controlling Population Size",
    "text": "Key Aspects 4 - Controlling Population Size\nMatching must go through all the population every time an environment instance is presented to the LCS.\n\\[O(match) = epochs*N(environment)*N(population)\\]\nWhere \\(N()\\) means ‚Äúsize of‚Äù.\n\\[e.g. 1.000 * 5.000 * 1.000 = 5.000.000.000\\]\n\nSolution: Reduce the population of rules."
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-4---controlling-population-size-1",
    "href": "others/RLCS_documentation.html#key-aspects-4---controlling-population-size-1",
    "title": "RLCS: An Introduction",
    "section": "Key Aspects 4 - Controlling Population Size",
    "text": "Key Aspects 4 - Controlling Population Size\n\n\nSubsumption: A ‚Äúperfect‚Äù classifier that has 100% accuracy might be simpler (more # characters) than other classifiers in the population with same classification. Keep only the best classifiers. (Implemented: Accuracy-based subsumption)\nCompaction: You can keep all classifiers that have a 60%+ accuracy after a number of epochs.\nDeletion: But you can also cap the population size, keeping only the 10.000 best classifiers."
  },
  {
    "objectID": "others/RLCS_documentation.html#keywords",
    "href": "others/RLCS_documentation.html#keywords",
    "title": "RLCS: An Introduction",
    "section": "Keywords",
    "text": "Keywords"
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-4---population-size",
    "href": "others/RLCS_documentation.html#key-aspects-4---population-size",
    "title": "RLCS: An Introduction",
    "section": "Key Aspects 4 - Population Size",
    "text": "Key Aspects 4 - Population Size\nMatching must go through all the population every time an environment instance is presented to the LCS.\n\\[O(match) = epochs*N(environment)*N(population)\\]\nWhere \\(N()\\) means ‚Äúsize of‚Äù.\n\\[e.g. 1.000 * 5.000 * 1.000 = 5.000.000.000\\]\n\nSolution: Reduce the population of rules."
  },
  {
    "objectID": "others/RLCS_documentation.html#key-aspects-4---population-size-1",
    "href": "others/RLCS_documentation.html#key-aspects-4---population-size-1",
    "title": "RLCS: An Introduction",
    "section": "Key Aspects 4 - Population Size",
    "text": "Key Aspects 4 - Population Size\n\n\nSubsumption: A ‚Äúperfect‚Äù classifier that has 100% accuracy might be simpler (more # characters) than other classifiers in the population with same classification. Keep only the best classifiers. (Implemented: Accuracy-based subsumption)\nCompaction: You can keep all classifiers that have a 60%+ accuracy after a number of epochs.\nDeletion: But you can also cap the population size, keeping only the 10.000 best classifiers."
  },
  {
    "objectID": "others/RLCS_documentation.html#matching",
    "href": "others/RLCS_documentation.html#matching",
    "title": "RLCS: An Introduction",
    "section": "2 - Matching",
    "text": "2 - Matching\nYou generate new rules when you see new environment instances that do not match any rule yet. Suppose you have a set of rules. That‚Äôs your population (of rules).\n\n\nIf one(+) rule(s) in your population matches your new instance state -&gt; increase the match count of the corresponding classifier.\nIf one(+) rule(s) in your population matches your new instance state && class/action ‚Äì&gt; increase the correct count.\n\n\nincluding their corresponding actions"
  },
  {
    "objectID": "others/RLCS_documentation.html#rule-discovery",
    "href": "others/RLCS_documentation.html#rule-discovery",
    "title": "RLCS: An Introduction",
    "section": "3 - Rule Discovery",
    "text": "3 - Rule Discovery\nAfter a few epochs of exposing the LCS to your full environment, you will have a few rules that match correctly a given instance, the correct set.\nMatch and Correct count are indicators of how good each rule is. But are there other better possibilities?\n\nTake all the correct set, and apply a Genetic Algorithm to that set, to generate new rules!"
  },
  {
    "objectID": "others/RLCS_documentation.html#rule-discovery-ga",
    "href": "others/RLCS_documentation.html#rule-discovery-ga",
    "title": "RLCS: An Introduction",
    "section": "3 - Rule Discovery (GA)",
    "text": "3 - Rule Discovery (GA)\n\nmut_point &lt;- which(runif(nchar(t_instance_state)) &lt; mut_prob)"
  },
  {
    "objectID": "others/RLCS_documentation.html#population-size",
    "href": "others/RLCS_documentation.html#population-size",
    "title": "RLCS: An Introduction",
    "section": "4 - Population Size",
    "text": "4 - Population Size\nMatching must go through all the population every time an environment instance is presented to the LCS.\n\\[O(match) = epochs*N(environment)*N(population)\\]\nWhere \\(N()\\) means ‚Äúsize of‚Äù.\n\\[e.g. 1.000 * 5.000 * 1.000 = 5.000.000.000\\]\n\nOne option: Reduce the population of rules."
  },
  {
    "objectID": "others/RLCS_documentation.html#population-size-1",
    "href": "others/RLCS_documentation.html#population-size-1",
    "title": "RLCS: An Introduction",
    "section": "4 - Population Size",
    "text": "4 - Population Size\n\n\nSubsumption: A ‚Äúperfect‚Äù classifier that has 100% accuracy might be simpler (more # characters) than other classifiers in the population with same classification. Keep only the best classifiers. (Implemented: Accuracy-based subsumption)\nCompaction: You can keep all classifiers that have e.g.¬†60%+ accuracy after a number of epochs.\nDeletion: But you can also cap the population size, keeping only e.g.¬†10.000 best classifiers."
  },
  {
    "objectID": "others/RLCS_documentation.html#prediction",
    "href": "others/RLCS_documentation.html#prediction",
    "title": "RLCS: An Introduction",
    "section": "5 - Prediction",
    "text": "5 - Prediction\nImagine a new sample/instance, never seen before. (Test environment)\n\nPrediction is about returning the match set for that new instance.\n\n\nMajority (possibly weighted by numerosity, accuracy‚Ä¶) of proposed class/action will be the prediction.\nThat‚Äôs it! It also means, this is natively an ensemble learning algorithm."
  },
  {
    "objectID": "others/RLCS_documentation.html#other-uses",
    "href": "others/RLCS_documentation.html#other-uses",
    "title": "RLCS: An Introduction",
    "section": "6 - Other uses!",
    "text": "6 - Other uses!\n\nWhy talk about ‚Äúenvironment‚Äù and ‚Äúaction‚Äù? This comes from the world of Reinforcement Learning.\n\n\nAnd because one can read the rules, and ‚Äúunderstand‚Äù the population, you can also use the LCS to interpret the results and thus do data mining!\nAll with the same algorithm!"
  },
  {
    "objectID": "others/RLCS_documentation.html#section",
    "href": "others/RLCS_documentation.html#section",
    "title": "RLCS: An Introduction",
    "section": "",
    "text": "kaizen-r blog on RLCS"
  },
  {
    "objectID": "others/RLCS_documentation.html#a-future-sorry-r-package",
    "href": "others/RLCS_documentation.html#a-future-sorry-r-package",
    "title": "RLCS: An Introduction",
    "section": "A (future, sorry!) R package",
    "text": "A (future, sorry!) R package\nA package to implement a simple version of the Learning Classifier System algorithm:\n\nBinary Alphabet, tournament/one-point crossover GA, accuracy based, Michigan-style LCS\nWith examples, demonstrating the implementation for:\n\nData Mining\nSupervised Learning\nReinforcement Learning"
  },
  {
    "objectID": "others/RLCS_documentation.html#rl-too",
    "href": "others/RLCS_documentation.html#rl-too",
    "title": "RLCS: An Introduction",
    "section": "RL, TOO!",
    "text": "RL, TOO!\n\nFirst self-brain-storm on RL with LCS"
  },
  {
    "objectID": "others/RLCS_documentation.html#parallel-computing",
    "href": "others/RLCS_documentation.html#parallel-computing",
    "title": "RLCS: An Introduction",
    "section": "Parallel Computing",
    "text": "Parallel Computing\nParallel computing? %dopar% was tested (it works, but‚Ä¶)\n\n\nvertical and horizontal partitioning\n\nBreak data set (vertical). Two options\n\ninstances subsets (reduce population covered per thread/core)\nSubstrings of states (reduce search space)\nboth are ‚Äúrisky‚Äù\n\nrun fewer iterations (epochs) on full dataset, but on several cores in parallel"
  },
  {
    "objectID": "others/RLCS_documentation.html#parallel-computing-1",
    "href": "others/RLCS_documentation.html#parallel-computing-1",
    "title": "RLCS: An Introduction",
    "section": "Parallel Computing",
    "text": "Parallel Computing\nDepending on the dataset, it can be done‚Ä¶ Or not‚Ä¶"
  },
  {
    "objectID": "others/RLCS_documentation.html#also-included-data-mining",
    "href": "others/RLCS_documentation.html#also-included-data-mining",
    "title": "RLCS: An Introduction",
    "section": "Also included: Data Mining",
    "text": "Also included: Data Mining\nGiven that the rules are ‚Äúexpressive‚Äù, sometimes you can ask the LCS to find rules that appear in your data, NOT to classify future samples, but to identify what is important in different classes of your data.\n\nREAL WORLD anecdote: inventory of 10K rows with 20 columns, each duly binary encoded. I learnt something about my inventory!"
  },
  {
    "objectID": "posts/2025-05-18_RLCS_with_Cpp/index.html",
    "href": "posts/2025-05-18_RLCS_with_Cpp/index.html",
    "title": "RLCS: Making things faster where it matters",
    "section": "",
    "text": "This entry was not ‚Äúplanned‚Äù, as I am still working on this last bit, and also I shouldn‚Äôt touch things as I have a presentation coming next week on this, but‚Ä¶\nOne key issue with the LCS algorithm, as repeatedly mentioned thus far, is training speed‚Ä¶ It‚Äôs not great. And after running it many (many many) times, it does work, but I am well aware this will be a problem at some point. We also know R is an interpreted and (thus) slow language. I have done my best to take advantage of SIMD capacity of nowadays CPUs (through the lapply() family of functions).\nAnd so, as also mentioned in the past, I have been profiling the code (using profvis) and I have known for quite a while that the bottleneck in the end is in the matching operation. I won‚Äôt explain what matching entails here, but it is maybe the most used operation of the algorithm, and one that is expensive.\nNow I could (I guess) re-code it all in C++ (or, say, Rust? I was reading up on that‚Ä¶ But that‚Äôs a story for some other time), but let‚Äôs face it‚Ä¶ I don‚Äôt wanna. Also, it‚Äôd be a lousy R Package if it had no R code‚Ä¶\nBut, let‚Äôs face it, sometimes one has no choice but to move beyond R."
  },
  {
    "objectID": "posts/2025-05-18_RLCS_with_Cpp/index.html#rcpp-for-the-win",
    "href": "posts/2025-05-18_RLCS_with_Cpp/index.html#rcpp-for-the-win",
    "title": "RLCS: Making things faster where it matters",
    "section": "",
    "text": "This entry was not ‚Äúplanned‚Äù, as I am still working on this last bit, and also I shouldn‚Äôt touch things as I have a presentation coming next week on this, but‚Ä¶\nOne key issue with the LCS algorithm, as repeatedly mentioned thus far, is training speed‚Ä¶ It‚Äôs not great. And after running it many (many many) times, it does work, but I am well aware this will be a problem at some point. We also know R is an interpreted and (thus) slow language. I have done my best to take advantage of SIMD capacity of nowadays CPUs (through the lapply() family of functions).\nAnd so, as also mentioned in the past, I have been profiling the code (using profvis) and I have known for quite a while that the bottleneck in the end is in the matching operation. I won‚Äôt explain what matching entails here, but it is maybe the most used operation of the algorithm, and one that is expensive.\nNow I could (I guess) re-code it all in C++ (or, say, Rust? I was reading up on that‚Ä¶ But that‚Äôs a story for some other time), but let‚Äôs face it‚Ä¶ I don‚Äôt wanna. Also, it‚Äôd be a lousy R Package if it had no R code‚Ä¶\nBut, let‚Äôs face it, sometimes one has no choice but to move beyond R."
  },
  {
    "objectID": "posts/2025-05-18_RLCS_with_Cpp/index.html#the-results-please",
    "href": "posts/2025-05-18_RLCS_with_Cpp/index.html#the-results-please",
    "title": "RLCS: Making things faster where it matters",
    "section": "The results, please?!",
    "text": "The results, please?!\nAlright alright, I digress, I know.\nHere.\nBEFORE:\n\n\n\nProfile and Guilty code in R\n\n\nNow this is a real, but incomplete test.\nAnd all I did was change the one piece of code like so:\n    ## Inside \"get_match_set()\", a sub-routine gets indices...\n\n    ## BEFORE\n    # match_set &lt;- which(sapply(pop, \\(item, ti_cond) {\n    #   rule &lt;- item$condition_list\n    #   !(any(ti_cond[rule$'0'] != 0) || any(ti_cond[rule$'1'] != 1))\n    # }, ti_cond))\n    \n    ## AFTER\n    match_set &lt;- get_match_set_cpp(pop, ti_cond)\nAnd (although it‚Äôs very basic and probably not great‚Ä¶) here the new version of the same functionality, but in C++ with some RCpp ‚Äúsugar‚Äù:\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\nbool element_matches(List element, NumericVector ti_cond) {\n  List temp_conds = element(\"condition_list\");\n  NumericVector temp_conds_0 = temp_conds(\"0\");\n  int j;\n  for(j = 0; j &lt; temp_conds_0.size(); j++) {\n    if(ti_cond[temp_conds_0[j]-1] != 0) { return(false); }\n  }\n  NumericVector temp_conds_1 = temp_conds(\"1\");\n  for(j = 0; j &lt; temp_conds_1.length(); j++) {\n    if(ti_cond[temp_conds_1[j]-1] != 1) { return(false); }\n  }\n  return(true);\n}\n\n// [[Rcpp::export]]\nRcpp::NumericVector get_match_set_cpp(List pop, NumericVector ti_cond) {\n  NumericVector matches_indices;\n  int i;\n  for(i = 0; i &lt; pop.length(); i++) {\n    if(element_matches(pop[i], ti_cond)) {\n      matches_indices.push_back(i+1);\n    }\n  }\n  return(matches_indices);\n}\nAnd AFTER said changes:\n\n\n\n1/3 runtime for that section, &lt; 1/2 overall!\n\n\nNotice the 4 seconds mark vs the 10 seconds (overall)? And that‚Äôs just one test‚Ä¶ That is, in spite of using vectorized operations, I was far from ‚Äúfast enough‚Äù, compared to a compiled version."
  },
  {
    "objectID": "posts/2025-05-18_RLCS_with_Cpp/index.html#the-issue-i-faced",
    "href": "posts/2025-05-18_RLCS_with_Cpp/index.html#the-issue-i-faced",
    "title": "RLCS: Making things faster where it matters",
    "section": "The issue I faced",
    "text": "The issue I faced\nOne problem I had with this‚Ä¶ Is that I need to update my whole MacBook OS and XCode to get RCpp to run on it ü§ï\nAnd I‚Äôm not home, I‚Äôm using Mobile data, and I‚Äôm literally 5 days away from having to demo this thing ‚Äúlive‚Äù, and I just don‚Äôt want to break anything right now!\nSo‚Ä¶ I went back to Docker (which I hadn‚Äôt used for a while‚Ä¶ And the OSX complained it was a ‚Äúpotential malware‚Äù, and I had to re-install that too‚Ä¶), and got myself a new RStudio container, and did all the tests from there‚Ä¶\nSo I‚Äôll do more testing AFTER I have a new full-backup, and I‚Äôm at home to upgrade the whole environment‚Ä¶\nBut regardless (and that‚Äôs what‚Äôs cool with Docker!), this just worked :)"
  },
  {
    "objectID": "posts/2025-05-18_RLCS_with_Cpp/index.html#conclusions",
    "href": "posts/2025-05-18_RLCS_with_Cpp/index.html#conclusions",
    "title": "RLCS: Making things faster where it matters",
    "section": "Conclusions",
    "text": "Conclusions\nSometimes, you have to go down one level (or two)‚Ä¶ But it‚Äôs soooo worth it :)"
  },
  {
    "objectID": "others/RLCS_documentation.html",
    "href": "others/RLCS_documentation.html",
    "title": "RLCS: An Introduction",
    "section": "",
    "text": "Let‚Äôs call it Machine Learning. Today that‚Äôs mostly Neural networks. And mostly, that means it‚Äôs all black boxes.\nThere are ways around that. (e.g.¬†Trees and other ‚Äúopen book‚Äù algorithms‚Ä¶)\n. . .\nJohn H. Holland proposed an algorithm with the Cognitive System One program (1976). Later, people came up with variations‚Ä¶ Today we focus on Michigan-style LCS.\n\nActually‚Ä¶ That‚Äôs it. No time to dive deeper here.\nToday, we‚Äôll be discussing one such explainable Machine Learning algorithm."
  },
  {
    "objectID": "others/RLCS_documentation.html#iris-1",
    "href": "others/RLCS_documentation.html#iris-1",
    "title": "RLCS: An Introduction",
    "section": "Iris",
    "text": "Iris\n\nvisualizing one classifier - iris"
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html",
    "title": "RLCS: Caring for better performance",
    "section": "",
    "text": "There was still room for improvement. Probably still is, of course, just‚Ä¶ Not with a total redo, I guess.\nThere were two or 3 key ingredients to the improvements.\nDaring with fewer epochs (and I found out eventually, I really didn‚Äôt need hundreds or thousands in some cases, so I‚Äôve been waiting during many tests for overall probably quite a few hours that could have been minutes, had I tried better combinations of hyperparameters!)\nJust so we‚Äôre clear: With the Rcpp trick last week, now working on my Mac (Macbook Air M1), and quite a few basic improvements (unnecessary iterations here and there (shame on me), particularly with subsumption and deletion, faster sorting now without resorting to rbind.fill(lapply()), which is nice, but quite unnecessary, and sorting is key at several moments)‚Ä¶\nAnd (and this is relevant) by using a better set of hyperparameters per use-case‚Ä¶\nOverall, we‚Äôre talking quite literally about one order of magnitude in performance gain, in some cases.\nNot bad!"
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#rcpp-did-its-part-but",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#rcpp-did-its-part-but",
    "title": "RLCS: Caring for better performance",
    "section": "",
    "text": "There was still room for improvement. Probably still is, of course, just‚Ä¶ Not with a total redo, I guess.\nThere were two or 3 key ingredients to the improvements.\nDaring with fewer epochs (and I found out eventually, I really didn‚Äôt need hundreds or thousands in some cases, so I‚Äôve been waiting during many tests for overall probably quite a few hours that could have been minutes, had I tried better combinations of hyperparameters!)\nJust so we‚Äôre clear: With the Rcpp trick last week, now working on my Mac (Macbook Air M1), and quite a few basic improvements (unnecessary iterations here and there (shame on me), particularly with subsumption and deletion, faster sorting now without resorting to rbind.fill(lapply()), which is nice, but quite unnecessary, and sorting is key at several moments)‚Ä¶\nAnd (and this is relevant) by using a better set of hyperparameters per use-case‚Ä¶\nOverall, we‚Äôre talking quite literally about one order of magnitude in performance gain, in some cases.\nNot bad!"
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#from-minutes-to-seconds",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#from-minutes-to-seconds",
    "title": "RLCS: Caring for better performance",
    "section": "From minutes to seconds",
    "text": "From minutes to seconds"
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#from-7-minutes-to-2-minutes-with-bigger-training-sets",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#from-7-minutes-to-2-minutes-with-bigger-training-sets",
    "title": "RLCS: Caring for better performance",
    "section": "From 7 minutes to <2 minutes (with bigger training sets)",
    "text": "From 7 minutes to &lt;2 minutes (with bigger training sets)\nQuite literally. I had a screenshot of me training with images here.\nThere it said, basically:\n6.9 (~7) minutes. For better results, even 9 minutes. For a training set of 500 samples. That was 2 months ago.\nHere is what it is today:\n\n\n\n\n\nThat‚Äôs well under one fourth of the runtime, and with 1.6x the data."
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#same-for-rl",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#same-for-rl",
    "title": "RLCS: Caring for better performance",
    "section": "Same for RL!",
    "text": "Same for RL!\nTraining an agent that is reasonably good (remember, it is stochastic, so the thing isn‚Äôt always easily comparable), at least from statistics of its last 1000 moves, used to take quite literally 10 minutes.\nI‚Äôve gotten under 2m now:\n\n\n\nTraining a valid agent under 2 minutes"
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#from-4-minutes-to-3-seconds",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#from-4-minutes-to-3-seconds",
    "title": "RLCS: Caring for better performance",
    "section": "From 4 minutes to <3 seconds",
    "text": "From 4 minutes to &lt;3 seconds\nYes. Well, this one is my fault, really.\nI THOUGHT (in my head, with no tangible proof) that the ‚Äúnot_bit_4_10‚Äù was supposed to be hard for the machine. What with 10-bits strings and all‚Ä¶ As it turns out, if you trust the process, it needs not be.\nHere is the original runtime for that particular test:\n\n\n\nnot_bit_4 in a 10bits string\n\n\nThat was bad!\nAs I‚Äôve been working on performance, here a screenshot of runtime with same perfect resulting classifier today:\n\n\n\nhard to believe: 10-20x faster‚Ä¶\n\n\nNow, to be fair: Hyperparameters were ‚Äúoptimized‚Äù for MUX6, and I was still focusing on functionality more than performance, back then, but now‚Ä¶ Well, now it is ‚Äúbearable‚Äù :)"
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#what-about-iris",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#what-about-iris",
    "title": "RLCS: Caring for better performance",
    "section": "What about iris?",
    "text": "What about iris?\nWell, not to worry!\nI did myself (and the community) a great disservice by not optimizing this a week ago. During my last presentation (last week), I mentioned it took 3 minutes to train on Iris dataset. What a pity. What poor performance that was, truly‚Ä¶\nHere is the update, and I believe it‚Äôs significant:\n\n\n\nTraining on Iris dataset within seconds\n\n\nFor reference (for myself, to have a sense of where the competition is at), I ran a demo with a Neuralnet. Indeed, that‚Äôs like &lt;2 seconds, maybe &lt;1 sec.¬†But it‚Äôs probably (my guess) all C.\nPlus, we know the LCS algorithm is inherently slow, there are limits to what we can do about it.\nStill.\nWell, now it‚Äôs faster. It takes 3 seconds. 1.7% the time announced a few days back‚Ä¶"
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#limits-of-rcpp-for-doparallel",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#limits-of-rcpp-for-doparallel",
    "title": "RLCS: Caring for better performance",
    "section": "Limits of Rcpp for DoParallel",
    "text": "Limits of Rcpp for DoParallel\nSo I found one thing that will require more work: %dopar% and Rcpp don‚Äôt play nice ‚Äúout of the box‚Äù.\nSeems like they will WHEN I make the whole thing into a package.\nEven then, parallel training for images classification with 1.6x the original training size, parallelized over 7 cores (M1) and split accordingly (input subsets), gets me similar results without the Rcpp gains as the Rcpp serial version. And the serial Rcpp version is already about &gt; 1.5x as fast as the pure-R version, so‚Ä¶\nMaybe we can go even faster (and I don‚Äôt know that this particular example was the harder one for justifying parallel computing‚Ä¶ Oh well).\nSo here, another thing I will have to work on."
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#conclusions",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#conclusions",
    "title": "RLCS: Caring for better performance",
    "section": "Conclusions",
    "text": "Conclusions\nI don‚Äôt know that I‚Äôll be capable of making this much faster by now, lest I review the overall approach with lists for something even better, maybe (data.table?). But that would be for a V3 or something, so not for now.\nRight now, it‚Äôs not too bad.\nHeck, dare I‚Ä¶ I‚Äôd say it‚Äôs actually maybe even quite competitive now, given the readability of the output model."
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#limits-of-rcpp-for-doparallel-for-now",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#limits-of-rcpp-for-doparallel-for-now",
    "title": "RLCS: Caring for better performance",
    "section": "Limits of Rcpp for DoParallel (for now)",
    "text": "Limits of Rcpp for DoParallel (for now)\nSo I found one thing that will require more work: %dopar% and Rcpp don‚Äôt play nice ‚Äúout of the box‚Äù.\nSeems like they will WHEN I make the whole thing into a package.\nEven then, parallel training for images classification with 1.6x the original training size, parallelized over 7 cores (M1) and split accordingly (input subsets), gets me similar results without the Rcpp gains as the Rcpp serial version. And the serial Rcpp version is already about &gt; 1.5x as fast as the pure-R version, so‚Ä¶\nMaybe we can go even faster (and I don‚Äôt know that this particular example was the harder one for justifying parallel computing‚Ä¶ Oh well).\nSo here, another thing I will have to work on."
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#simplistic-while-functional-2d-multi-agent-rl",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#simplistic-while-functional-2d-multi-agent-rl",
    "title": "Multi-Agent RL",
    "section": "",
    "text": "Really all I did here, is modify the simple World I created a few months back to support multiple agents (blue-dots).\nThen I simply loop through each agent in turn (which means, they act sequentially), which is a simplification indeed, but sufficient for a quick demo.\nFinally, I train them, 16K steps for good measure, to look for green dots (food) and avoid the rest (red enemies, walls, etc.).\nHere is what it looks like (I even added an audio track :D)"
  },
  {
    "objectID": "posts/2025-05-25_RLCS_overall_faster/index.html#what-for",
    "href": "posts/2025-05-25_RLCS_overall_faster/index.html#what-for",
    "title": "Multi-Agent RL",
    "section": "What for?",
    "text": "What for?\nGood question. Because I can? Nah‚Ä¶\nBut also, consider this: Each of these 5 agents here have learnt independently their own set of rules with the LCS algorithm. And they seem to do a pretty good job.\nIn terms of Applications, I don‚Äôt know, but maybe a non-centrally coordinated, fully distributed behaviour, for a cleaning team of robots would be a kind of application here.\nAnd no, that‚Äôs not a great idea (it would be easy to teach the robots what to do in this particular scenario) here. But in a more complex setting, maybe writing down behaviour rules would be harder, and then the LCS approach might make sense."
  },
  {
    "objectID": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html",
    "href": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html",
    "title": "Multi-Agent RL",
    "section": "",
    "text": "Really all I did here, is modify the simple World I created a few months back to support multiple agents (blue-dots).\nThen I simply loop through each agent in turn (which means, they act sequentially), which is a simplification indeed, but sufficient for a quick demo.\nFinally, I train them, 16K steps for good measure, to look for green dots (food) and avoid the rest (red enemies, walls, etc.).\nHere is what it looks like (I even added an audio track :D)"
  },
  {
    "objectID": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#rcpp-did-its-part-but",
    "href": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#rcpp-did-its-part-but",
    "title": "RLCS: Caring for better performance",
    "section": "",
    "text": "There was still room for improvement. Probably still is, of course, just‚Ä¶ Not with a total redo, I guess.\nThere were two or 3 key ingredients to the improvements.\nDaring with fewer epochs (and I found out eventually, I really didn‚Äôt need hundreds or thousands in some cases, so I‚Äôve been waiting during many tests for overall probably quite a few hours that could have been minutes, had I tried better combinations of hyperparameters!)\nJust so we‚Äôre clear: With the Rcpp trick last week, now working on my Mac (Macbook Air M1), and quite a few basic improvements (unnecessary iterations here and there (shame on me), particularly with subsumption and deletion, faster sorting now without resorting to rbind.fill(lapply()), which is nice, but quite unnecessary, and sorting is key at several moments)‚Ä¶\nAnd (and this is relevant) by using a better set of hyperparameters per use-case‚Ä¶\nOverall, we‚Äôre talking quite literally about one order of magnitude in performance gain, in some cases.\nNot bad!"
  },
  {
    "objectID": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#from-7-minutes-to-2-minutes-with-bigger-training-sets",
    "href": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#from-7-minutes-to-2-minutes-with-bigger-training-sets",
    "title": "RLCS: Caring for better performance",
    "section": "From 7 minutes to <2 minutes (with bigger training sets)",
    "text": "From 7 minutes to &lt;2 minutes (with bigger training sets)\nQuite literally. I had a screenshot of me training with images here.\nThere it said, basically:\n6.9 (~7) minutes. For better results, even 9 minutes. For a training set of 500 samples. That was 2 months ago.\nHere is what it is today:\n\n\n\n\n\nThat‚Äôs well under one fourth of the runtime, and with 1.6x the data."
  },
  {
    "objectID": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#from-4-minutes-to-3-seconds",
    "href": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#from-4-minutes-to-3-seconds",
    "title": "RLCS: Caring for better performance",
    "section": "From 4 minutes to <3 seconds",
    "text": "From 4 minutes to &lt;3 seconds\nYes. Well, this one is my fault, really.\nI THOUGHT (in my head, with no tangible proof) that the ‚Äúnot_bit_4_10‚Äù was supposed to be hard for the machine. What with 10-bits strings and all‚Ä¶ As it turns out, if you trust the process, it needs not be.\nHere is the original runtime for that particular test:\n\n\n\nnot_bit_4 in a 10bits string\n\n\nThat was bad!\nAs I‚Äôve been working on performance, here a screenshot of runtime with same perfect resulting classifier today:\n\n\n\nhard to believe: 10-20x faster‚Ä¶\n\n\nNow, to be fair: Hyperparameters were ‚Äúoptimized‚Äù for MUX6, and I was still focusing on functionality more than performance, back then, but now‚Ä¶ Well, now it is ‚Äúbearable‚Äù :)"
  },
  {
    "objectID": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#what-about-iris",
    "href": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#what-about-iris",
    "title": "RLCS: Caring for better performance",
    "section": "What about iris?",
    "text": "What about iris?\nWell, not to worry!\nI did myself (and the community) a great disservice by not optimizing this a week ago. During my last presentation (last week), I mentioned it took 3 minutes to train on Iris dataset. What a pity. What poor performance that was, truly‚Ä¶\nHere is the update, and I believe it‚Äôs significant:\n\n\n\nTraining on Iris dataset within seconds\n\n\nFor reference (for myself, to have a sense of where the competition is at), I ran a demo with a Neuralnet. Indeed, that‚Äôs like &lt;2 seconds, maybe &lt;1 sec.¬†But it‚Äôs probably (my guess) all C.\nPlus, we know the LCS algorithm is inherently slow, there are limits to what we can do about it.\nStill.\nWell, now it‚Äôs faster. It takes 3 seconds. 1.7% the time announced a few days back‚Ä¶"
  },
  {
    "objectID": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#same-for-rl",
    "href": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#same-for-rl",
    "title": "RLCS: Caring for better performance",
    "section": "Same for RL!",
    "text": "Same for RL!\nTraining an agent that is reasonably good (remember, it is stochastic, so the thing isn‚Äôt always easily comparable), at least from statistics of its last 1000 moves, used to take quite literally 10 minutes.\nI‚Äôve gotten under 2m now:\n\n\n\nTraining a valid agent under 2 minutes"
  },
  {
    "objectID": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#limits-of-rcpp-for-doparallel-for-now",
    "href": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#limits-of-rcpp-for-doparallel-for-now",
    "title": "RLCS: Caring for better performance",
    "section": "Limits of Rcpp for DoParallel (for now)",
    "text": "Limits of Rcpp for DoParallel (for now)\nSo I found one thing that will require more work: %dopar% and Rcpp don‚Äôt play nice ‚Äúout of the box‚Äù.\nSeems like they will WHEN I make the whole thing into a package.\nEven then, parallel training for images classification with 1.6x the original training size, parallelized over 7 cores (M1) and split accordingly (input subsets), gets me similar results without the Rcpp gains as the Rcpp serial version. And the serial Rcpp version is already about &gt; 1.5x as fast as the pure-R version, so‚Ä¶\nMaybe we can go even faster (and I don‚Äôt know that this particular example was the harder one for justifying parallel computing‚Ä¶ Oh well).\nSo here, another thing I will have to work on."
  },
  {
    "objectID": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#conclusions",
    "href": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#conclusions",
    "title": "Multi-Agent RL",
    "section": "Conclusions",
    "text": "Conclusions\nI don‚Äôt kid myself, but I was half-hoping to see some kind of ‚Äúemergent‚Äù behaviour :D Obviously, that didn‚Äôt happen.\nThen again, there is much more to that, and my agents are pretty simple‚Ä¶ Oh well.\nIt works. That counts.\nNext up: I think I want to look into ‚ÄúComplex Adaptive Systems‚Äù. I have found a book that explains them pretty good, and provides some examples in prose, that I might want to transform into actual computer models‚Ä¶ If, you know, I make the time for it."
  },
  {
    "objectID": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#simplistic-while-functional-2d-multi-agent-rl",
    "href": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#simplistic-while-functional-2d-multi-agent-rl",
    "title": "Multi-Agent RL",
    "section": "",
    "text": "Really all I did here, is modify the simple World I created a few months back to support multiple agents (blue-dots).\nThen I simply loop through each agent in turn (which means, they act sequentially), which is a simplification indeed, but sufficient for a quick demo.\nFinally, I train them, 16K steps for good measure, to look for green dots (food) and avoid the rest (red enemies, walls, etc.).\nHere is what it looks like (I even added an audio track :D)"
  },
  {
    "objectID": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#what-for",
    "href": "posts/2025-06-05_RLCS_MultiAgent_RL/index.html#what-for",
    "title": "Multi-Agent RL",
    "section": "What for?",
    "text": "What for?\nGood question. Because I can? Nah‚Ä¶\nBut also, consider this: Each of these 5 agents here have learnt independently their own set of rules with the LCS algorithm. And they seem to do a pretty good job.\nIn terms of Applications, I don‚Äôt know, but maybe a non-centrally coordinated, fully distributed behaviour, for a cleaning team of robots would be a kind of application here.\nAnd no, that‚Äôs not a great idea (it would be easy to teach the robots what to do in this particular scenario) here. But in a more complex setting, maybe writing down behaviour rules would be harder, and then the LCS approach might make sense."
  },
  {
    "objectID": "posts/2025-06-08_Schelling_Segregation_Model/index.html",
    "href": "posts/2025-06-08_Schelling_Segregation_Model/index.html",
    "title": "Classic Computational Social Model: Schelling‚Äôs Segregation",
    "section": "",
    "text": "And I thought some of these ideas were quite interesting. I don‚Äôt know yet what to make of it all (I haven‚Äôt read it all, either). It feels some of it is related to cybernetics, feedback loops, agents-based modelling (that last one, for sure). I am not the biggest fan of sociology, but these agents ideas have me curious though.\nAnyhow, so I was reading on that topic, of modelling things from a bottoms-up perspective, with independent agents making their own decisions, and of course whether there is an emergent behaviour then is interesting.\nOne such social model is the ‚Äúclassic‚Äù computational model of Schelling (1978). I‚Äôm not a fan of the concept it illustrates (supposedly related to some concept of segregation, whichever the kind or inspiratioin‚Ä¶). But the thing is, the emergent behavior of some level of self-organization is interesting, in-as-much there is no central push here for organization, just agents preferences ‚Äúplaying out‚Äù."
  },
  {
    "objectID": "posts/2025-06-08_Schelling_Segregation_Model/index.html#i-was-reading-about-complex-adaptive-systems",
    "href": "posts/2025-06-08_Schelling_Segregation_Model/index.html#i-was-reading-about-complex-adaptive-systems",
    "title": "Classic Computational Social Model: Schelling‚Äôs Segregation",
    "section": "",
    "text": "And I thought some of these ideas were quite interesting. I don‚Äôt know yet what to make of it all (I haven‚Äôt read it all, either). It feels some of it is related to cybernetics, feedback loops, agents-based modelling (that last one, for sure). I am not the biggest fan of sociology, but these agents ideas have me curious though.\nAnyhow, so I was reading on that topic, of modelling things from a bottoms-up perspective, with independent agents making their own decisions, and of course whether there is an emergent behaviour then is interesting.\nOne such social model is the ‚Äúclassic‚Äù computational model of Schelling (1978). I‚Äôm not a fan of the concept it illustrates (supposedly related to some concept of segregation, whichever the kind or inspiratioin‚Ä¶). But the thing is, the emergent behavior of some level of self-organization is interesting, in-as-much there is no central push here for organization, just agents preferences ‚Äúplaying out‚Äù."
  },
  {
    "objectID": "posts/2025-06-08_Schelling_Segregation_Model/index.html#the-algorithm-and-the-results",
    "href": "posts/2025-06-08_Schelling_Segregation_Model/index.html#the-algorithm-and-the-results",
    "title": "Classic Computational Social Model: Schelling‚Äôs Segregation",
    "section": "The algorithm and the results",
    "text": "The algorithm and the results\nSo the concept is pretty simple. You have ‚Äúagents‚Äù (say, people), that prefer to ‚Äúlive next to one another‚Äù if they are of the same type. What that means is up to the reader, I don‚Äôt mind.\nHere, it just means, ‚Äúgreen agents‚Äù will prefer to move if less than 30% of their neighbours are also green. ‚ÄúRed agents‚Äù are less tolerant and want at least 50% of their neighbours to be red as well.\nIf those wishes are not fulfilled, the agents will move to a random (empty) location. We then iterate a few steps of time. At each step, each agent is asked in turn whether they want to move.\nThat‚Äôs two loops, and a very simple logic. We start out with a ‚Äúrandom‚Äù world, with roughly (in this example) 33% of cells filled with green agents, 33% of red agents, 33% of empty cells.\nThat‚Äôs it.\nAnd with just the agents‚Äô preferences, you can probably already guess what will happen. But the fun part is, there will be organization, without a central policy or ruling system, and even though none of the agents prefers to have a majority of the same type of agents. And yet‚Ä¶\nHere the resulting animation of 1 run of said algorithm:"
  },
  {
    "objectID": "posts/2025-06-08_Schelling_Segregation_Model/index.html#conclusions",
    "href": "posts/2025-06-08_Schelling_Segregation_Model/index.html#conclusions",
    "title": "Classic Computational Social Model: Schelling‚Äôs Segregation",
    "section": "Conclusions",
    "text": "Conclusions\nThis was more of an interlude than anything else. I am far from done with the RLCS work. But I can‚Äôt avoid looking into more stuff.\nThe agents-based ideas, complex systems, systems theory, cybernetics, etc. is always quite fascinating to me.\nAnd so I wanted to do one exercise while I had it in mind. The book didn‚Äôt say how to implement it, but gave out enough of the concepts to easily make it into a working demo program."
  },
  {
    "objectID": "posts/2025-06-08_Schelling_Segregation_Model/index.html#references",
    "href": "posts/2025-06-08_Schelling_Segregation_Model/index.html#references",
    "title": "Classic Computational Social Model: Schelling‚Äôs Segregation",
    "section": "References",
    "text": "References\nThe book inspiring this post."
  },
  {
    "objectID": "posts/2025-06-09_Notes_to_Self/index2.html",
    "href": "posts/2025-06-09_Notes_to_Self/index2.html",
    "title": "Morning Coffee Thoughts",
    "section": "",
    "text": "Just a quick note to myself here on a realization.\n\nGame Theory and FSM\nFinite States Machines (FSM), e.g.¬†Moore Machines\nMoore‚Äôs Machines and Ashby‚Äôs transition matrices\nMoore‚Äôs machines and binary representations\nBinary representations and RLCS\nRLCS and RL\nTransition Matrices and Graphs\nGraphs & infectious processes representation (?)\nSchelling‚Äôs model & Agents evolving and making decisions in a simulated world\nSo RL, but not only!\n\nNow:\nCan I use RLCS to create optimize Agents that in turn are representations of FSMs, thereby representing rather complex decision processes, all using a simple binary representation (maybe?), and hence allowing for rather complex modelling with all the pieces of the puzzle almost already implemented?\nI need to learn how to represent FSMs as binary strings to represent them as agents compatible with the RLCS code.\nBut then‚Ä¶ What could be done?"
  },
  {
    "objectID": "posts/2025-06-09_Notes_to_Self/index2.html#sum-of-the-parts",
    "href": "posts/2025-06-09_Notes_to_Self/index2.html#sum-of-the-parts",
    "title": "Morning Coffee Thoughts",
    "section": "",
    "text": "Just a quick note to myself here on a realization.\n\nGame Theory and FSM\nFinite States Machines (FSM), e.g.¬†Moore Machines\nMoore‚Äôs Machines and Ashby‚Äôs transition matrices\nMoore‚Äôs machines and binary representations\nBinary representations and RLCS\nRLCS and RL\nTransition Matrices and Graphs\nGraphs & infectious processes representation (?)\nSchelling‚Äôs model & Agents evolving and making decisions in a simulated world\nSo RL, but not only!\n\nNow:\nCan I use RLCS to create optimize Agents that in turn are representations of FSMs, thereby representing rather complex decision processes, all using a simple binary representation (maybe?), and hence allowing for rather complex modelling with all the pieces of the puzzle almost already implemented?\nI need to learn how to represent FSMs as binary strings to represent them as agents compatible with the RLCS code.\nBut then‚Ä¶ What could be done?"
  },
  {
    "objectID": "posts/2025-06-09_Notes_to_Self/index.html",
    "href": "posts/2025-06-09_Notes_to_Self/index.html",
    "title": "Morning Coffee Thoughts",
    "section": "",
    "text": "Just a quick note to myself here on a realization.\n\nGame Theory and FSM\nFinite States Machines (FSM), e.g.¬†Moore Machines\nMoore‚Äôs Machines and Ashby‚Äôs transition matrices\nMoore‚Äôs machines and binary representations\nBinary representations and RLCS\nRLCS and RL\nTransition Matrices and Graphs\nGraphs & infectious processes representation (?)\nSchelling‚Äôs model & Agents evolving and making decisions in a simulated world\nSo RL, but not only!\n\nNow:\nCan I use RLCS to create optimize Agents that in turn are representations of FSMs, thereby representing rather complex decision processes, all using a simple binary representation (maybe?), and hence allowing for rather complex modelling with all the pieces of the puzzle almost already implemented?\nI need to learn how to represent FSMs as binary strings to represent them as agents compatible with the RLCS code.\nBut then‚Ä¶ What could be done?"
  },
  {
    "objectID": "posts/2025-06-09_Notes_to_Self/index.html#i-was-reading-about-complex-adaptive-systems",
    "href": "posts/2025-06-09_Notes_to_Self/index.html#i-was-reading-about-complex-adaptive-systems",
    "title": "Classic Computational Social Model: Schelling‚Äôs Segregation",
    "section": "",
    "text": "And I thought some of these ideas were quite interesting. I don‚Äôt know yet what to make of it all (I haven‚Äôt read it all, either). It feels some of it is related to cybernetics, feedback loops, agents-based modelling (that last one, for sure). I am not the biggest fan of sociology, but these agents ideas have me curious though.\nAnyhow, so I was reading on that topic, of modelling things from a bottoms-up perspective, with independent agents making their own decisions, and of course whether there is an emergent behaviour then is interesting.\nOne such social model is the ‚Äúclassic‚Äù computational model of Schelling (1978). I‚Äôm not a fan of the concept it illustrates (supposedly related to some concept of segregation, whichever the kind or inspiratioin‚Ä¶). But the thing is, the emergent behavior of some level of self-organization is interesting, in-as-much there is no central push here for organization, just agents preferences ‚Äúplaying out‚Äù."
  },
  {
    "objectID": "posts/2025-06-09_Notes_to_Self/index.html#the-algorithm-and-the-results",
    "href": "posts/2025-06-09_Notes_to_Self/index.html#the-algorithm-and-the-results",
    "title": "Classic Computational Social Model: Schelling‚Äôs Segregation",
    "section": "The algorithm and the results",
    "text": "The algorithm and the results\nSo the concept is pretty simple. You have ‚Äúagents‚Äù (say, people), that prefer to ‚Äúlive next to one another‚Äù if they are of the same type. What that means is up to the reader, I don‚Äôt mind.\nHere, it just means, ‚Äúgreen agents‚Äù will prefer to move if less than 30% of their neighbours are also green. ‚ÄúRed agents‚Äù are less tolerant and want at least 50% of their neighbours to be red as well.\nIf those wishes are not fulfilled, the agents will move to a random (empty) location. We then iterate a few steps of time. At each step, each agent is asked in turn whether they want to move.\nThat‚Äôs two loops, and a very simple logic. We start out with a ‚Äúrandom‚Äù world, with roughly (in this example) 33% of cells filled with green agents, 33% of red agents, 33% of empty cells.\nThat‚Äôs it.\nAnd with just the agents‚Äô preferences, you can probably already guess what will happen. But the fun part is, there will be organization, without a central policy or ruling system, and even though none of the agents prefers to have a majority of the same type of agents. And yet‚Ä¶\nHere the resulting animation of 1 run of said algorithm:"
  },
  {
    "objectID": "posts/2025-06-09_Notes_to_Self/index.html#conclusions",
    "href": "posts/2025-06-09_Notes_to_Self/index.html#conclusions",
    "title": "Classic Computational Social Model: Schelling‚Äôs Segregation",
    "section": "Conclusions",
    "text": "Conclusions\nThis was more of an interlude than anything else. I am far from done with the RLCS work. But I can‚Äôt avoid looking into more stuff.\nThe agents-based ideas, complex systems, systems theory, cybernetics, etc. is always quite fascinating to me.\nAnd so I wanted to do one exercise while I had it in mind. The book didn‚Äôt say how to implement it, but gave out enough of the concepts to easily make it into a working demo program."
  },
  {
    "objectID": "posts/2025-06-09_Notes_to_Self/index.html#references",
    "href": "posts/2025-06-09_Notes_to_Self/index.html#references",
    "title": "Classic Computational Social Model: Schelling‚Äôs Segregation",
    "section": "References",
    "text": "References\nThe book inspiring this post."
  },
  {
    "objectID": "posts/2025-06-09_Notes_to_Self/index.html#sum-of-the-parts",
    "href": "posts/2025-06-09_Notes_to_Self/index.html#sum-of-the-parts",
    "title": "Morning Coffee Thoughts",
    "section": "",
    "text": "Just a quick note to myself here on a realization.\n\nGame Theory and FSM\nFinite States Machines (FSM), e.g.¬†Moore Machines\nMoore‚Äôs Machines and Ashby‚Äôs transition matrices\nMoore‚Äôs machines and binary representations\nBinary representations and RLCS\nRLCS and RL\nTransition Matrices and Graphs\nGraphs & infectious processes representation (?)\nSchelling‚Äôs model & Agents evolving and making decisions in a simulated world\nSo RL, but not only!\n\nNow:\nCan I use RLCS to create optimize Agents that in turn are representations of FSMs, thereby representing rather complex decision processes, all using a simple binary representation (maybe?), and hence allowing for rather complex modelling with all the pieces of the puzzle almost already implemented?\nI need to learn how to represent FSMs as binary strings to represent them as agents compatible with the RLCS code.\nBut then‚Ä¶ What could be done?"
  },
  {
    "objectID": "posts/2025-06-21_RLCS_for_text/index.html",
    "href": "posts/2025-06-21_RLCS_for_text/index.html",
    "title": "RLCS for text? And a new idea‚Ä¶",
    "section": "",
    "text": "Alright so this a bit of a crazy idea.\nAt work, these past few weeks, when I had the time, I had this side project, whereby I would try to create a model to classify texts in two categories, like ‚Äúimportant vs not-important‚Äù. Sounded easy enough upfront, in my mind even a basic Na√Øve-Bayes (i.e.¬†supervised learning) classifier would easily do the trick‚Ä¶\nHow wrong I was. After some testing, and several more or less complex algorithms all failing with similarly low accuracy (and that was enough for me, I needed not check other metrics)‚Ä¶ I dug further, and I finally found some texts that were repeated across both classes. That is, yes: same identical complete text but present in the two classes. You know what they say: ‚ÄúGarbage-in, Garbage-out‚Äù. Now, to be fair, there was a reason for it, through no fault of the original/historical manual tagging! The thing just wasn‚Äôt originally meant to be used as a training dataset for a future Supervised Learning classifier, and the tags used were in fact a mess - from the perspective of this exercise, although they made sense in their original context. Anyhow‚Ä¶\nSo we went back and started a fresh new (manual) classification system (well, my colleagues did) and right away the newly classified data allowed for a 10% accuracy increase across all models/algorithms tested with it as training/testing set. And that, with a much smaller new dataset. So we were onto something, for sure.\nAnd yet, none of the algorithms still provided very good results. Part of the issue is, some of the texts in both classes are very similar. Me, I‚Äôd have a hard time deciding for some of the entries which class should apply (I‚Äôve tried, and failed, so maybe I shouldn‚Äôt ask my machine to do something I can‚Äôt manually do myself on a small sample‚Ä¶).\nAs a note, part of that classification exercise is in fact a bit of an art, maybe, and somewhat tacit information is required, experience: Hard to explain, and hence hard to manually write down in rules, which is why the ML approach was suggested in the first place (otherwise, simple rules could have worked?).\nAn example: My company works in Healthcare, so mentions of hospitals and patients would be an easy give-away, compared to news about say fashion, or transportation‚Ä¶. But most of it isn‚Äôt as clear-cut‚Ä¶"
  },
  {
    "objectID": "posts/2025-06-21_RLCS_for_text/index.html#i-have-been-busy-with-something-for-work-lately",
    "href": "posts/2025-06-21_RLCS_for_text/index.html#i-have-been-busy-with-something-for-work-lately",
    "title": "RLCS for text? And a new idea‚Ä¶",
    "section": "",
    "text": "Alright so this a bit of a crazy idea.\nAt work, these past few weeks, when I had the time, I had this side project, whereby I would try to create a model to classify texts in two categories, like ‚Äúimportant vs not-important‚Äù. Sounded easy enough upfront, in my mind even a basic Na√Øve-Bayes (i.e.¬†supervised learning) classifier would easily do the trick‚Ä¶\nHow wrong I was. After some testing, and several more or less complex algorithms all failing with similarly low accuracy (and that was enough for me, I needed not check other metrics)‚Ä¶ I dug further, and I finally found some texts that were repeated across both classes. That is, yes: same identical complete text but present in the two classes. You know what they say: ‚ÄúGarbage-in, Garbage-out‚Äù. Now, to be fair, there was a reason for it, through no fault of the original/historical manual tagging! The thing just wasn‚Äôt originally meant to be used as a training dataset for a future Supervised Learning classifier, and the tags used were in fact a mess - from the perspective of this exercise, although they made sense in their original context. Anyhow‚Ä¶\nSo we went back and started a fresh new (manual) classification system (well, my colleagues did) and right away the newly classified data allowed for a 10% accuracy increase across all models/algorithms tested with it as training/testing set. And that, with a much smaller new dataset. So we were onto something, for sure.\nAnd yet, none of the algorithms still provided very good results. Part of the issue is, some of the texts in both classes are very similar. Me, I‚Äôd have a hard time deciding for some of the entries which class should apply (I‚Äôve tried, and failed, so maybe I shouldn‚Äôt ask my machine to do something I can‚Äôt manually do myself on a small sample‚Ä¶).\nAs a note, part of that classification exercise is in fact a bit of an art, maybe, and somewhat tacit information is required, experience: Hard to explain, and hence hard to manually write down in rules, which is why the ML approach was suggested in the first place (otherwise, simple rules could have worked?).\nAn example: My company works in Healthcare, so mentions of hospitals and patients would be an easy give-away, compared to news about say fashion, or transportation‚Ä¶. But most of it isn‚Äôt as clear-cut‚Ä¶"
  },
  {
    "objectID": "posts/2025-06-21_RLCS_for_text/index.html#rlcs-for-text-then",
    "href": "posts/2025-06-21_RLCS_for_text/index.html#rlcs-for-text-then",
    "title": "RLCS for text? And a new idea‚Ä¶",
    "section": "RLCS for text, then?",
    "text": "RLCS for text, then?\nAs we (you, my reader, and myself) have learnt over the past few months, the RLCS implementation has a few shortcomings, and today two of them are of particular interest:\n\nThe binary input string format issue is going to be a bother\nThe very high importance, for performance, of the length of said binary input strings, is another one.\n\nWhy so?\nWell, free text is not great to transform to binary strings. I am however willing to do a simplified test: Generate a TF-IDF (done) of the input texts, keep the main keywords, say top‚Ä¶ 100 (rather easy), and instead of real numbers, just encode in binary (i.e.¬†a keyword (stemmed and all, cleaned up) appears or doesn‚Äôt, for a particular entry of text). That seems simple enough: just ‚Äúgrep‚Äù over new text and look for a particular keyword, and that encodes one variable, and repeat with all 100 top keywords. Here you go: Binary string of length 100.\nBut 100 variables from a TD-IDF is a brutal simplification, when the original matrix might well have 3 or 4 thousand distinct stemmed-keywords‚Ä¶ That‚Äôs a big information loss right there. (And I don‚Äôt mean to go the way of PCA and such, I could, but explainability of the results would be quite impacted, and so no, not today).\nAlso, going from real number to binary true/false is another brutal information loss. More on this one in a moment.\n‚Äú100‚Äù here is just an idea, but come to think of it, I don‚Äôt think I can do much more than, say, 300? That is, without crawling to a halt in terms of processing time. I haven‚Äôt tried yet, and with the improvements in speed of a few weeks back, who knows‚Ä¶ :D"
  },
  {
    "objectID": "posts/2025-06-21_RLCS_for_text/index.html#my-own-rosetta-stone",
    "href": "posts/2025-06-21_RLCS_for_text/index.html#my-own-rosetta-stone",
    "title": "RLCS for text? And a new idea‚Ä¶",
    "section": "My own Rosetta Stone",
    "text": "My own Rosetta Stone\nActually, this is one excuse for this entry:\n\n\n\na rosetta stone to use with RLCS\n\n\nI have been meaning to do something about the limitation with the binary input strings issue. But instead of changing the code of the RLCS (for now), what if instead I facilitated the future data scientists‚Äô efforts by offering a sort of translation engine?\nI could in fact create a function that:\n\ntakes a data frame as input (say), or even a matrix.\nencodes factors or texts into variables with binary strings. Length of said variables would depend on the number of distinct values (i.e.¬†levels of factors). Now that won‚Äôt work for pure free text, but it just might for short tags received as text columns in the DF.\nand using the function I demonstrated for the Iris dataset a few weeks back, I can ‚Äúeasily‚Äù encode real numbers into ‚Äúbins‚Äù that I can then encode with binary strings. See this entry about how that could work.\nKeep track of the above transformations and offer a ‚Äútranslator‚Äù to translate back the resulting rules learnt by the RLCS into the original format, or a readable version of it anyway.\n\nI call this the Rosetta Stone for (what I believe are) obvious reasons.\nAnd I shall create such a Rosetta Stone function (or maybe an object?) as part of the RLCS package, for sure. That should help with adoption, or so I would hope.\nBut maybe for now I focus on the simpler application. Direct binary encoding of top TF-IDF keywords, a simple binary present-or-not encoding. Simplistic, maybe too much so, but until I try‚Ä¶ I won‚Äôt know, now will I?"
  },
  {
    "objectID": "posts/2025-06-21_RLCS_for_text/index.html#testing-the-idea",
    "href": "posts/2025-06-21_RLCS_for_text/index.html#testing-the-idea",
    "title": "RLCS for text? And a new idea‚Ä¶",
    "section": "Testing the idea",
    "text": "Testing the idea\nSo I am willing to work on this approach starting this very weekend. (Maybe that will spill over the beginning of next week, but since this is actually ‚Äúfor work‚Äù, well, I don‚Äôt see an issue with merging my personal- and work-time for a few days here‚Ä¶)\nWhat I am hoping is, I can get some machine-provided rules to help with the automated classification of future texts. The texts at work are about Cybersecurity news (but that detail is not in fact important for this post). I just want to create something that will help the analysts in choosing what to worry about and what they can safely discard as noise/irrelevant pieces of news.\nOne important aspect here will be the False-Positive vs False-Negative ratio, among other things. I am also worried about data-drift, as one keyword in the overall exercise is ‚Äúnews‚Äù, which, you know, change over time.\nBut heck, this is all future issues, the first thing is to get a good enough classifier, that we shall later refine.\nAnd if I make the RLCS package (code) part of the final solution, if I actually show it can be useful for such a practical, real-world scenario, well‚Ä¶ But that‚Äôs a big ‚Äúif‚Äù."
  },
  {
    "objectID": "posts/2025-06-21_RLCS_for_text/index.html#conclusions",
    "href": "posts/2025-06-21_RLCS_for_text/index.html#conclusions",
    "title": "RLCS for text? And a new idea‚Ä¶",
    "section": "Conclusions",
    "text": "Conclusions\nNa√Øve-Bayes, SVM, Random-Forest, Lasso, heck even DNN, you name it: If the data is not good, and two classes are in fact not ‚Äúsomewhat clearly‚Äù separable (or no generalized rule will stand), then no amount of processing will create great models. I have tried (thanks to suggestion by other colleagues) more modern approaches, such as full-text embeddings (maybe my approach was simplistic: I used word2vec, with say 200 dimensions, and then averaged full entries vectors‚Ä¶ obtaining simple encodings of texts in 200-dimensional vectors, each text one point‚Ä¶ It actually kind‚Äôa worked, but still wasn‚Äôt great, and aside from clustering approaches, well, results would be harder to explain‚Ä¶ Anyhow, I digress: This wasn‚Äôt a solution. Yet :D)\nThat said, maybe a subset of good, understandable rules, while not always useful, can make a dent into the problem. And that‚Äôs what I‚Äôm hoping the application of RLCS to this text classification problem might offer: Find us some rules that will classify perfectly some of the new texts, so that an analyst can skip reading through them, and maybe move on to other news.\nThen, where the rule-set won‚Äôt actually work, the RLCS output will hopefully tell you it‚Äôs not confident (mixed subset of recommended classes) in the proposed classification, and so as an analyst you can go back to other approaches (or say manually review, tag, and save for future improvements), but at least you will have received help for the clear-cut cases, in reviewing some of the news pieces.\nSo I am hoping I can at the very least reduce the size of the problem, in the short term. And I do believe, this is something the RLCS can help with :)"
  },
  {
    "objectID": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html",
    "href": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html",
    "title": "RLCS as a package?",
    "section": "",
    "text": "I have plenty of work still, with this RLCS Package idea.\nOne of them, indeed, is to make it into‚Ä¶ A package.\nI have been postponing this for quite some time, and well, I can only stall for so long."
  },
  {
    "objectID": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html#finally-i-am-moving-forward-with-this",
    "href": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html#finally-i-am-moving-forward-with-this",
    "title": "RLCS as a package?",
    "section": "",
    "text": "I have plenty of work still, with this RLCS Package idea.\nOne of them, indeed, is to make it into‚Ä¶ A package.\nI have been postponing this for quite some time, and well, I can only stall for so long."
  },
  {
    "objectID": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html#might-actually-be-feasible",
    "href": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html#might-actually-be-feasible",
    "title": "RLCS as a package?",
    "section": "Might actually be feasible",
    "text": "Might actually be feasible\nThe thing is, I have been worried about how hard it would be. Well‚Ä¶\nI use RStudio (side note: I hope it lives as product for still a long time‚Ä¶).\nSo I ‚Äújust‚Äù created a ‚ÄúPackage Project‚Äù, and then went through some of the chapters of the ‚ÄúR Packages‚Äù book once again this morning with a coffee.\nTime has come, and so‚Ä¶ What I needed to actually get it moving was to copy my current, non-package, functional code from my (usual) RLibs/ folder to the new project‚Äôs R/ folder‚Ä¶\nThen I deleted some temporary code that shouldn‚Äôt have been there‚Ä¶\nThen I know and accept this is far from finished and I have plenty more to do still but‚Ä¶\nLong story short, I then made sure I had a clean environment, ran ‚Äúdevtools::load_all()‚Äù and ran a function I knew was stand-alone and should work (actually I ‚Äúfudged‚Äù even the new project name, but this is just a first test :D):\n\n\n\nfrom scripts to package, one step at a time"
  },
  {
    "objectID": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html#conclusions",
    "href": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html#conclusions",
    "title": "RLCS as a package?",
    "section": "Conclusions",
    "text": "Conclusions\nIt‚Äôs very early in that part of the process.\nBut while reviewing the book this morning, I realized I had actually been fairly organized following my own rules, and minimal changes would get me somewhere.\nIndeed, I have a lot more to do. For instance:\n\nwhat if I do not want to expose a function, and make it available only internally? Not sure yet about how to go about that.\nDocumentation, roxygen2? I‚Äôm not too familiar yet. Vignette? Sounds like a nice idea but, not there yet either.\nRCpp? I have just the one function that did make a difference in terms of processing speed, and it makes sense it is kept as part of the package, so I need to make sure I include it, although I‚Äôm unclear on exactly how‚Ä¶\n\nSo yes, plenty yet to do. But it looks quite‚Ä¶ Feasible? That‚Äôs good news."
  },
  {
    "objectID": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html#resources",
    "href": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html#resources",
    "title": "RLCS as a package?",
    "section": "Resources",
    "text": "Resources\nhttps://r-pkgs.org/"
  },
  {
    "objectID": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html#basically-it-works-with-kinks",
    "href": "posts/2025-07-05_RLCS_as_a_package_at_last/index.html#basically-it-works-with-kinks",
    "title": "RLCS as a package?",
    "section": "Basically, it works, with ‚Äúkinks‚Äù",
    "text": "Basically, it works, with ‚Äúkinks‚Äù\nSo it looks like it ‚Äúworks‚Äù almost as-is:\n\n\n\nworking basics in package form\n\n\nHowever the above has for now a couple of issues at least:\n\nThat‚Äôs the version WITHOUT RCpp faster version of match function\nAnd for some reason, I have to manually re-load the functions that set the print and plot for rules and population of RLCS, which‚Ä¶ Well I don‚Äôt know yet why :D\n\nBut as-is, I can actually use this ‚Äúas a package‚Äù to do supervised learning and it would in fact work, essentially."
  },
  {
    "objectID": "posts/2025-07-06_RLCS_package_test_progress/index.html",
    "href": "posts/2025-07-06_RLCS_package_test_progress/index.html",
    "title": "RLCS package: Progressing nicely",
    "section": "",
    "text": "Yesterday was the first day of testing.\nToday, I fixed 2 things I considered key.\n\nOverwriting the defaults for prints and plots\nusing the C++ component for matching (and thereby fixing a bit of parallel processing!)\n\nBoth I describe next."
  },
  {
    "objectID": "posts/2025-07-06_RLCS_package_test_progress/index.html#documentation-aside-things-move-forward",
    "href": "posts/2025-07-06_RLCS_package_test_progress/index.html#documentation-aside-things-move-forward",
    "title": "RLCS package: Progressing nicely",
    "section": "",
    "text": "Yesterday was the first day of testing.\nToday, I fixed 2 things I considered key.\n\nOverwriting the defaults for prints and plots\nusing the C++ component for matching (and thereby fixing a bit of parallel processing!)\n\nBoth I describe next."
  },
  {
    "objectID": "posts/2025-07-06_RLCS_package_test_progress/index.html#easy-or-not-so-easy-s3methods-overwrites",
    "href": "posts/2025-07-06_RLCS_package_test_progress/index.html#easy-or-not-so-easy-s3methods-overwrites",
    "title": "RLCS package: Progressing nicely",
    "section": "Easy or not so easy: S3Methods overwrites",
    "text": "Easy or not so easy: S3Methods overwrites\nI will say: It gets confusing to me, whether/how/when Roxyen2 will be key‚Ä¶ And when not?\nI know the thing works now with a mix of roxygen2\n#' @export\nprint.rlcs_rule &lt;- function(x, ...) {\n  print(paste(x$condition_string, x$action), ...)\n}\nBut also required was an EDIT to NAMESPACE (which roxygen somehow then complains about?)\nS3method(print,rlcs_rule)\nS3method(print,rlcs_population)\nS3method(plot,rlcs_population)\nBoth together, at least, do work as expected.\n\n\n\nfixed_printing_and_plotting‚Ä¶"
  },
  {
    "objectID": "posts/2025-07-06_RLCS_package_test_progress/index.html#using-c-was-not-difficult",
    "href": "posts/2025-07-06_RLCS_package_test_progress/index.html#using-c-was-not-difficult",
    "title": "RLCS package: Progressing nicely",
    "section": "Using C++ was not difficult",
    "text": "Using C++ was not difficult\nJust‚Ä¶ Put your correctly formatted .cpp file in the src/ directory, and RStudio will know how to go about it (supposing you created your RStudio project with C++ support, at least).\nAnd I know it works, because I made sure the function for matching uses it, and more importantly‚Ä¶"
  },
  {
    "objectID": "posts/2025-07-06_RLCS_package_test_progress/index.html#it-fixed-something-else",
    "href": "posts/2025-07-06_RLCS_package_test_progress/index.html#it-fixed-something-else",
    "title": "RLCS package: Progressing nicely",
    "section": "It fixed something else",
    "text": "It fixed something else\nParallelizing with %dopar% with the C++ version failed before, but with the package loaded, it now works seamlessly!\n\n\n\nparallel processing failed with C++ components until packaged"
  },
  {
    "objectID": "posts/2025-07-06_RLCS_package_test_progress/index.html#conclusion",
    "href": "posts/2025-07-06_RLCS_package_test_progress/index.html#conclusion",
    "title": "RLCS package: Progressing nicely",
    "section": "Conclusion",
    "text": "Conclusion\nMy next focus should be on clean-up and documentation. And maybe a better understanding of the Roxygen2 role and NAMESPACE (for exporting only certain functions, I believe‚Ä¶).\nBut overall, I‚Äôm back on track with the RLCS package :)"
  },
  {
    "objectID": "others/RLCS_documentation.html#images-classifier-1",
    "href": "others/RLCS_documentation.html#images-classifier-1",
    "title": "RLCS: An Introduction",
    "section": "Images Classifier",
    "text": "Images Classifier"
  },
  {
    "objectID": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html",
    "href": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html",
    "title": "Studying Nonlinear Dynamics: Day 1",
    "section": "",
    "text": "Math. More specifically, Ordinary Differential Equations. More specifically, in fact, probably things related with evolution in time. Although not only.\nI just want to really understand mathematical models used for simulating things.\nAnd ODEs are just the thing.\nPhysics, Chemistry, etc. maybe are beyond the point. Simulating interactions of‚Ä¶ Well, agents, that‚Äôs what got me curious. Or populations growth (i.e.¬†the Logistic Model‚Ä¶). That kind of things.\nAnyhow, that‚Äôs the why for today‚Äôs post."
  },
  {
    "objectID": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#i-really-want-to-understand",
    "href": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#i-really-want-to-understand",
    "title": "Studying Nonlinear Dynamics: Day 1",
    "section": "",
    "text": "Math. More specifically, Ordinary Differential Equations. More specifically, in fact, probably things related with evolution in time. Although not only.\nI just want to really understand mathematical models used for simulating things.\nAnd ODEs are just the thing.\nPhysics, Chemistry, etc. maybe are beyond the point. Simulating interactions of‚Ä¶ Well, agents, that‚Äôs what got me curious. Or populations growth (i.e.¬†the Logistic Model‚Ä¶). That kind of things.\nAnyhow, that‚Äôs the why for today‚Äôs post."
  },
  {
    "objectID": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#the-book",
    "href": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#the-book",
    "title": "Studying Nonlinear Dynamics: Day 1",
    "section": "The book",
    "text": "The book\nI have done exercises about ODE in the past, but being able to follow the math, and being able to understand the math, are, to me, two different things. My goal here is to understand, not just copy and paste.\nAfter reading some comments online, I came across this book, highly recommended in its approach to the topic:\n\n\n\nThe reference book that I hope will change things"
  },
  {
    "objectID": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#day-1",
    "href": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#day-1",
    "title": "Studying Nonlinear Dynamics: Day 1",
    "section": "‚ÄúDay 1‚Äù",
    "text": "‚ÄúDay 1‚Äù\nActually, Day 2. Day 1, I went through Chapter 1 (Overview) which confirmed this looked promising.\nSo Day 2. Chapter 2. Well, that took me the whole day, yesterday. And a hard day of thinking about the math, too. And that didn‚Äôt even include none of the whole section of exercises, either. But no one said it would be easy, hu?\nSo I did go through the whole theory of the chapter in the end, but in the middle, I made a detour of sorts, when reaching ‚Äúexample 2.2.1‚Äù. This was the provided information:\n\\[\n\\dot{x} = x^2-1\n\\]\nThat seemed innocuous enough, right? You would look for roots, fair enough: (-1, 1). Then where \\(\\dot{x} &gt; 0\\), you know the ‚Äúflow‚Äù will move to the right, and to the left when negative, so you can draw the vector field.\nObviating the ‚Äúarrows‚Äù of the vector field, I just put a ‚Äúfull dot‚Äù for stable equilibrium, and ‚Äúempty dot‚Äù for unstable equilibrium.\nIt looks like so (and that was not the part that got me stumbling). One thing I did, is code the whole thing (in R, obviously) and so here I am generating a phase portrait for that scenario:\n\n\n\n\n\n\n\n\n\nBut then something came up. See, one argument of the methods presented in the book (thus far, anyway) is to get a ‚Äúgeometric‚Äù sense of what happens with nonlinear equations that are hard to solve analytically.\nThis one example ODE was easy to ‚Äúintegrate‚Äù, so I could visualize the ‚Äúpotential function‚Äù that leads to this. That‚Äôs where I made a conceptual mistake that got me wondering what was wrong‚Ä¶\n\nJust integrate?\nWell, if you integrate the above, you get:\n\\[f(x) = x^3/3-x+C\\]\nBut when you draw that, you get quite exactly the opposite of what you‚Äôd expect. Why? Why is it, when I integrate the ODE directly, I get the opposite function of what I‚Äôd expect?\nI could see that what you want for this example, what fits nicely with the ODE, is in fact the following:\n\n\n\n\n\n\n\n\n\nThat is the same thing, but multiplied by -1.\n\n\nThe Key\n\n‚ÄúA first order ODE can be understood as gradient descent of its potential function.‚Äù\n\nThat‚Äôs what clicked for me and fixed a conceptual problem that got me worried for‚Ä¶ Hours? Until I asked a friend (thanks Omar) and at long last, it all made sense.\nSo here is the concept of the whole exercise. Let‚Äôs go back and go step-by-step. So, again, this:\n\ndraw_phase_portrait(dot_x, x_min, x_max)\n\n\n\n\n\n\n\n\nHow I can make an intuition of that phase portrait and the vector field for it is:\nWith the passing of time:\n\na particle ‚Äòlanding‚Äô near x=-1 will tend to move towards, and then stay, at x=-1.\na particle ‚Äòlanding‚Äô between x=-1 and x=1 will tend to go to the left until it stabilizes at x=-1.\na particle ‚Äòlanding‚Äô on the right of x=1 will move away from 1, and it will move ever faster.\na particle ‚Äòlanding‚Äô precisely at x=1 will stay there, but any perturbation to it, however small, will push it away from that ‚Äúequilibrium‚Äù.\n\nI could see that here: The concept I understand is, if you consider some sort of a particle (ball? flow? no matter) that falls vertically onto this next curve, see below (which you have to imagine is ‚Äúcovered in some dense fluid, say honey or similar‚Äù, thereby eliminating the possibility of overshooting a stable equilibrium because of previous horizontal inertia‚Ä¶), it will indeed behave as described above, right?\n\nplot(orig_points, type=\"l\", main=\"original function (C=0)\")\n\n\n\n\n\n\n\n\nRight? Now I had missed a step in the logic of the ODE. Because what I just described is the ‚Äúgradient descent‚Äù.\nHeck! But I know that one! Why could I not ‚Äúsee‚Äù the solution?\nGradient descent is, by definition, the negated derivative!!\nSo if I am given:\n\\[\n\\dot{x} = x^2-1\n\\]\nI should have known (!!) that the potential function corresponding to that ODE, the one that negates the gradient hereby represented, is in fact:\n\\[\n\\int -(x^2-1) = -(x^3/3-x+C)\n\\]\nThat was ALL I needed to understand to see where I had made a (conceptual) mistake.\nAnd it finally clicked when that friend gave me that key. Sometimes, you need help. I‚Äôm just glad I know a few people that can answer such questions.\nEdit: I kept getting confused by trying to ‚Äúsolve the ODE‚Äù and getting to the potential function.\nHere is the clarification I need to keep in mind, for first order ODE.\nSolving the ODE analytically means solving:\n\\[\n\\dot{x} = f(x) = dx/dt \\rightarrow\\int{{1\\over{f(x)}} dx}=\\int{dt}\n\\]\nWhich should lead to an expression \\(x(t)\\), which is related to \\(t\\) for a specifically given \\(x(0)\\).\nWhereas the potential function is given by:\n\\[\n\\dot{x} = f(x) = {-dV\\over{}dx}\n\\]\nWhich is where one can ‚Äúsee‚Äù the negative of the derivative‚Ä¶ Which was the gradient descent all along. I just didn‚Äôt grab the distinction :(\nActually, that also makes it clear that ‚Äúsolving‚Äù analytically what looked like a simple ODE is in fact a real pain‚Ä¶ And why one would prefer to use geometric approaches.\nAnyhow, I‚Äôm still not 100% clear on what the ‚Äúsolving analytically the ODE‚Äù means: I can understand how the first order ODE itself expresses a change over time for a given initial condition. Which is why I can understand the next section. But I guess I still need to wrap my head conceptually around what ‚Äúsolving it‚Äù means."
  },
  {
    "objectID": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#references",
    "href": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#references",
    "title": "Studying Nonlinear Dynamics: Day 1",
    "section": "References",
    "text": "References\n‚ÄúNonlinear Dynamics and Chaos (‚Ä¶)‚Äù, by S. Strogatz (CRC Press)\nDrawing the Slopes Field, I was initially not sure about how to draw the arrows, this helped."
  },
  {
    "objectID": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#slopes-field",
    "href": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#slopes-field",
    "title": "Studying Nonlinear Dynamics: Day 1",
    "section": "Slopes Field",
    "text": "Slopes Field\nOn to the last piece of the puzzle, visualizing the ‚Äúfield‚Äù of that equation.\nOne ‚Äúcool thing‚Äù about this whole concept is you can visualize ‚Äúall‚Äù initial positions and effect of the passing of time for the above ‚Äúpotential‚Äù equation, that is, all the possible behaviours of a ‚Äúparticle falling onto the curve‚Äù. That‚Äôs called a slopes field.\nAnd you only need the ODE (here, first order) for that!\nThe only real difficulty I had there was to draw the ‚Äúarrows‚Äù. I know it shows the ‚Äúslopes‚Äù at some given points. Which, well, is the definition of the derivative, so yes, the ODE is the key. As per each \\(x0\\), that‚Äôs easy too. And generating the positions over time, well‚Ä¶\nI used the (simple, far from ideal but sufficient for today) Euler method for getting a numerical solution for each \\(x0\\), iterating with small \\(\\Delta t\\). Then I extract a subset of each possible \\(t\\) positions, and thereby space a bit the arrows so that it can be visualized.\nOne thing I still don‚Äôt quite understand: the cos, tan, and all that trigonometry stuff required to draw the arrows. So yeah, I‚Äôll admit: I just copied that part of the code from here.\nAside from that slight difficulty of the drawing of the arrows, I‚Äôm quite happy with the outcome!\n\n\n\n\n\n\n\n\n\nYou can see what the vector field predicted!"
  },
  {
    "objectID": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#this-makes-me-happy",
    "href": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#this-makes-me-happy",
    "title": "Studying Nonlinear Dynamics: Day 1",
    "section": "This makes me happy",
    "text": "This makes me happy\nAt long last, I have a conceptual understanding of what a given (simple, first order) ODE represents, in a mathematical sense, but also as an intuition.\nThis book, I will say, is very promising‚Ä¶ But it‚Äôs also very dense with concepts to be understood. Here is me ‚Äúreading‚Äù chapter 2:\n\n\n\nDoes it make sense to underline EVERYTHING? :D\n\n\n\n\n\nWrapping my head around vector fields"
  },
  {
    "objectID": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#conclusions",
    "href": "posts/2025-07-13_Studying_NonLinear_Dynamics_FirstDay/index.html#conclusions",
    "title": "Studying Nonlinear Dynamics: Day 1",
    "section": "Conclusions",
    "text": "Conclusions\nWell, that was‚Ä¶ Challenging.\nAt the rythm of one full day of work per chapter, just to read the theory, and 13 such chapters, notwithstanding that doing some exercises might very well add another day or two for each of the chapters (at least), and considering this is not something I can do on a workday (no way, this requires waaaaay too much focus of me), so considering one such effective day per week, we‚Äôre looking at‚Ä¶ 26 weeks, at best?\nHalf a year. For this one book.\nAnd that is, if I keep the focus on that one book (that‚Äôs something I do wrong: I jump from book to book, all the time. Too many interesting things to learn, what can I say‚Ä¶)\nI should really consider reading short novels, instead :D\nOn the other hand: The satisfaction of getting the intuition behind the math is incredibly fulfilling. So maybe it‚Äôs worth it, after all‚Ä¶"
  },
  {
    "objectID": "posts/2025-07-20_NonLinearDynamics_Chapter3/index.html",
    "href": "posts/2025-07-20_NonLinearDynamics_Chapter3/index.html",
    "title": "Nonlinear Dynamics: on Bifurcations",
    "section": "",
    "text": "It turns out, back in February 2022, I published on the ‚Äúold‚Äù Blog an entry on the Logistic Map.\nAs I was reading through Chapter 2 last week, it was mentioned. This week, I browsed through Chapter 3 (that one will take time to fully follow), which discusses Bifurcations.\nAs the math was getting complicated, with ‚Äúsupercritical pitchfork‚Äù bifurcations, or ‚Äúcusp catastrophe‚Äù including an imperfection parameter and hence play with 2 independent variables (forcing us to look at things as folded surfaces in 3D).\nLong story short, it was a bit overwhelming, and so at one point I disconnected and thought I‚Äôd browse a bit through all the chapters to get a sense of what was ahead, which I did this morning.\nAnd sure enough, chapter 10 is about one-dimensional maps and it comes after the introduction of chaos with the Lorenz equations.\nWell, I‚Äôm not re-writing what I wrote some time ago, so instead I hereby reproduce the old entry, only updating pointers to images.\nBut I will say, the new book mentioned last week does put things in perspective, beyond ‚Äúreproducing‚Äù the code and math, and that is quite exactly what I wanted in the first place."
  },
  {
    "objectID": "posts/2025-07-20_NonLinearDynamics_Chapter3/index.html#new-intro",
    "href": "posts/2025-07-20_NonLinearDynamics_Chapter3/index.html#new-intro",
    "title": "Nonlinear Dynamics: on Bifurcations",
    "section": "",
    "text": "It turns out, back in February 2022, I published on the ‚Äúold‚Äù Blog an entry on the Logistic Map.\nAs I was reading through Chapter 2 last week, it was mentioned. This week, I browsed through Chapter 3 (that one will take time to fully follow), which discusses Bifurcations.\nAs the math was getting complicated, with ‚Äúsupercritical pitchfork‚Äù bifurcations, or ‚Äúcusp catastrophe‚Äù including an imperfection parameter and hence play with 2 independent variables (forcing us to look at things as folded surfaces in 3D).\nLong story short, it was a bit overwhelming, and so at one point I disconnected and thought I‚Äôd browse a bit through all the chapters to get a sense of what was ahead, which I did this morning.\nAnd sure enough, chapter 10 is about one-dimensional maps and it comes after the introduction of chaos with the Lorenz equations.\nWell, I‚Äôm not re-writing what I wrote some time ago, so instead I hereby reproduce the old entry, only updating pointers to images.\nBut I will say, the new book mentioned last week does put things in perspective, beyond ‚Äúreproducing‚Äù the code and math, and that is quite exactly what I wanted in the first place."
  },
  {
    "objectID": "posts/2025-07-20_NonLinearDynamics_Chapter3/index.html#the-original-post-reproduced",
    "href": "posts/2025-07-20_NonLinearDynamics_Chapter3/index.html#the-original-post-reproduced",
    "title": "Nonlinear Dynamics: on Bifurcations",
    "section": "The original Post, reproduced",
    "text": "The original Post, reproduced\n\nSome Context\nThis was written while I was taking a course within the scope of my last MSc, which explains the reference to the course, next."
  },
  {
    "objectID": "posts/2025-07-20_NonLinearDynamics_Chapter3/index.html#old-intro",
    "href": "posts/2025-07-20_NonLinearDynamics_Chapter3/index.html#old-intro",
    "title": "Nonlinear Dynamics: on Bifurcations",
    "section": "OLD Intro",
    "text": "OLD Intro\nSo I took a course in ‚ÄúNumerical Methods‚Äù, and (somehow) passed (I loved every bit of it, although it was HARD!).\nBut then of course, taking a course doesn‚Äôt make me ‚Äúproficient‚Äù at the subject matter. At best, it has exposed me to it and given me some confidence that I in fact can understand its concepts and solve problems ‚Äì even if I‚Äôm not too fast‚Ä¶ I simply feel there is SO MUCH MORE to it, and indeed I have several books on the subject to confirm it: I still have plenty to learn. Which is why I want to keep digging into it on my own.\nIf anything, I did get a good dose of Engineering-level Maths exposure, and I needed that (after a few years working in Cyber ‚Äì and I guess in most jobs ‚Äì one might tend to forget about matrices or how/why to differentiate equations, and many white-collar workers will simply spend their days using Outlook, PowerPoint and maybe Excel‚Ä¶).\nAnyhow, I was thrilled I took that course. And I am looking forward for more similar courses (the Master continues, for me spread over three years‚Ä¶). But while that comes, I was particularly interested in some ideas of non-linear equations, differential equations, and Chaos Theory. We did do an exercise about Lorenz‚Äôs typical differential equations, and I saw the strange-attractors and the ‚Äúbutterfly-like‚Äù drawing come to life from my own implementation (in Matlab) of these 3 simple equations, and I kind of fell in love with it.\nLet me be clear: I am not a mathematician, and I know I don‚Äôt fully grasp the ideas (far from it, in fact!). But I keep reading about it. While reading ‚ÄúChaos‚Äù by James Gleick (a reference on the subject), I came across the chapter about Ecology and the ‚ÄúLogistic Map‚Äù. And I thought: This is BEAUTIFUL.\n\n\n\n\n\nLast note for the Intro: The below has been done MANY TIMES, I am not inventing the wheel. I will just write my own R code to play around the simple equation and see its magic. I found references, particularly one that I will mention, that HEAVILY inspired this post (although I‚Äôm creating my own code ‚Äì and doing it in R), on top of the book. Please do have a look at both if you have any interest.\n\nThe Logistic Map\nSo I‚Äôm not the right person to explain it in details, but the below is basically a mathematical expression of the evolution of a supposed population (say of fish), that considers a ‚Äúgrowth rate‚Äù r, an initial population x0, and then tries to estimate the population say one year later (x1), and then the year after that (x2), etc. The following short equation ‚Äì the ‚Äúlogistic map‚Äù ‚Äì is all you need to produce the graph from the introduction!\n\\[\nx_{k+1} = rx_k (1-x_k)\n\\]\nIf you consider a limited environment, the typical example being a pond with fish, you need to consider that maybe a predator-prey thing might happen: If a population grows too much, it will deplete its resources (food, space‚Ä¶), and then will have to reduce its population so as to adapt to the available resources. This is the ‚Äú* (1-x)‚Äù part of the equation: If there is too much fish, there won‚Äôt be enough food, space or oxygen. Hence the population would decrease. The decrease would leave more resources available, so that the fish population would be able to increase again (but in cycles, not continuously, as we can suppose the fish doesn‚Äôt reproduce and die instantaneously).\nFor the sake of the example, let‚Äôs assume an initial population x0 of 0.5. (Here x is set between 0 and 1, so that things work as expected).\n\n\nBack to the graph of the introduction\nSo what is the graph¬† in the intro all about?\nIt‚Äôs called a Bifurcation Diagram. It shows the evolution of the theoretical population over several generation (each dot is one value of the population at a given generation), as described above, but with different values for the growth ratio, after say 1000 generations.\nOn the x axis, different values of our growth ratio, and on the y axis, the number of individuals in the population. For a growth ratio below 1, as can be expected, the population tends to grow smaller and smaller and hence tends to decrease to 0. That‚Äôs the left part of the blue line, after 100 generations, the population is 0 in these cases.\n\n\n\n\n\nWhat‚Äôs interesting is that for growth ratios between 1 and 3.0, the population tends to stabilize to one specific value, which is why we see how the line grows somewhat but appears as a line: For each growth ratio in that range, the population stabilizes to one value. (What you can‚Äôt really see is that, in fact, there are many dots on each position of the line, one for each generation: For a given growth ratio, the population is stable at that particular value).\n\n¬†\nFrom 3.0 to roughly 3.4, it turns out that the population oscillates between two values! That is: Either they have enough resources (population grows to a specific value) or not (the population decreases to another specific value), and each generation, it cycles to one of those two values.\nAfter that it gets weirder:\nBefore a growth of¬† ~3.45, more possible values appear, then again at ~3.54. But then the number of individuals in each generation seems to become ‚Äúrandom‚Äù, and takes any value within some ranges, which is why we don‚Äôt see ‚Äúlines‚Äù anymore, but rather clouds of points.\nWith a growth ratio of 3.999, it would seem each generation has a different number of individual, ranging basically from 0 to 1 (all possible values!), seemingly random!\n\n\nInterlude\nRandomness is important in cybersecurity. So having an equation THAT SIMPLE generating seemingly random numbers sounds appealing. It‚Äôs not much more complex than the ‚Äúlinear congruential generator‚Äú, in fact. Maybe we could compare those two?\nSo let‚Äôs implement a simplistic generator of random number:\n# Linear congruential generator\nrandom_numbers_vector &lt;- c()\nX0 &lt;- 12 # seed\na &lt;- 36\nc &lt;- 2\nm &lt;- 3583 # also, this one is prime\nmax_iterations &lt;- 200\nXi_plus_1 &lt;- function(x, current_iter = 0, max_iter = max_iterations) {\n  random_numbers_vector[current_iter+1] &lt;&lt;- x\n  if(current_iter == max_iter) return(x)\n  Xi_plus_1( (a * x + c) %% m, current_iter+1, max_iter)\n}\nrandom_numbers_vector &lt;- rep(0, max_iterations)\nXi_plus_1(X0)\n# scale to have values between 0 and 1\nrandom_numbers_vector &lt;- random_numbers_vector/max(random_numbers_vector)\nThen, for comparison, let‚Äôs generate our population values with a growth rate of 3.999, for the same amount of generations:\n# Recursive version of our Logistic Map function\nlogistic_numbers_vector &lt;- rep(0.5, max_iterations)\nmy_logistic_map3 &lt;- function(my_r, x, current_iter = 0, max_iter = max_iterations) {\n  logistic_numbers_vector[current_iter+1] &lt;&lt;- x\n  if(current_iter == max_iter) return(x)\n  my_logistic_map3(my_r, my_r * x * (1-x), current_iter+1, max_iter)\n}\nmy_logistic_map3(3.999, 0.5)\nThen let‚Äôs compare two consecutive values, from each set of generated numbers:\ncomparator_df &lt;- data.frame(generation = 0:200, \n   logistic_map_num = logistic_numbers_vector, \n   random_num = random_numbers_vector)\ncomparator_df %&gt;% pivot_longer(cols = contains(\"num\"),\n   names_to = \"generator\",\n   values_to = \"vals\") -&gt; for_plot\nggplot(for_plot[for_plot$generation %in% 30:50,], aes(x = generation, y = vals, colour = generator)) +\n  geom_line()\nAnd they seem rather ‚Äúrandom‚Äù at first sight, both of them, wouldn‚Äôt you say?\n\n\n\n\n\nMe at least, at first sight, I wouldn‚Äôt be able to tell which one is random, and which one isn‚Äôt (but is chaotic).\nBut HERE IS THE KEY: The logistic map is NOT at all random, but rather perfectly deterministic (well, to be fair, our pseudo-random number generator is deterministic too, but it behaves more as expected).\nSo how can we tell the difference? Here the ‚Äúhorse-shoe‚Äù figure makes its entrance. In the above graph, we could have thought that both generators gave back random numbers.\nBUT if we graph the values from two consecutive generations against each other (apparently that‚Äôs called a Phase Diagram), the magic appears:\ncomparator_df$logistic_map_num1 &lt;- c(comparator_df$logistic_map_num[2:nrow(comparator_df)],0)\ncomparator_df$random_num1 &lt;- c(comparator_df$random_num[2:nrow(comparator_df)],0)\n# Finally, the beauty!\nggplot(comparator_df) +\n  geom_point(aes(x = logistic_map_num, y = logistic_map_num1, colour = \"logistic_map\")) +\n  geom_point(aes(x = random_num, y = random_num1, colour = \"pseudo_random\")) + \n  scale_color_manual(values = c(\"logistic_map\" = \"blue\", \"pseudo_random\" = \"red\"))+\n  ggtitle(\"Comparing Logistic Map and Pseudo-Random Generator\")+\n  xlab(\"'Generation N'\") +\n  ylab(\"'Generation N+1'\")\n\nAnd, to me, that‚Äôs absolutely & simply wonderful. I can‚Äôt avoid but to feel impressed by the structure appearing there, from something that anyone would otherwise have reasonably assumed is complete ‚Äúchaos‚Äù‚Ä¶ (And it is üòÄ Structure in Chaos, I suppose that‚Äôs where this came from‚Ä¶ Although I haven‚Äôt read enough yet :D)\n\n\nThe code\nOf course, the above is done in R‚Ä¶ I leave the demo code on my GitHub account for reference.\nLet‚Äôs just say, it involves a few loops applying the very simple equation above, but with different growth rates, for a certain number of generations (at one point, we‚Äôre talking of about ~4 million points).\nIn the spirit of ‚ÄúNumerical Methods‚Äù, and taking advantage of our computer, we can repeat the calculations many times. In this case, the ‚Äúapply()‚Äù family of functions were not too helpful, as the equation is recurrent, so we need one result before we can calculate the next (we could probably trick it, but that‚Äôs not the point for today).\n\n\nConclusion\nI just loved this example: One of the simplest of equations (it doesn‚Äôt look that scary, does it), and yet so much can happen from one small change in one parameter!\nTo me, this is a thing of wonder. But then, maybe I am a bit of a geek for math after all (while not at all good at it).\nThere is of course MUCH MORE to it all. But I really wanted to implement this exercise for myself and reproduce it.\n\n\nReferences\nSomeone did all the above (and better/more) in Python\nThe book (I don‚Äôt earn anything from the link, buy it where you like best)\nMy code on GitHub"
  },
  {
    "objectID": "posts/2020-06-20_FirstEntry_orig/index.html#note-first-post-ever-reproduced",
    "href": "posts/2020-06-20_FirstEntry_orig/index.html#note-first-post-ever-reproduced",
    "title": "First Entry: Variables",
    "section": "NOTE: First post ever, reproduced",
    "text": "NOTE: First post ever, reproduced\nThis one was my first-ever Blog post on the original kaizen-r.com site. I‚Äôm just reproducing it here out of‚Ä¶ Nostalgia, I guess.\nAnyway‚Ä¶ Onward!"
  },
  {
    "objectID": "posts/2020-06-20_FirstEntry_orig/index.html#variables",
    "href": "posts/2020-06-20_FirstEntry_orig/index.html#variables",
    "title": "First Entry: Variables",
    "section": "Variables",
    "text": "Variables\nVariables are sometimes overseen as ‚Äúobvious‚Äù. Or at least, ‚Äúhow to name them‚Äù.\nBut after some time programming on your own, particularly in languages like R (from my experience at least), you find you could make mistakes. And after a while, you kind of re-use the same variable names, not necessarily for a reason.\nFor me, it was:\nmydf\nThat I read in a book one day (some years ago now) when I was beginning my programming in R, and it just stuck, from the very first few exercises onwards. There is nothing particularly wrong with that‚Ä¶ Or is there?\nIt‚Äôs also easy to use a variable called ‚Äúa‚Äù for a quick script. Then obviously you end up using: ‚Äúb‚Äù, ‚Äúc‚Äù‚Ä¶\na &lt;- 1\nb &lt;- 2\nc &lt;- \"a string\"\nBut wait. c() is actually a function in R (a pretty important one at that). You actually CAN use ‚Äúc‚Äù as a variable name, but maybe you shouldn‚Äôt‚Ä¶\nYes, this is a basic concept, but even then, you must be careful. I created a ‚Äúfactorial‚Äù variable not long ago to demo how to program a factorial calculation using a while loop‚Ä¶ Only to find out later on that, duh!, factorial() is also a function in R‚Ä¶ (One that wasn‚Äôt useful in that particular case as the goal was to demo while loops, but‚Ä¶ I had never used ‚Äúfactorial(n)‚Äù before in R, I simply never had had the need‚Ä¶ Anyway!)"
  },
  {
    "objectID": "posts/2020-06-20_FirstEntry_orig/index.html#so-how-do-i-name-a-variable-and-a-function-and-why-not-a-file",
    "href": "posts/2020-06-20_FirstEntry_orig/index.html#so-how-do-i-name-a-variable-and-a-function-and-why-not-a-file",
    "title": "First Entry: Variables",
    "section": "So how DO I name a variable? And a function? And (why not) a file?",
    "text": "So how DO I name a variable? And a function? And (why not) a file?\nLet‚Äôs try to answer the first one for now.\nVariables can be named almost anything in R. Well, mostly, alfa-numerics, plus dot(.) and underscore are allowed. Some exceptions apply:\n.1myvariable\nThat would not work. More to the point, I would not recommend it üôÇ\nOther considerations, besides what‚Äôs permitted or not, might be:\n\nLong variable names end up being heavy on the eye, particularly when you do things like (in R):\n\nmylongdataframename[mylongdataframename$somevariablenameinthedataframe == 1,]\nThat happens a lot. Yes, it did happen to me (more than I care to admit).\n\nYou also might want to consider that others might want to read your code, and so variables like ‚Äúx‚Äù, or ‚Äútdffwd‚Äù might not be too good either.\nThen you come across recommendations out there: camelCase, dot (.) separator, underscores‚Ä¶ Each of which have pros and cons. Too much literature on the topic to be convinced ‚Äì one way or another, with good arguments for or against any of these.\nVariable names might include object information: Is it a string, so I can end my variable name with _s? a data frame, _df? But then this goes against the idea of making variables ‚Äúshort‚Äù, or, at least, ‚Äúconcise‚Äù.\nMoreover, you might want to choose one way to go for ALL the coding languages you use (or, well, as many as possible). So in Python (usually Python is ‚Äúright there‚Äù when you talk about R), calling an object‚Äôs method uses the dot (.), which in turn might push you away from it for your day-to-day variable naming conventions.\n\nAt the end of the day, I chose the following:\n\nI use underscore (_), to separate words in the variable names. This is subject to debate out there, but that‚Äôs just my choice üôÇ\nI use all lower-case for all variable names, and for functions. This is not a recommendation, you can come up with your own system of course. I can differentiate simply because if I‚Äôm calling a function, there will be a () after it (duh!)\nI try to use real complete words, or ‚Äústems‚Äù where it makes sense, to keep it short.\nI agree with the concept of ‚Äúnouns‚Äù for variable names, and ‚Äúverbs‚Äù for functions.\nI have ‚Äúshortcuts‚Äù: For example, I use ‚Äún‚Äù for a number, usually a quantity/count."
  },
  {
    "objectID": "posts/2020-06-20_FirstEntry_orig/index.html#conclusion",
    "href": "posts/2020-06-20_FirstEntry_orig/index.html#conclusion",
    "title": "First Entry: Variables",
    "section": "Conclusion",
    "text": "Conclusion\nAs you can see, this post is extremely ‚Äúbasic‚Äù in concept.¬†\nBut I found that this kind of details, overtime, can impact your code, how readable it is, how fast you can program‚Ä¶ How much you have to think about how to name a variable is surprisingly full of impact on the resulting code. And maybe it is, partly at least, an art, more than science. Either way, every programmer has had these concepts in mind at some point. Those that have thought about it might be, arguably, one step ahead. ‚ÄúThe devil lies in the details‚Äù, or so they say.\nTO BE CONTINUED‚Ä¶"
  },
  {
    "objectID": "posts/2020-06-20_FirstEntry_orig/index.html#references",
    "href": "posts/2020-06-20_FirstEntry_orig/index.html#references",
    "title": "First Entry: Variables",
    "section": "References",
    "text": "References\nhttps://www.r-bloggers.com/r-code-best-practices/\nhttps://style.tidyverse.org/syntax.html#object-names\nhttps://stackoverflow.com/questions/1944910/what-is-your-preferred-style-for-naming-variables-in-r\nSomewhat related references:\nhttps://www.r-bloggers.com/r-best-practices-r-you-writing-the-r-way/\nhttps://google.github.io/styleguide/Rguide.html"
  },
  {
    "objectID": "posts/2024-09-29_Cholesky_Matrix/index.html",
    "href": "posts/2024-09-29_Cholesky_Matrix/index.html",
    "title": "Cholesky‚Äôs matrix decomposition and where not to use it",
    "section": "",
    "text": "Cholesky Decomposition to compress info of some matrices\n\n\n\n\nAs I was asked to talk about AI for Cybersecurity, I have been thinking about it for a few weeks, refreshing concepts, taking some notes, gathering some ideas, sources‚Ä¶\nAnd one thing I keep coming back to is: how central matrices are to AI and Computational Engineering. They just are all over the place, the moment you start considering Artificial Neural Networks (and of course, that applies to DL‚Ä¶). Come to think of it, Word2Vec for words embedding learning is a NeuralNet. (I‚Äôm only mentioning this because Word Embeddings are important for LLM, and it seems everyone only wants to hear about LLMs lately‚Ä¶ Not that I agree.)\nAnd then you would know Matrices calculations are important nowadays if it were only for the recent NVidia stock price evolution. GPUs are great at one thing: Multiplying matrices.\nSo yeah, I‚Äôve been thinking about the importance of matrices‚Ä¶\n\n\n\nAs you know, I‚Äôm an enthusiast of Graph Theory.\nLong story short, one option to represent simple graphs is the ‚ÄúAdjacency Matrix‚Äù, whereby one sets a number (usually 1) in position a_ij of the matrix, representing there is an edge between points i and j. This is a square matrix. For an undirected graph, convention (and related calculations) expect that matrix to be symmetrical.\n\\[\na_{ij} = a_{ji}\n\\]\nWell, this is cool, but then the square matrix has a lot of repeated info, doesn‚Äôt it? And that‚Äôs inefficient for, say, storage purposes. Maybe.\n(Lists of nodes + adjacency lists, say as those used in the Pajek Net file formats, are better suited than adjacency matrices, particularly for sparse ones, if storage is actually an issue‚Ä¶ But that‚Äôs a different topic).\nWouldn‚Äôt it be cool to be able to ‚Äúcompress‚Äù the symmetric matrix?\n\n\n\nI was reading random pages of a book on Numerical Methods, looking for inspiration for a Blog entry. I ended up on an explanation of the Cholesky Matrix Decomposition.\nLong story short, at first I only read the first paragraph, which focused on symmetric matrices (like, as noted, those of a simple undirected graph!). You could see how I got enthused upfront‚Ä¶\nThen I kept reading, through the math, until I found a problem‚Ä¶ The matrix needs to be ‚Äúdefinite positive‚Äù?!\n\n\n\nI unwittingly reviewed a bit of spectral theory üòÄ\nSo OK, for a matrix to be definite positive, it needs to fullfil:\n\\[\nX^T[A]X &gt; 0, \\forall{X}\\neq0,X\\in\\mathbb{R}\n\\]¬†\nWhich is to say, for any vector X of real numbers different from vector {0}, the definite positive square matrix A satisfies the above.\nNow on to why it won‚Äôt work for an Adjacency matrix of a simple connected graph:\nFirst, with no loops, well, diagonal elements of our matrix will be 0. Which means, the trace (tr(A)) will be‚Ä¶ zero. Which is not a good sign.\nIndeed, we know that to be definite positive, an equivalent here is that our eigenvalues need to all be (strictly) positive. The trace being the sum of said eigenvalues‚Ä¶ If it is zero indeed, then that‚Äôs already confirming that we cannot apply the chosen algorithm üôÅ\nAnd if there were loop? Well, then we‚Äôd still need to verify for *strictly* positive eigenvalues, as the matrix could still be rank deficient (and so have one or more eigenvalue = 0).\n\nSide note (completely side-tracking things by now‚Ä¶ Just because I was on the topic of matrix decomposition), this is cool: For symmetric matrices, you can simply use the eigenvectors and eigenvalues to reconstruct the matrix.\n\nMoving on.\n\n\n\nRight, so I can‚Äôt apply this algorithm to simple undirected graphs‚Äô Adjacency Matrices. Pity. That‚Äôs not to say it‚Äôs completely useless, though.\nI‚Äôll go ahead and implement from pseudo-code into R the algorithm (which is, incidentally, already in base R with the function chol(), of course, mind you).\nI also use the example provided.\nA &lt;- matrix(c(6, 15, 55, 15, 55, 225, 55, 225, 979), nrow=3)\nmy_chol &lt;- function(mat_A) {\n  ## I should check for definite positive properties here...\n  mat_L &lt;- matrix(rep(0, length(mat_A)), nrow=nrow(mat_A))\n  for(k in 1:nrow(mat_A)) {\n    t_sum &lt;- 0\n    if(k &gt; 1) {\n      for(i in 1:(k-1)) {\n        t_sum &lt;- 0 \n        if(i &gt; 1) { \n          for(j in 1:(i-1)) {\n            t_sum &lt;- t_sum + mat_L[i,j]*mat_L[k, j] \n          }\n        }\n        mat_L[k, i] &lt;- (mat_A[k, i] - t_sum) / mat_L[i, i]\n      }\n      t_sum &lt;- 0\n      for(j in 1:(k-1)) {\n        t_sum &lt;- t_sum + mat_L[k, j]^2\n      }\n    }\n    mat_L[k , k] &lt;- sqrt(mat_A[k , k] - t_sum)\n  }\n  mat_L\n}\nall.equal(my_chol(A)%*%t(my_chol(A)), A)\nAnd this is it. Not very impressive with a small matrix, but for a large (positive definite symmetric) matrix, this could potentially save some resources.\n\n\n\nThis was a weird entry. I know. I thought it would come out different, but it didn‚Äôt.\nAlso, it‚Äôs mostly useless for the topic of AI, as it is, I am well aware. But that wasn‚Äôt really a goal anyway. For one brief moment, I thought there‚Äôd be a cool way, the Cholesky Decomposition, to compress info of adjacency matrices, which‚Ä¶ Most often, doesn‚Äôt apply.\nWell. I learnt something, I guess.\n\n\n\nMatrix Trace\nMatrix Rank\nDefinite Positive Matrices\nChapter 11 of ‚ÄúNumerical Methods for Engineers‚Äù, S. Chapra, R. Canale (Ed. Mc Graw Hill Education) (No link is affiliated)"
  },
  {
    "objectID": "posts/2024-09-29_Cholesky_Matrix/index.html#its-been-a-while-since-i-last-produced-anything-for-the-blog.-so-ill-start-easy-or-so-i-thought-initially.",
    "href": "posts/2024-09-29_Cholesky_Matrix/index.html#its-been-a-while-since-i-last-produced-anything-for-the-blog.-so-ill-start-easy-or-so-i-thought-initially.",
    "title": "Cholesky‚Äôs matrix decomposition and where not to use it",
    "section": "",
    "text": "Cholesky Decomposition to compress info of some matrices\n\n\n\n\nAs I was asked to talk about AI for Cybersecurity, I have been thinking about it for a few weeks, refreshing concepts, taking some notes, gathering some ideas, sources‚Ä¶\nAnd one thing I keep coming back to is: how central matrices are to AI and Computational Engineering. They just are all over the place, the moment you start considering Artificial Neural Networks (and of course, that applies to DL‚Ä¶). Come to think of it, Word2Vec for words embedding learning is a NeuralNet. (I‚Äôm only mentioning this because Word Embeddings are important for LLM, and it seems everyone only wants to hear about LLMs lately‚Ä¶ Not that I agree.)\nAnd then you would know Matrices calculations are important nowadays if it were only for the recent NVidia stock price evolution. GPUs are great at one thing: Multiplying matrices.\nSo yeah, I‚Äôve been thinking about the importance of matrices‚Ä¶\n\n\n\nAs you know, I‚Äôm an enthusiast of Graph Theory.\nLong story short, one option to represent simple graphs is the ‚ÄúAdjacency Matrix‚Äù, whereby one sets a number (usually 1) in position a_ij of the matrix, representing there is an edge between points i and j. This is a square matrix. For an undirected graph, convention (and related calculations) expect that matrix to be symmetrical.\n\\[\na_{ij} = a_{ji}\n\\]\nWell, this is cool, but then the square matrix has a lot of repeated info, doesn‚Äôt it? And that‚Äôs inefficient for, say, storage purposes. Maybe.\n(Lists of nodes + adjacency lists, say as those used in the Pajek Net file formats, are better suited than adjacency matrices, particularly for sparse ones, if storage is actually an issue‚Ä¶ But that‚Äôs a different topic).\nWouldn‚Äôt it be cool to be able to ‚Äúcompress‚Äù the symmetric matrix?\n\n\n\nI was reading random pages of a book on Numerical Methods, looking for inspiration for a Blog entry. I ended up on an explanation of the Cholesky Matrix Decomposition.\nLong story short, at first I only read the first paragraph, which focused on symmetric matrices (like, as noted, those of a simple undirected graph!). You could see how I got enthused upfront‚Ä¶\nThen I kept reading, through the math, until I found a problem‚Ä¶ The matrix needs to be ‚Äúdefinite positive‚Äù?!\n\n\n\nI unwittingly reviewed a bit of spectral theory üòÄ\nSo OK, for a matrix to be definite positive, it needs to fullfil:\n\\[\nX^T[A]X &gt; 0, \\forall{X}\\neq0,X\\in\\mathbb{R}\n\\]¬†\nWhich is to say, for any vector X of real numbers different from vector {0}, the definite positive square matrix A satisfies the above.\nNow on to why it won‚Äôt work for an Adjacency matrix of a simple connected graph:\nFirst, with no loops, well, diagonal elements of our matrix will be 0. Which means, the trace (tr(A)) will be‚Ä¶ zero. Which is not a good sign.\nIndeed, we know that to be definite positive, an equivalent here is that our eigenvalues need to all be (strictly) positive. The trace being the sum of said eigenvalues‚Ä¶ If it is zero indeed, then that‚Äôs already confirming that we cannot apply the chosen algorithm üôÅ\nAnd if there were loop? Well, then we‚Äôd still need to verify for *strictly* positive eigenvalues, as the matrix could still be rank deficient (and so have one or more eigenvalue = 0).\n\nSide note (completely side-tracking things by now‚Ä¶ Just because I was on the topic of matrix decomposition), this is cool: For symmetric matrices, you can simply use the eigenvectors and eigenvalues to reconstruct the matrix.\n\nMoving on.\n\n\n\nRight, so I can‚Äôt apply this algorithm to simple undirected graphs‚Äô Adjacency Matrices. Pity. That‚Äôs not to say it‚Äôs completely useless, though.\nI‚Äôll go ahead and implement from pseudo-code into R the algorithm (which is, incidentally, already in base R with the function chol(), of course, mind you).\nI also use the example provided.\nA &lt;- matrix(c(6, 15, 55, 15, 55, 225, 55, 225, 979), nrow=3)\nmy_chol &lt;- function(mat_A) {\n  ## I should check for definite positive properties here...\n  mat_L &lt;- matrix(rep(0, length(mat_A)), nrow=nrow(mat_A))\n  for(k in 1:nrow(mat_A)) {\n    t_sum &lt;- 0\n    if(k &gt; 1) {\n      for(i in 1:(k-1)) {\n        t_sum &lt;- 0 \n        if(i &gt; 1) { \n          for(j in 1:(i-1)) {\n            t_sum &lt;- t_sum + mat_L[i,j]*mat_L[k, j] \n          }\n        }\n        mat_L[k, i] &lt;- (mat_A[k, i] - t_sum) / mat_L[i, i]\n      }\n      t_sum &lt;- 0\n      for(j in 1:(k-1)) {\n        t_sum &lt;- t_sum + mat_L[k, j]^2\n      }\n    }\n    mat_L[k , k] &lt;- sqrt(mat_A[k , k] - t_sum)\n  }\n  mat_L\n}\nall.equal(my_chol(A)%*%t(my_chol(A)), A)\nAnd this is it. Not very impressive with a small matrix, but for a large (positive definite symmetric) matrix, this could potentially save some resources.\n\n\n\nThis was a weird entry. I know. I thought it would come out different, but it didn‚Äôt.\nAlso, it‚Äôs mostly useless for the topic of AI, as it is, I am well aware. But that wasn‚Äôt really a goal anyway. For one brief moment, I thought there‚Äôd be a cool way, the Cholesky Decomposition, to compress info of adjacency matrices, which‚Ä¶ Most often, doesn‚Äôt apply.\nWell. I learnt something, I guess.\n\n\n\nMatrix Trace\nMatrix Rank\nDefinite Positive Matrices\nChapter 11 of ‚ÄúNumerical Methods for Engineers‚Äù, S. Chapra, R. Canale (Ed. Mc Graw Hill Education) (No link is affiliated)"
  },
  {
    "objectID": "posts/2024-10-12_Testing_LLM_Apple/index.html",
    "href": "posts/2024-10-12_Testing_LLM_Apple/index.html",
    "title": "Testing LLMs locally on basic Apple Silicon",
    "section": "",
    "text": "I have learned since. But back in October 2024, I did run the following test.\nNowadays, I prefer Meta‚Äôs OLlama for its simplicity - although I still haven‚Äôt tested it on Apple Silicon, that‚Äôs true too. (And I‚Äôm no fan of Meta, but OLlama is easy to use, runs locally, and my tests thus far were quite good. You have to admit the good, sometimes, too).\nThis entry is reproduced from the ‚Äúold‚Äù blog."
  },
  {
    "objectID": "posts/2024-10-12_Testing_LLM_Apple/index.html#new-intro",
    "href": "posts/2024-10-12_Testing_LLM_Apple/index.html#new-intro",
    "title": "Testing LLMs locally on basic Apple Silicon",
    "section": "",
    "text": "I have learned since. But back in October 2024, I did run the following test.\nNowadays, I prefer Meta‚Äôs OLlama for its simplicity - although I still haven‚Äôt tested it on Apple Silicon, that‚Äôs true too. (And I‚Äôm no fan of Meta, but OLlama is easy to use, runs locally, and my tests thus far were quite good. You have to admit the good, sometimes, too).\nThis entry is reproduced from the ‚Äúold‚Äù blog."
  },
  {
    "objectID": "posts/2024-10-12_Testing_LLM_Apple/index.html#old-intro",
    "href": "posts/2024-10-12_Testing_LLM_Apple/index.html#old-intro",
    "title": "Testing LLMs locally on basic Apple Silicon",
    "section": "OLD Intro",
    "text": "OLD Intro\nFair enough, I keep criticizing the GenAI hype. But in order to criticize something, I guess I believe I have to have SOME kind of understanding of what happens (silly me I guess for thinking like that).\nAfter brushing up on AI these past few weeks, mostly through readings, including what others might consider the ‚Äúless fun stuff‚Äù (which I personally find fascinating, though) such as ‚Äúwords embeddings‚Äù, transformers architecture (still not 100% clear on these), Recurrent NeuralNets, ‚ÄúWord2Vec‚Äù (very cool!), it was time I moved on to the actual fun: Testing an LLM.\nAnd I have had a worry: Being a MacBook (Air, M1) user, is there any way I can test using an LLM and make it work locally?\nThere sure is. I did it the other day."
  },
  {
    "objectID": "posts/2024-10-12_Testing_LLM_Apple/index.html#mlx-lm",
    "href": "posts/2024-10-12_Testing_LLM_Apple/index.html#mlx-lm",
    "title": "Testing LLMs locally on basic Apple Silicon",
    "section": "MLX-LM",
    "text": "MLX-LM\nThat‚Äôs the name for it. Think Apple‚Äôs version of things to use its GPU for (in this case) GenAI (more generally: MLX is for ML‚Ä¶ Duh!).\nJust a few things to set up. This post is not about me being clever or coding much, instead I‚Äôm just testing the thing. I just want to KNOW it works. No NVidia GPU, no crazy resources, no Internet connection‚Ä¶\nAnd to achieve that, I only had to follow a mix of a couple of resources (see Resources section).\nWith a Python3 venv, some pip, the mlx-ml, a HuggingFace account (for login, and then to download Mistral-7B), and of course a MacBook Air (M1, in my case), I just was able to generate 100 tokens out of a simple prompt, WHILE OFFLINE:\n\n\n\nAsking a local LLM to answer a simple question\n\n\nAnd that‚Äôs about it!"
  },
  {
    "objectID": "posts/2024-10-12_Testing_LLM_Apple/index.html#conclusion",
    "href": "posts/2024-10-12_Testing_LLM_Apple/index.html#conclusion",
    "title": "Testing LLMs locally on basic Apple Silicon",
    "section": "Conclusion",
    "text": "Conclusion\nAll that is not to say my laptop is well suited for any of this. I‚Äôd have to consider training things a bit to learn more, just ‚Äúgenerating answers‚Äù is not a use-case that would justify worrying so much. But I really wanted to try it for myself, and make it work OFFLINE (which to me is very important, go figure). There would be much more to this, but it‚Äôs a good start, and I‚Äôll leave it at that for now.\n\nResources\nIt started here (But then me signing licenses for this one test was silly, so I moved away from Llama-3-8B‚Ä¶)\nThis resource made the day\nAnd HuggingFace is COOL. I read a bit through this, for instance."
  },
  {
    "objectID": "posts/2023-11-07_Interlude/index.html",
    "href": "posts/2023-11-07_Interlude/index.html",
    "title": "Interlude",
    "section": "",
    "text": "But then‚Ä¶ I finished this book, and I liked it, and I thought I would recommend it.\n\nI don‚Äôt know how something not at the level of ‚Äúintroduction‚Äù would look like, but this was just sooo in line with my MSc dissertation‚Ä¶ Funny I hadn‚Äôt read it before!\nAlso, code in there is in Python, I‚Äôm sure many would appreciate that.\nSo there. A book recommendation."
  },
  {
    "objectID": "posts/2023-11-07_Interlude/index.html#i-said-i-wouldnt-code-and-i-didnt.",
    "href": "posts/2023-11-07_Interlude/index.html#i-said-i-wouldnt-code-and-i-didnt.",
    "title": "Interlude",
    "section": "",
    "text": "But then‚Ä¶ I finished this book, and I liked it, and I thought I would recommend it.\n\nI don‚Äôt know how something not at the level of ‚Äúintroduction‚Äù would look like, but this was just sooo in line with my MSc dissertation‚Ä¶ Funny I hadn‚Äôt read it before!\nAlso, code in there is in Python, I‚Äôm sure many would appreciate that.\nSo there. A book recommendation."
  },
  {
    "objectID": "posts/2024-06-15_MSC_Thesis_delivered/index.html",
    "href": "posts/2024-06-15_MSC_Thesis_delivered/index.html",
    "title": "MSC Thesis delivered",
    "section": "",
    "text": "In the old blog, I did comment/discuss a lot about the latest Master Thesis I was working on.\nInstead of reproducing every single entry, I thought I‚Äôd just link to the resulting paper, and reproduce the last entry on the topic.\nHere goes:\nThesis Link\n\n\n\n\n\nAnd the last Blog entry said‚Ä¶"
  },
  {
    "objectID": "posts/2024-06-15_MSC_Thesis_delivered/index.html#from-the-old-blog",
    "href": "posts/2024-06-15_MSC_Thesis_delivered/index.html#from-the-old-blog",
    "title": "MSC Thesis delivered",
    "section": "",
    "text": "In the old blog, I did comment/discuss a lot about the latest Master Thesis I was working on.\nInstead of reproducing every single entry, I thought I‚Äôd just link to the resulting paper, and reproduce the last entry on the topic.\nHere goes:\nThesis Link\n\n\n\n\n\nAnd the last Blog entry said‚Ä¶"
  },
  {
    "objectID": "posts/2024-06-15_MSC_Thesis_delivered/index.html#old-entry",
    "href": "posts/2024-06-15_MSC_Thesis_delivered/index.html#old-entry",
    "title": "MSC Thesis delivered",
    "section": "Old entry",
    "text": "Old entry\n‚ÄúIt‚Äôs done.\nThe thesis was successfully defended.\nIt has been a great almost 3 years. I‚Äôve learnt many things, re-discovered others. It has opened up new topics to dive in.\nAnd now comes a period I should use to slow down a bit, as the last few weeks have been a bit intense.\nBut I very much liked this Master and if anything, I would take more courses of it if I could‚Ä¶\nAnyhow. This is the end of a 3 years studies period, and I will start thinking more seriously about what‚Äôs next. In a few weeks.\nIn the meantime, I will read, I have lots of reading in the backlog. (Already started üòå)‚Äù"
  },
  {
    "objectID": "posts/2023-08-06_GA_2_3/index.html",
    "href": "posts/2023-08-06_GA_2_3/index.html",
    "title": "Optimization (4/n): Genetic Algorithm(s) (2/2)",
    "section": "",
    "text": "This week, I‚Äôll make it short, and instead of boring with code and explanations, I thought I‚Äôd just show an example output‚Ä¶\n\n\nThis is a genetic algorithm in action: A population ‚Äúevolves‚Äù (reproduces, ‚Äúselection of the fittest‚Äù, iterate) towards an objective. Complexity of many local minima don‚Äôt seem to be an issue for this algorithm. The below shows population over 5 frames extracted from 50 generations at regular intervals.\n\n\n\n\nThis example is quite simple, really. I have yet to implement ‚Äúmutations‚Äù, and I chose one of many possible mixes of parents selection, as well as the most simple crossing of parents to create children.\nBut it still works üôÇ"
  },
  {
    "objectID": "posts/2023-08-06_GA_2_3/index.html#intro",
    "href": "posts/2023-08-06_GA_2_3/index.html#intro",
    "title": "Optimization (4/n): Genetic Algorithm(s) (2/2)",
    "section": "",
    "text": "This week, I‚Äôll make it short, and instead of boring with code and explanations, I thought I‚Äôd just show an example output‚Ä¶\n\n\nThis is a genetic algorithm in action: A population ‚Äúevolves‚Äù (reproduces, ‚Äúselection of the fittest‚Äù, iterate) towards an objective. Complexity of many local minima don‚Äôt seem to be an issue for this algorithm. The below shows population over 5 frames extracted from 50 generations at regular intervals.\n\n\n\n\nThis example is quite simple, really. I have yet to implement ‚Äúmutations‚Äù, and I chose one of many possible mixes of parents selection, as well as the most simple crossing of parents to create children.\nBut it still works üôÇ"
  },
  {
    "objectID": "posts/2023-07-29_GA_1_3/index.html",
    "href": "posts/2023-07-29_GA_1_3/index.html",
    "title": "Optimization (4/n): Genetic Algorithm(s) (1/m)",
    "section": "",
    "text": "This time around, I keep going studying and implementing optimization algorithms, based on the already mentioned reference book, and I want to work on a slightly different family of these, now a full blown ‚Äúmetaheuristic‚Äù in fact, called ‚ÄúGenetic Algorithms‚Äù.\nAs this one is a bit different, I will take my time, and go bit by bit. So today is about ‚Äúsetting things up‚Äù (mostly).\n\n\nThis time, we will try to minimize this function (see visualization), which would be quite hard for (say) Gradient Descent (it most probably wouldn‚Äôt find the global minimum at all).\n \n\n\n\nAs usual, I am not inventing anything. Not only am I using a book for reference pseudo-code, but of course these algorithms are documented all over, say for instance in Wikipedia.\nOverall, the idea is to start with a ‚Äúpopulation of individuals‚Äù (each representing a possible solution in the search space). Then individuals are selected as parents, have one or more children, which can also suffer (in theory, rarely) mutations (which allows to explore more of the search space).\nThis leaves us with a bigger population of individuals. At this point, we stop concerning ourselves with parents or childrens and consider them all as individuals.\nA new ‚Äúgeneration‚Äù is then ‚Äúselected‚Äù (in reference to ‚Äúnatural selection‚Äù) using (normally) the fittest individuals (e.g.¬†minimum values) from that bigger pool of individuals, and then the process can start anew.\nThe selection of parents can be done in several ways, the mix of parents ‚Äúgenes‚Äù into each child can follow some process (usually varying say the proportion & redistribution of each progenitor‚Äôs genes), and mutations can happen more or less often (through a parameter, typically).\nThe algorithm stops when it ‚Äúconverges‚Äù (or after a set number of generations), whereby the best individual in the final generation is representing the best found solution (hopefully, a global minimum, in our scenario).\n\n\n\nAlthough there are alternatives, the simplest approach is to use ‚Äúbinary genes‚Äù. As numbers can ‚Äì obviously ‚Äì be binary encoded, all we have to do is encode the search space (min/max ranges for say x, y coordinates) so that given a number of bits per coordinate, any value stays in range, and any individual will be a binary representation of two (in our example) such coordinates.\nAs the fitness corresponds to a real value, we then just need to have a ‚Äúdecoder‚Äù of the binary values to evaluate the objective function to minimize using real numbers.\nAt this stage of the development, I have ensured I can generate the ‚Äúsearch space‚Äù visually (similar to recent blog entries), but also that I can generate such individuals, and maybe even a ‚Äúgeneration‚Äù.\n\n\n\n100 individuals, one generation\n\n\n\n\n\nThis is all provided in my GitHub.\nAs this is a bit ‚Äúmessier‚Äù with more concepts than most recent entries, I chose to create for myself an RStudio Project, whereby I can then source files with relative paths‚Ä¶ And so the code for today requires two files (*_v001.R).\n\n\n\nWell, in upcoming entry/ies, I shall code for selecting parents, generating children, mutation, and iteration across a number of generations, thereby in principle closing the loop on this particular topic of Genetic Algorithms applied to optimization."
  },
  {
    "objectID": "posts/2023-07-29_GA_1_3/index.html#intro",
    "href": "posts/2023-07-29_GA_1_3/index.html#intro",
    "title": "Optimization (4/n): Genetic Algorithm(s) (1/m)",
    "section": "",
    "text": "This time around, I keep going studying and implementing optimization algorithms, based on the already mentioned reference book, and I want to work on a slightly different family of these, now a full blown ‚Äúmetaheuristic‚Äù in fact, called ‚ÄúGenetic Algorithms‚Äù.\nAs this one is a bit different, I will take my time, and go bit by bit. So today is about ‚Äúsetting things up‚Äù (mostly).\n\n\nThis time, we will try to minimize this function (see visualization), which would be quite hard for (say) Gradient Descent (it most probably wouldn‚Äôt find the global minimum at all).\n \n\n\n\nAs usual, I am not inventing anything. Not only am I using a book for reference pseudo-code, but of course these algorithms are documented all over, say for instance in Wikipedia.\nOverall, the idea is to start with a ‚Äúpopulation of individuals‚Äù (each representing a possible solution in the search space). Then individuals are selected as parents, have one or more children, which can also suffer (in theory, rarely) mutations (which allows to explore more of the search space).\nThis leaves us with a bigger population of individuals. At this point, we stop concerning ourselves with parents or childrens and consider them all as individuals.\nA new ‚Äúgeneration‚Äù is then ‚Äúselected‚Äù (in reference to ‚Äúnatural selection‚Äù) using (normally) the fittest individuals (e.g.¬†minimum values) from that bigger pool of individuals, and then the process can start anew.\nThe selection of parents can be done in several ways, the mix of parents ‚Äúgenes‚Äù into each child can follow some process (usually varying say the proportion & redistribution of each progenitor‚Äôs genes), and mutations can happen more or less often (through a parameter, typically).\nThe algorithm stops when it ‚Äúconverges‚Äù (or after a set number of generations), whereby the best individual in the final generation is representing the best found solution (hopefully, a global minimum, in our scenario).\n\n\n\nAlthough there are alternatives, the simplest approach is to use ‚Äúbinary genes‚Äù. As numbers can ‚Äì obviously ‚Äì be binary encoded, all we have to do is encode the search space (min/max ranges for say x, y coordinates) so that given a number of bits per coordinate, any value stays in range, and any individual will be a binary representation of two (in our example) such coordinates.\nAs the fitness corresponds to a real value, we then just need to have a ‚Äúdecoder‚Äù of the binary values to evaluate the objective function to minimize using real numbers.\nAt this stage of the development, I have ensured I can generate the ‚Äúsearch space‚Äù visually (similar to recent blog entries), but also that I can generate such individuals, and maybe even a ‚Äúgeneration‚Äù.\n\n\n\n100 individuals, one generation\n\n\n\n\n\nThis is all provided in my GitHub.\nAs this is a bit ‚Äúmessier‚Äù with more concepts than most recent entries, I chose to create for myself an RStudio Project, whereby I can then source files with relative paths‚Ä¶ And so the code for today requires two files (*_v001.R).\n\n\n\nWell, in upcoming entry/ies, I shall code for selecting parents, generating children, mutation, and iteration across a number of generations, thereby in principle closing the loop on this particular topic of Genetic Algorithms applied to optimization."
  },
  {
    "objectID": "posts/2023-07-16_GradientDescent/index.html",
    "href": "posts/2023-07-16_GradientDescent/index.html",
    "title": "Optimization (2/n): Gradient Descent",
    "section": "",
    "text": "Intro\nSo I keep going with the reference book for a while. BTW, the book can be found here (NOT a referral or anything, I don‚Äôt make money out of this, it‚Äôs just a pointer to one place you can find it). And yes, the book is in Spanish‚Ä¶\nSo one of the first algorithms (beyond ‚Äúrandom search‚Äù, I guess) is the well known Gradient Descent. Let‚Äôs implement that in R.\n\n\nR-based Gradient Descent for bi-variate function\nThe code for today can be found here.\nThe Gradient Descent method (to look for a minimum value, say) is fine for mono-modal (convex) functions that are twice differentiable (in theory at least), in spite of being reasonably slow in converging.\nA numerical method is easy to implement in this case. Let‚Äôs have a look at what it looks like:\n\n\n\nGradient Descent Contour\n\n\n\n\n\nGradient Descent Perspective\n\n\nWhy is Gradient Method important, you ask? Well, at least for what I learnt some time ago, let‚Äôs just say it‚Äôs used in ML for instance to tune neural network through back-propagation. And of course that‚Äôs just one example (but a very common application these days). One could also use it to minimize (or well, maximize) any compatible functions, of course, and so this has application in Operations Research at large, obviously.\n\n\nOne draw-back\nOne of the limitations of the Gradient Descent method is that it works fine for mono-modal functions (in the search space at least, that is), but it can‚Äôt avoid falling into local minima if the starting point goes in that direction‚Ä¶\n\n\n\nFalling in local minima\n\n\n\n\nNext time\nI‚Äôll try and implement the next proposed algorithm, ‚ÄúSimulated Annealing‚Äù, that helps with looking for global minima."
  },
  {
    "objectID": "posts/2023-07-23_Simulated_Annealing/index.html",
    "href": "posts/2023-07-23_Simulated_Annealing/index.html",
    "title": "Optimization (3/n): Simulated Annealing",
    "section": "",
    "text": "Continuing with this simple ‚Äúseries‚Äù (see here and here), I implement the next algorithm proposed by the reference book, but in R.\nThis time around, it‚Äôs the turn of ‚ÄúSimulated Annealing‚Äù.\n\n\nI like how the concept of molecules excitement and temperatures is used for this algorithm. All in all, it‚Äôs a bit different from Gradient Descent in that it allows to keep going with WORSE results in some cases, proportional to the progress, so that it explores better the overall range of possible values. This is supposed to ensure that this algorithm will in fact avoid falling in local minimum, and so this is better for situations where there are multiple minima.\nThat said, here the code for this algorithm. I haven‚Äôt made any effort to make this implementation ‚Äúgood‚Äù or fast, mind you, but it‚Äôs valid for a bi-variate function.\nAnd here the results. This was not surprising after the algorithm was understood, but still, one can tell it was off-track in this example iteration, and still got to the right results in the end.\n \n\n\n\nAt this point, we might want to look into optimization that work also for non-differentiable functions (maybe). That‚Äôs a probable future post üôÇ"
  },
  {
    "objectID": "posts/2023-07-23_Simulated_Annealing/index.html#intro",
    "href": "posts/2023-07-23_Simulated_Annealing/index.html#intro",
    "title": "Optimization (3/n): Simulated Annealing",
    "section": "",
    "text": "Continuing with this simple ‚Äúseries‚Äù (see here and here), I implement the next algorithm proposed by the reference book, but in R.\nThis time around, it‚Äôs the turn of ‚ÄúSimulated Annealing‚Äù.\n\n\nI like how the concept of molecules excitement and temperatures is used for this algorithm. All in all, it‚Äôs a bit different from Gradient Descent in that it allows to keep going with WORSE results in some cases, proportional to the progress, so that it explores better the overall range of possible values. This is supposed to ensure that this algorithm will in fact avoid falling in local minimum, and so this is better for situations where there are multiple minima.\nThat said, here the code for this algorithm. I haven‚Äôt made any effort to make this implementation ‚Äúgood‚Äù or fast, mind you, but it‚Äôs valid for a bi-variate function.\nAnd here the results. This was not surprising after the algorithm was understood, but still, one can tell it was off-track in this example iteration, and still got to the right results in the end.\n \n\n\n\nAt this point, we might want to look into optimization that work also for non-differentiable functions (maybe). That‚Äôs a probable future post üôÇ"
  },
  {
    "objectID": "posts/2023-07-09_SummerFun_Optimization/index.html",
    "href": "posts/2023-07-09_SummerFun_Optimization/index.html",
    "title": "Summer fun: Testing Optimization Algorithms (1/n)",
    "section": "",
    "text": "Intro\nI have some more spare time for the upcoming few weeks, and I haven‚Äôt had the opportunity to take the ‚ÄúMetaheuristics‚Äù course in the Master, and so I feel I‚Äôm missing out a bit‚Ä¶ But then, what precludes me from learning on my own?\nSo I came across a book (I spend a bit too much time in book stores just ‚Äúbrowsing‚Äù‚Ä¶ Or buying books online) about ‚ÄúOptimisation‚Äù in Matlab. As it turns out, this appears to be in fact a great book to understand a few algorithms of the field of ‚ÄúMetaheuristics‚Äù.\n\n\nFirst thing First\nBefore I start with optimization per-se, as the books relies somewhat heavily on two visualizations, Contours and 3D perspectives of bi-variate functions, I have to get acquainted with the necessary R code for just that.\nThe code for that is all in here.\nAside from learning that the base graphics in R are very capable, with ‚Äúcontour()‚Äù and ‚Äúpersp()‚Äù readily available, I‚Äôve had to learn what Azimuth meant‚Ä¶ Thankfully given the context, it wasn‚Äôt hard, and the Wikipedia is always helpful.\nHere the results for today. I generate a 3D cone, and then we visualize through a couple of drawings that will be helpful later on.\nA contour plot will show altitudes in 2D, which can be helpful when visualizing say ‚Äúwhere is the minimum altitude‚Äù:\n\n\n\nA Contour Plot\n\n\nA Contour Plot\nThen the 3D perspective visualization can be as bad as this:\n\n\n\nBad perspective for 3D\n\n\nOr a bit nicer, like so:\n\n\n\nBetter 3D visualization\n\n\n\n\nNext time\nIn the next blog entry, I will be discussing one of the first algorithms the book comments: Gradient Descent.\n\n\nReferences\nAbout persp and the cone"
  },
  {
    "objectID": "posts/2022-07-10_CISA_KEV/index.html",
    "href": "posts/2022-07-10_CISA_KEV/index.html",
    "title": "A quick look at CISA KEV",
    "section": "",
    "text": "I keep hearing about it, so it was time I had a look at that famous CISA ‚ÄúKnown Exploited Vulnerability‚Äù dataset. Also, it‚Äôs been a while since I did something more directly related to IT Security, so this is good.\nIt turns out, it‚Äôs quite‚Ä¶ Simple (the dataset), and even clean, which makes things rather easy. So we‚Äôll be quick about it."
  },
  {
    "objectID": "posts/2022-07-10_CISA_KEV/index.html#looking-for-a-codebook",
    "href": "posts/2022-07-10_CISA_KEV/index.html#looking-for-a-codebook",
    "title": "A quick look at CISA KEV",
    "section": "Looking for a ‚ÄúCodebook‚Äù",
    "text": "Looking for a ‚ÄúCodebook‚Äù\nSo first, let‚Äôs have a look here to understand what this dataset is. (Looking at data without having any background sometimes needs to happen, but whenever possible, let‚Äôs avoid that)\nThere is not exactly a ‚Äúcodebook‚Äù that I could find, describing the dataset column by column, but as it is quite straightforward, the provided background is probably sufficient."
  },
  {
    "objectID": "posts/2022-07-10_CISA_KEV/index.html#getting-the-data",
    "href": "posts/2022-07-10_CISA_KEV/index.html#getting-the-data",
    "title": "A quick look at CISA KEV",
    "section": "Getting the data",
    "text": "Getting the data\nToday the code won‚Äôt be that complicated at all. We‚Äôll first get the data, in JSON, and put it into a dataframe.\nlibrary(jsonlite) #fromJSON -&gt; data.frame\nlibrary(httr) # GET/POST\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(wordcloud2)\n\ncisa_response &lt;- httr::GET(url = \"https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json\",\ncontent_type_json(), accept_json())\nView(rawToChar(cisa_response$content))\nkev_list &lt;- fromJSON(txt = rawToChar(cisa_response$content))\nkev_df &lt;- kev_list[['vulnerabilities']]\nAnd that‚Äôs about it."
  },
  {
    "objectID": "posts/2022-07-10_CISA_KEV/index.html#about-prevalence",
    "href": "posts/2022-07-10_CISA_KEV/index.html#about-prevalence",
    "title": "A quick look at CISA KEV",
    "section": "About Prevalence",
    "text": "About Prevalence\nSo if we look (again: real quick) at the dataset, we easily get a feeling Microsoft, Google, Apple, Adobe‚Ä¶ A few familiar names appear more often than others. Let‚Äôs check that assumption (not all results shown there, but you get the idea), and no need to be fancy, a simple table will do:\n\nSo yes, there is something there. But this time around, let‚Äôs use our understanding of the so-called ‚Äúdomain knowledge‚Äù: So the bad guys they really want to have tools that get them into systems (to get data) that are‚Ä¶ used, by their targets.\nThis can be checked against things like (random search results for OS and software prevalence):\n\nhttps://gs.statcounter.com/os-market-share\nhttps://www.muchskills.com/blog/top-software-technical-tools-muchskills\nhttps://statisticsanddata.org/data/most-popular-pc-software/\n‚Ä¶\n\nSo it makes sense that the ‚Äúknown exploited vulnerabilities‚Äù affect often used products, say such as Microsoft (Windows, Office, etc.), Google (Chrome, Android‚Ä¶) or Apple (mostly iOS stuff). That‚Äôs not necessarily to say those products have more vulnerabilities than other products (although, the more complex the software‚Ä¶). No, not necessarily. It does say that more people (and organizations) use Windows, Chrome and iPhones, than say Firefox, though, but Firefox is still definitely in the short list.\nWell, it‚Äôs probably really a mix of a few aspects: Prevalence (hackers interest), complexity (possibility to make mistakes), and due diligence (how much you care about the security of your software in the first place).¬†\nBut let‚Äôs keep going."
  },
  {
    "objectID": "posts/2022-07-10_CISA_KEV/index.html#patch-your-systems",
    "href": "posts/2022-07-10_CISA_KEV/index.html#patch-your-systems",
    "title": "A quick look at CISA KEV",
    "section": "Patch your systems",
    "text": "Patch your systems\nWhen looking at the recommended actions, we get few options, and really given the nature of the data provided here, it makes sense.\n\nSo it really boils down to this:\n\nPatch your software\nRemove unsupported software\n\nNote: Patching implicitly supposes you have a license, where applicable. But that‚Äôs a different story.\nHow did I get the above tables, you ask? I‚Äôm surprised you would care, but here goes some sample code:\nkev_actions_freqs &lt;- as.data.frame(table(kev_df$requiredAction))\nnames(kev_actions_freqs)[1] &lt;- \"Rec_Action\"\nView(kev_actions_freqs %&gt;% arrange(desc(Freq)))"
  },
  {
    "objectID": "posts/2022-07-10_CISA_KEV/index.html#making-it-a-coding-exercise",
    "href": "posts/2022-07-10_CISA_KEV/index.html#making-it-a-coding-exercise",
    "title": "A quick look at CISA KEV",
    "section": "Making it a coding exercise",
    "text": "Making it a coding exercise\nSo far, no real coding challenge. But when trying to make sense of the ‚ÄúVulnerability Names‚Äù, it became a bit more interesting.\nWhile looking (manually) at the data first, I found out that most vulnerability names end with 2-3 words that somehow help categorize them. Things like ‚Äúmemory corruption vulnerability‚Äù, ‚Äúadobe flash player‚Äù, ‚Äúdirectory traversal vulnerability‚Äù‚Ä¶\nOK so how can we have a look at the most commonly found 3-words occurrences among the vulnerability names? Using ‚ÄúN-grams‚Äù.\nIn text analysis and ‚ÄúNatural Language Processing‚Äù, N-grams are basically that: things (letters, words‚Ä¶) that occur together in the text. This is very useful for validating texts, propose next words to be typed, things like that.\nAnyway, in our case, we‚Äôll look for most common strings of ‚Äú3 words‚Äù happening in our vulnerability names (just for fun). And for the visualization, although it‚Äôs more ‚Äúfun‚Äù than ‚Äúuseful‚Äù (a table, like those above, would probably do better for serious work‚Ä¶), we‚Äôll use a wordcloud (I mean, for this exercise, why not?)\nlibrary(ngram)\nng_words_seq &lt;- paste(unlist(strsplit(tolower(kev_df$vulnerabilityName), \" \")), collapse = \" \")\nng_list &lt;- ngram(ng_words_seq, n = 3)\nwordcloud2(get.phrasetable(ng_list)[, 1:2] %&gt;% filter(freq &gt; 2))\nThat‚Äôs it (it looks easy, but as it was the first time ever I used that ngram package, I had to read the documentation a bit :)).\nAnd here the results:\n\nYou can tell some things occur more than others (also see Prevalence as explained earlier ;)).\n‚ÄúBuffer Overflow‚Äù, ‚ÄúMemory Corruption‚Äù, ‚ÄúCommand Injection‚Äù, ‚ÄúPrivilege Escalation‚Äù, ‚ÄúImproper input validation‚Äù, ‚ÄúDirectory Traversal‚Äù, ‚ÄúType Confusion‚Äù, ‚ÄúAccess Control‚Äù‚Ä¶\nWell, maybe a 2 words N-gram was a better idea after all (and removing ‚Äúvulnerability‚Äù). Let‚Äôs try that (just for fun):\nng_words_seq &lt;- gsub(\"vulnerability\", \"\", ng_words_seq)\nng_list &lt;- ngram(ng_words_seq, n = 2)\nwordcloud2(get.phrasetable(ng_list)[, 1:2] %&gt;% filter(freq &gt; 2))\n\nIt‚Äôs not much better. So maybe this is not the best approach, but fair enough, a good, quick exercise for today."
  },
  {
    "objectID": "posts/2022-07-10_CISA_KEV/index.html#what-kev-does-not-say",
    "href": "posts/2022-07-10_CISA_KEV/index.html#what-kev-does-not-say",
    "title": "A quick look at CISA KEV",
    "section": "What KEV does not say",
    "text": "What KEV does not say\nI‚Äôve seen it, more than once, so I‚Äôm going to say it: It‚Äôs ‚ÄúNOT THAT EASY‚Äù to match two datasets of software names and versions. It turns out that some inventory (like this KEV thing) and security products (say some vulnerability management tool for example) will list the SAME SOFTWARE with DIFFERENT NAMES.\nIt sounds ridiculous, I know, but it goes like this, for instance: One might say ‚ÄúMicrosoft Windows‚Äù, another one might say ‚ÄúMS Win Server 2019‚Äù. This very simple example, times the number of vendors of software, their product names, and versions names, times the number of possible ideas each product vendor comes up with‚Ä¶ It rapidly becomes rather difficult (for an automated approach‚Ä¶).\nOne solution is to use the ‚ÄúCPE‚Äù (Common Platform Enumeration ‚Äì which incidentally was created for just this purpose, see here) where possible.\nFor example, the KEV dataset mentions ‚ÄúCVE-2020-1350‚Äù, applicable to Vendor: Microsoft, Product: Windows. That‚Äôs a bit too generic for my taste, most companies have MANY Windows machines‚Ä¶ Does this vulnerability affect servers, clients‚Ä¶?\nOne would then need to pivot somewhat, so if you search for the CVE, you easily end up on the MITRE page for it:\nhttps://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-1350\nThat still doesn‚Äôt give you the info you need, then you keep clicking links from there, and maybe you end up finding what you need here for example:\nhttps://nvd.nist.gov/vuln/detail/CVE-2020-1350\n\n\n\nCPE for CVE\n\n\n¬†\nFinally: This CVE affects certain (most, recent) versions of Windows Server. Now we‚Äôre able to leverage this information and go looking for servers in our network that match this and make sure these are patched.\nI‚Äôm pretty sure there will be a downloadable CVE database (actually there seems to be an API from the NIST NVD, so there would be a good place to start‚Ä¶ Maybe I‚Äôll have some time to look into it in upcoming weeks.), but in the worst case scenario you can always crawl (one-by-one, no brute-forcing there, please, that would be against ethical use of their service) the pages referencing each CVE as you need them. Anyhow."
  },
  {
    "objectID": "posts/2022-07-10_CISA_KEV/index.html#conclusions",
    "href": "posts/2022-07-10_CISA_KEV/index.html#conclusions",
    "title": "A quick look at CISA KEV",
    "section": "Conclusions",
    "text": "Conclusions\nFirst of: No ‚Äúbig surprise‚Äù anywhere with this dataset. As said, it‚Äôs clean, clearcut, and just for that I‚Äôm thankful üòÄ\nSecond of all, we could keep going (there is a column of description, and one of dates‚Ä¶ So we might want to use that maybe to extrapolate how much active the bad guys are by month, for example ‚Äì although there are again too many confounding variables to conclude much there from just this dataset‚Ä¶)\nThe key aspect however of the CISA KEV dataset is its purpose: to help organizations (or anyone with some IT gear) to, as they put it on the website, ‚Äúprioritize remediation‚Äù.\nAnd that‚Äôs already an important aspect, as it‚Äôs easy in a company to have thousands of devices, with quite a few software products on each‚Ä¶ Knowing what to take care of first is a good helping hand.\nSo once you have the KEV dataset, you‚Äôll need to find the machines that are affected by each corresponding CVE, so that you can actually patch those first.\nAnd as you‚Äôll want to patch ALL your systems at some point, you might ask ‚Äúwhy not do that directly‚Äù? Well, sometimes applying patches break stuff. So out of precaution, companies usually patch things in waves, usually by importance/business risk (say ‚ÄúCritical patches first, focus on Dev environment first, then low-business-criticality servers‚Ä¶‚Äù).\nAll this is saying is: you might want to review your patch waves to make sure you patch the systems affected by any of the CVEs in the KEV list sooner rather than later, somehow."
  },
  {
    "objectID": "posts/2022-07-10_CISA_KEV/index.html#references",
    "href": "posts/2022-07-10_CISA_KEV/index.html#references",
    "title": "A quick look at CISA KEV",
    "section": "References",
    "text": "References\nQuite a few today, but the main ones were:\nCISA Known Exploited Vulnerabilities\nMITRE CVE\nNIST NVD API\nR Package ngram Documentation"
  },
  {
    "objectID": "posts/2025-07-25_RLCS_lessSequential/index.html",
    "href": "posts/2025-07-25_RLCS_lessSequential/index.html",
    "title": "RLCS: Less sequential?",
    "section": "",
    "text": "I‚Äôve let time pass lately as I looked into other topics (ODE and migrating content from the old Blog).\nBut I do not forget about my side-project of the RLCS package.\nRecently I mentioned I had it working ‚Äúas a package‚Äù, which to me was an important step (and a useful experiment too, as I have several small R projects that will become easier in the future if I make them into personal R packages‚Ä¶).\nAnd one key aspect of my package is, little to no dependencies. I insist on that.\nHowever‚Ä¶"
  },
  {
    "objectID": "posts/2025-07-25_RLCS_lessSequential/index.html#i-always-say-its-slow-because-its-inherently-sequential-but-what-if",
    "href": "posts/2025-07-25_RLCS_lessSequential/index.html#i-always-say-its-slow-because-its-inherently-sequential-but-what-if",
    "title": "RLCS: Less sequential?",
    "section": "",
    "text": "I‚Äôve let time pass lately as I looked into other topics (ODE and migrating content from the old Blog).\nBut I do not forget about my side-project of the RLCS package.\nRecently I mentioned I had it working ‚Äúas a package‚Äù, which to me was an important step (and a useful experiment too, as I have several small R projects that will become easier in the future if I make them into personal R packages‚Ä¶).\nAnd one key aspect of my package is, little to no dependencies. I insist on that.\nHowever‚Ä¶"
  },
  {
    "objectID": "posts/2025-07-25_RLCS_lessSequential/index.html#what-if-i-could-de-couple-part-of-it",
    "href": "posts/2025-07-25_RLCS_lessSequential/index.html#what-if-i-could-de-couple-part-of-it",
    "title": "RLCS: Less sequential?",
    "section": "What if I could de-couple part of it?",
    "text": "What if I could de-couple part of it?\nSo I have used already %dopar% to run some training in parallel, with some effort on the part of the user (me, in this case), and the corresponding dependencies.\nOne thing I have tested is separating the training data in subsets, and train separate LCS, then combining them, and then iterating.\nWell, what if instead, I separated the ruleset consolidation?\nThe LCS algorithm assumes you expose it to new environment instances, and every now and then you apply subsumption, compaction, deletion and whatever other ‚Äúconsolidation‚Äù processes to the ruleset.\nImportantly, it assumes you do it sequentially.\nWell, what if you didn‚Äôt?\nWhat if every now and then you copied the complete current ruleset, and in a separate subprocess, you had a go at consolidating it?\nThen, once consolidated, you can replace the ruleset used with the cleaner one and keep going. Said replacement would be next to instantaneous. So effectively you would run the consolidation in a separate process in parallel.\nAlthough matching is more costly, consolidation is expensive too. And even though it happens only ever so often, well."
  },
  {
    "objectID": "posts/2025-07-25_RLCS_lessSequential/index.html#caveats",
    "href": "posts/2025-07-25_RLCS_lessSequential/index.html#caveats",
    "title": "RLCS: Less sequential?",
    "section": "Caveats",
    "text": "Caveats\nSure, it‚Äôs not as easy as that. Parallelizing means, while you‚Äôre consolidating a ruleset, copy of the original, in a separate process, the original gets updated!\nSo you potentially could loose track of changes.\nHowever‚Ä¶ Well, you can replace the new with the old and add only the rules that weren‚Äôt part of the original while consolidating. That is, after consolidation, you would be adding back the new rules that have appeared (typically through mutations and rule-discovery).\nBut these will be taken care of in the next consolidation, so you‚Äôre only delaying the cleaning of the new ruleset (if cleaning is needed).\nSo this is not perfect, indeed. But it feels like it might be helpful."
  },
  {
    "objectID": "posts/2025-07-25_RLCS_lessSequential/index.html#conclusions",
    "href": "posts/2025-07-25_RLCS_lessSequential/index.html#conclusions",
    "title": "RLCS: Less sequential?",
    "section": "Conclusions",
    "text": "Conclusions\nI‚Äôm not 100% convinced here. But it seems, for slower cases, say big datasets or long binary strings in the environment (i.e.¬†bigger search space) this decoupling could help, if consolidation per-se is expensive.\nIt might not always make sense, and I‚Äôm actually adding a dependency on parallelizing libraries‚Ä¶\nStill, it sounds like an interesting idea."
  },
  {
    "objectID": "posts/2025-08-10_RLCS_MIRAI/index.html",
    "href": "posts/2025-08-10_RLCS_MIRAI/index.html",
    "title": "RLCS with the mirai package",
    "section": "",
    "text": "I have actually now have had the time to evaluate (only briefly) the use of the mirai R package and it‚Äôs‚Ä¶ interesting.\n\n\n\ninstalling mirai package\n\n\nApplied to the RLCS (because, why not), I thought I‚Äôd give it a go and use it where I thought it would make a difference‚Ä¶ My use-case for today was to address the population cleaning process of the RLCS package, that is (just a bit) slow and very necessary.\nBut the thing is‚Ä¶ Nuanced."
  },
  {
    "objectID": "posts/2025-08-10_RLCS_MIRAI/index.html#i-hinted-at-the-idea-last-time-and-well",
    "href": "posts/2025-08-10_RLCS_MIRAI/index.html#i-hinted-at-the-idea-last-time-and-well",
    "title": "RLCS with the mirai package",
    "section": "",
    "text": "I have actually now have had the time to evaluate (only briefly) the use of the mirai R package and it‚Äôs‚Ä¶ interesting.\n\n\n\ninstalling mirai package\n\n\nApplied to the RLCS (because, why not), I thought I‚Äôd give it a go and use it where I thought it would make a difference‚Ä¶ My use-case for today was to address the population cleaning process of the RLCS package, that is (just a bit) slow and very necessary.\nBut the thing is‚Ä¶ Nuanced."
  },
  {
    "objectID": "posts/2025-08-10_RLCS_MIRAI/index.html#the-question-is-how-much-i-gain-with-it",
    "href": "posts/2025-08-10_RLCS_MIRAI/index.html#the-question-is-how-much-i-gain-with-it",
    "title": "RLCS with the mirai package",
    "section": "The question is how much I gain with it",
    "text": "The question is how much I gain with it\nLaunching a parallel dedicated process like so:\nmirai::daemon(1)\n\nmirai::mirai({ &lt;do something here&gt;; }, dependency1=dependency1...)\nSounds easy enough, but it becomes quickly complicated to decide when to wait for the result, why, how to sync things‚Ä¶ Things of parallel computation that are always fun. But it‚Äôs not all.\n\n\n\ncreating a local daemon to receive processing request, avoiding a bit the overhead\n\n\nAnd yet, supposing I manage (I did, I did), I ran some tests and (duh!) it‚Äôs not as simple as it might seem upfront.\nSo here is the thing:\nFor large populations, a process to reduce population size (be it sumbsumption, deletion, compaction or what-not) is a bit slower. For small populations however, it is rather fast.\nIn proportion then, you would want to use a separate process for population size control only if:\n\nit is slow (e.g.¬†large populations)\nOR it happens often\n(or ideally both)\n\nThe trick with this is then that choosing to separate that process into another core/thread is a balancing exercise that can (very easily) prove counter-productive. I know all this because I have just spent half my Sunday afternoon running some tests (profvis and microbenchmark, both handy).\nPlus, to be clear, the code is quite a bit messier (yes, even more :D). That said, I do feel it was a very interesting exercise."
  },
  {
    "objectID": "posts/2025-08-10_RLCS_MIRAI/index.html#so-why-bother",
    "href": "posts/2025-08-10_RLCS_MIRAI/index.html#so-why-bother",
    "title": "RLCS with the mirai package",
    "section": "So why bother?",
    "text": "So why bother?\nWell, here is the thing: Having that separated sub-process run in parallel allows me to decide to run subsumption quite a bit more often, without a real impact to the runtimes.\nThis means, in the long run, and depending on other parameters (‚Ä¶), smaller populations for matching, and overall faster runtimes then.\nBut well, it‚Äôs not perfect."
  },
  {
    "objectID": "posts/2025-08-10_RLCS_MIRAI/index.html#some-pictures-to-support-the-claims",
    "href": "posts/2025-08-10_RLCS_MIRAI/index.html#some-pictures-to-support-the-claims",
    "title": "RLCS with the mirai package",
    "section": "Some pictures to support the claims",
    "text": "Some pictures to support the claims\n\n\n\none CAN get small gains, for say mux6(), but it is highly dependent on parameters/use-case\n\n\nHere is an example with the Iris dataset, whereby I set things to do a population cleanup every 8th generation. I ran both the new version and the old. Here another screenshot:\n\n\n\ndifference in delay\n\n\nSo what happening above is: I launch subsumption and deletion processes at every 8th epoch in both cases.\nBut with a mirai process, I delay consolidation of my population for after the cleanup is finished, and I keep running discovery of new rules in the meatime (roughly, this goes on for 3 or 4 epochs), and then only do I consolidate with the cleaned-up population from 3 (or 4) epochs ago (which is made a bit more complicated, as you might guess, by the rules that might have been discovered in the meantime, which also explains that ‚ÄúFinal Population Cleanup (sequential):‚Äù message in there).\nWhere withouth mirai, I have to wait for the same cleanup process to happen before I can continue.\nIf you stop and review the convoluted explanation above, you might see that: If I ‚Äúsave‚Äù 3-4 epochs of processing time associated to the population cleanup, and I clean the population often (say, like here, every 8th epoch), and suppose I want to run this process for a very high number of epochs (which might in some corner cases be warranted), here I‚Äôd be saving 3-4 steps every 11-12 steps. Which is a big gain!\nIncorrect, though, as I do have an overhead for reconciliation of the two parallel populations (and to be perfectly clear, and honest, right now my implementation also makes some educated simplifications in there).\nBut what matters to me here is the order of magnitude. 30%. Or say ‚Äúonly‚Äù 20%, to account for the overhead of reconciliation (it‚Äôs not that hard). That‚Äôs not a lot unless you have a very long running process. Then you might start to appreciate it.\n(Note: In the case of an iris classifier as above with some chosen hyperparameters, I get 9.96 seconds without mirai, and 7.17 seconds with‚Ä¶ Not too bad.)"
  },
  {
    "objectID": "posts/2025-08-10_RLCS_MIRAI/index.html#conclusion",
    "href": "posts/2025-08-10_RLCS_MIRAI/index.html#conclusion",
    "title": "RLCS with the mirai package",
    "section": "Conclusion",
    "text": "Conclusion\nPopulation consolidation was never the biggest bottleneck. But it is a slow process in cases of large populations. And the overall matching efforts benefit from a more compact population, so more frequent clean-ups might prove very useful sometimes‚Ä¶\nI have yet to test this with my images classifier or the RLCS RL demo use-case. There, I expect to see some more value out of these efforts.\nAt this time and with the tests I ran this afternoon (rather simple ones with not_bit4, mux6 and the iris dataset), is it worth it? Well‚Ä¶ Not really.\nBut depending on the configurations, I could (I did) get roughly 40% time savings. Not too shabby.\nSo the balance between cleaner code, one-less-dependency (mirai, which is also a complex dependency, for a package, as I would have to decide how I let my user manage that or not‚Ä¶), and not awfully slow run-times, or slightly better run-times with less readable code, but possibly a key improvement in larger cases‚Ä¶\nI‚Äôm not 100% sold. I‚Äôll have to keep running tests."
  },
  {
    "objectID": "posts/2025-08-10_RLCS_MIRAI/index.html#reference",
    "href": "posts/2025-08-10_RLCS_MIRAI/index.html#reference",
    "title": "RLCS with the mirai package",
    "section": "Reference",
    "text": "Reference\nI looked for things for a while, but last week this showed up. You have to love the R community for these things."
  },
  {
    "objectID": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html",
    "href": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html",
    "title": "Thinking about Systems",
    "section": "",
    "text": "I did mention some of it a while back, but also I am re-reading ‚ÄúThinking in Systems‚Äù by D. H. Meadows.\nWell, I still truly do NOT know how I will go about it (if and when I decide to put more efforts into coding these things), but for the time being, I thought I‚Äôd run some extremely simplistic - and absolutely incorrect and personal - approach to it all."
  },
  {
    "objectID": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#i-keep-coming-back-to-these-ideas",
    "href": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#i-keep-coming-back-to-these-ideas",
    "title": "Thinking about Systems",
    "section": "",
    "text": "I did mention some of it a while back, but also I am re-reading ‚ÄúThinking in Systems‚Äù by D. H. Meadows.\nWell, I still truly do NOT know how I will go about it (if and when I decide to put more efforts into coding these things), but for the time being, I thought I‚Äôd run some extremely simplistic - and absolutely incorrect and personal - approach to it all."
  },
  {
    "objectID": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#about-stocks",
    "href": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#about-stocks",
    "title": "Thinking about Systems",
    "section": "About ‚Äústocks‚Äù",
    "text": "About ‚Äústocks‚Äù\nOne topic that appears a bit in the referenced book is the idea of stock, as in ‚Äústorage buffers‚Äù of ‚Äúflows‚Äù (that‚Äôs how I understood it anyway). Think bathtub, faucet and drain.\nSo how about this for a simplistic idea. Two companies compete to ‚Äúgrow‚Äù with two different HR policies, one more focused on improving hiring, the other on retention.\nBoth have supposedly ‚Äúgood policies‚Äù towards an objective of growth, in that their respective policies seem ‚Äúpositive‚Äù, more ‚Äúin-flow‚Äù that ‚Äúout-flow‚Äù.\nNow (and again, this is stupid as a model I suppose, it is just me warming up to the concepts), what if there was a limited ‚Äúpool‚Äù of potential employees, a ‚Äúgeneral population‚Äù from which to hire from (or ‚Äúfire into‚Äù).\nI coded a very very simplistic (and absolutely zero-optimized, nor probably even quite correct) thing to represent this, which looks at the ‚Äúcurrent stock‚Äù of each company (and well, the general population).\nWhy not visualize that as a basic network graph (or simply ‚Äúgraph‚Äù, which I prefer):\n\n\n\nsimple scenario - starting setup\n\n\nIn the following, for all results, I always initialize the simulation scenario as follows:\nstocks &lt;- list()\n## Let's work with two stocks\n## First will be the unemployed population, with \"big\" capacity to consume/pour flows:\nstocks &lt;- add_stock(stocks, stock_name = \"population\", initial_stock=100, inflow=1000, outflow=1000) \n    \nstocks &lt;- add_stock(stocks, stock_name = \"company1\", initial_stock=20, inflow=3, outflow=2)\nstocks &lt;- add_stock(stocks, stock_name = \"company2\", initial_stock=20, inflow=7, outflow=3)\nEach ‚Äústock‚Äù in turn can evolve with the (discrete) passing of time (how much time will be important as we‚Äôll see in a moment), and I choose to represent that as ‚Äúin-flows‚Äù and ‚Äúout-flows‚Äù volume, which in turn are supposed here to represent the corresponding ‚Äúpolicy‚Äù of each company.\nAgain, I‚Äôm making stuff up as I go here, so‚Ä¶ For instance, I add some randomness to the process (just for the fun of it) so that at time step X, company 1 with inflow 3 would try to ‚Äúhire‚Äù 3 employees, but I add a probability of 40% whereby the company at that particular time-step X decides to hire someone. The same goes for firing (in the case of company 1, the policy says ‚Äúfire 2‚Äù). And all that applies exactly the same for company 2, with its own policy.\nSo it could look like so:\nconsume_from_population &lt;- function(stocks, consumer_stock_num) {\n    ## Add some level of randomness\n    if(runif(1) &gt; 0.4)\n        if(stocks[[1]]$current_stock &gt; 0) { \n            ## stocks[[1]] is the general population...\n            consumable &lt;- min(stocks[[1]]$current_stock, stocks[[consumer_stock_num]]$inflow)\n            stocks[[1]]$current_stock &lt;- stocks[[1]]$current_stock - consumable\n            stocks[[consumer_stock_num]]$current_stock &lt;- stocks[[consumer_stock_num]]$current_stock + consumable\n        }\n    stocks\n}\n\n...\nThe general population starts off with 100 individuals, and is not limiting in that it could potentially gather ‚Äúpeople‚Äù from - or pour into - the companies at a rate much higher than what the companies would do."
  },
  {
    "objectID": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#simulating-the-passing-of-time",
    "href": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#simulating-the-passing-of-time",
    "title": "Thinking about Systems",
    "section": "Simulating the passing of time",
    "text": "Simulating the passing of time\nQuite simply, this is the core of the simulation, really:\nfor(current_time in 1:n_time_steps) {\n    ## flows processing: circle through the companies\n    for(node_id in sample(2:3, 2, replace = F)) {\n        ## SIMPLISTIC, probably BAD, I know\n        stocks &lt;- consume_from_population(stocks, node_id)\n        stocks &lt;- loose_to_population(stocks, node_id)\n    }\n    ...\n}\nRight now, I only throw graph-related code to show current size of each in number of employees:\nV(g)$size &lt;- sapply(stocks, \\(x) x$current_stock)\nPretty basic stuff, very ‚Äúalgorithmic‚Äù, and again, not much thinking went into doing this ‚Äúright‚Äù.\nThis is what it looks like for one simulation then:"
  },
  {
    "objectID": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#some-results",
    "href": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#some-results",
    "title": "Thinking about Systems",
    "section": "Some results",
    "text": "Some results\nWhile all the above is pretty simplistic, it does suggest ‚Äúagent-based modeling‚Äù stuff, simulations, etc.\nAnd even for such a simple approach, not realistic in the least, let‚Äôs have a look at some conclusions‚Ä¶\nBecause I added some stochastic parameters in there (the 40% thing), I would need to run several simulations per scenario to get a sense of what really is happening.\nToday I‚Äôll focus on how long I run the simulation and the impact of limited general population. It‚Äôs not rocket science, really, so let‚Äôs break it down.\nIn all cases, I run one scenario 50 times (and then I‚Äôll use that to average results). In all scenarios I start with the same configuration.\nThen I decide to run each scenario a number of steps:\nn_time_steps_vec &lt;- c(100, 500, 1000, 5000, 10000)\nfor(n_time_steps in n_time_steps_vec) {\n    for(i in 1:50) { ## Let's do this simulation a few times, shall we? MC-like\n        ...\n    }\n}\nI‚Äôll show in the next graphics the following, for each company:\n\nAfter 50 simulations of one scenario, what was the resulting size of the company (in number of employee) in each scenario, when the scenario was run 100 time steps.\nSave the resulting 50 simulations per scenario, but run it 100, 500, 1000, 5000 and 10000 time steps. Average (blue line) the resulting size of the company. (Green line: the companies start with 20 employees each)\n\nShow that for both companies (1 on top, 2 at the bottom).\nSo what does this say?\nWell, there is something I am not showing above: The general population!\nSee, as long as the general population has people to hire, both companies grow, one more than the other, as expected per their respective policies. That is: Company 2 hires even more than it fires people, comparatively, in spite of being less careful with loosing people than Company 1.\nHowever, when the population of candidates is depleted, Company 1, which is less aggressive in hiring but more conservative when it comes to loosing people (i.e.¬†possibly better culture, while maybe less ‚Äúattractive‚Äù to the market ‚Äúupfront‚Äù, say maybe it pays a little less than company 2‚Ä¶ Whatever the reason!)‚Ä¶\nRight, so when the ‚Äúgeneral population‚Äù is depleted, the policy of the first company pays off!\n\n\n\nWhat happens if you prolong the simulations\n\n\nIt makes sense, right? If hiring isn‚Äôt an option, well, you better make efforts towards retaining your talent.\nCompany 1 has a better policy when it directly competes with Company 2 for its resources, instead of consuming from a large pool of candidates.\nIn other words, if a resource is scarce, and you‚Äôre competing directly with others, maybe you should focus on saving that resource, more so than getting more.\nAnd maybe it‚Äôs wrong, but this simplistic simulation scenario hints at such a conclusion."
  },
  {
    "objectID": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#limitations",
    "href": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#limitations",
    "title": "Thinking about Systems",
    "section": "Limitations",
    "text": "Limitations\nWell, to begin with, I don‚Äôt know whether my code even makes sense, for example I simply loop through a list of companies, and the ‚Äúgraph‚Äù structure really is just used for visualization (maybe I could use some adjacency matrix multiplications instead? Ashby‚Äôs Cybernetics book hinted at that‚Ä¶ But for states‚Ä¶ IDK, right now, no matter).\nThis is also all very static, too. Both companies stick to their policies no matter what. Not realistic in the least.\nAnd yes, the population is limited. Like a bounded resource. It feels wrong, or at least very incomplete (if that ever happened, people would rush to train themselves to compete to get hired in that market, wouldn‚Äôt they?).\nPlus, the ‚Äú40%‚Äù decision for hiring/firing at any given time step makes no sense whatsoever, either.\nAnd a long etc. Then again, ‚Äúall models are wrong‚Äù‚Ä¶ Well, this one is very wrong. Anyhow, this was just for the fun of the exercise!"
  },
  {
    "objectID": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#conclusions",
    "href": "posts/2025-08-15_Thinking_About_Thinking_in_Systems/index.html#conclusions",
    "title": "Thinking about Systems",
    "section": "Conclusions",
    "text": "Conclusions\nI know, too simplistic, not realistic at all. Even poor algorithmic approach, for sure, I don‚Äôt know. The goal here was never to go for perfect, but to try out my hand at simulating flows and stocks and a simple interaction scenario.\nAnd yet, some conclusions can come out of it, somewhat in the spirit of ‚ÄúSystems Thinking‚Äù.\nAnd that is more than enough for me for now.\nIt does tell me, once I shift my focus onto a new project for next year (just an idea I have for myself, because I want to work on something that interests me like that, say like the RLCS project these past few months‚Ä¶)‚Ä¶ In between ODE and ABM and FSMs, yes‚Ä¶\nYes, maybe that‚Äôs where I‚Äôll keep looking for fun stuff to learn."
  },
  {
    "objectID": "posts/2025-08-24_NewProjectForProgressTracking/index.html",
    "href": "posts/2025-08-24_NewProjectForProgressTracking/index.html",
    "title": "Organizing a bit for future work/exercises",
    "section": "",
    "text": "I have dedicated a bit of my spare time to the RLCS package creation. And it‚Äôs still very much ongoing. But as I give myself until end of 2025 to have a first ‚Äúshared‚Äù version of the package, well, I am looking into future work. Something to keep my head busy for 2026, if you will."
  },
  {
    "objectID": "posts/2025-08-24_NewProjectForProgressTracking/index.html#collecting-thoughts-and-code-for-future-project",
    "href": "posts/2025-08-24_NewProjectForProgressTracking/index.html#collecting-thoughts-and-code-for-future-project",
    "title": "Organizing a bit for future work/exercises",
    "section": "",
    "text": "I have dedicated a bit of my spare time to the RLCS package creation. And it‚Äôs still very much ongoing. But as I give myself until end of 2025 to have a first ‚Äúshared‚Äù version of the package, well, I am looking into future work. Something to keep my head busy for 2026, if you will."
  },
  {
    "objectID": "posts/2025-08-24_NewProjectForProgressTracking/index.html#new-github-repo",
    "href": "posts/2025-08-24_NewProjectForProgressTracking/index.html#new-github-repo",
    "title": "Organizing a bit for future work/exercises",
    "section": "New GitHub Repo",
    "text": "New GitHub Repo\nJust because I am slowly getting used to sharing my code more and more (in spite of the shame it supposes sometimes, my code being‚Ä¶ Well, far from perfect), I thought I‚Äôd help myself by setting up a new project early on to collect some ideas.\nAnd so this repo was born earlier today."
  },
  {
    "objectID": "posts/2025-08-24_NewProjectForProgressTracking/index.html#on-the-issue-of-generating-animations",
    "href": "posts/2025-08-24_NewProjectForProgressTracking/index.html#on-the-issue-of-generating-animations",
    "title": "Organizing a bit for future work/exercises",
    "section": "On the issue of generating animations",
    "text": "On the issue of generating animations\nOne thing I still struggle with is generating animations in 2D that are visually pleasant and particulary, that are not slow. (Yes, this is related to the above, as I want to be able to ‚Äúshow‚Äù agent-based modelling simulations.)\nUsing RStudio Viewer and image() to follow the evolution of the cellular automata in past exercises has often required, for niceness, for me to edit a video screenshot and accelerate it.\nAlso, using image(), this is the fastest way I‚Äôve found during my tests:\n## From past exercise with Schelling Seggregation Model, I had this:\ntf &lt;- function(m) t(m)[, nrow(m):1]\nimageM &lt;- function(m, grid = max(dim(m)) &lt;= 25, asp = (nrow(m)-1)/(ncol(m)-1), ...) {\n  image(tf(m), asp=asp, axes = FALSE, useRaster=TRUE, ...)\n}\n\nt_start &lt;- Sys.time()\nfor(i in 1:10) {\n  imageM(world[[i]])\n  Sys.sleep(.2) ## Renders, but runtime goes to 2.22secs for 100*100px.\n  ## Not awful, but not great.\n}\nSys.time() - t_start\nHowever I try‚Ä¶ I just can‚Äôt seem to get past the 10 FPS mark, (i.e.¬†reduce the Sys.sleep() there) whatever I do, and that‚Äôs only for pre-computed matrices, of reasonable size.\nHowever, if you use the animation package like I did at the end of the code here, for instance:\n## Not in RStudio/so post-processing visualization, but can be made fast:\nlibrary(animation)\n# Create the animation\nsaveHTML({ ## This one also offers speed controls...\n  for (i in 1:100) { ## Careful, it generates one PNG per matrix, and JS/CSS/HTML stuff...\n    image(world[[i]], axes=FALSE, useRaster = TRUE, col = grey.colors(256))\n    ani.options(interval = 0.02) ## Key to actual visual speed here\n  }\n},\nhtmlfile = \"random_points.html\",\nani.width = 500,\nani.height = 500)\nWell, the results are nicer.\nApplying this approach to a past exercise on Fire Spread simulation, you get to this result here:\n\n(It‚Äôs actually a cut-down version with low resolution as the original capture of it was 26MB big‚Ä¶)"
  },
  {
    "objectID": "posts/2025-08-24_NewProjectForProgressTracking/index.html#conclusions",
    "href": "posts/2025-08-24_NewProjectForProgressTracking/index.html#conclusions",
    "title": "Organizing a bit for future work/exercises",
    "section": "Conclusions",
    "text": "Conclusions\nI should really be working on finishing the RLCS project, but I am looking into future work instead, just to keep myself entertained really."
  },
  {
    "objectID": "posts/2025-09-15_RLCS_To_Nationals/index.html",
    "href": "posts/2025-09-15_RLCS_To_Nationals/index.html",
    "title": "RLCS goes to the National R Congress!",
    "section": "",
    "text": "I received the news that my proposal to present the RLCS work to the yearly Spanish R congress was accepted!\nAnd so, I am re-shifting my focus onto the RLCS project a bit for the upcoming weeks :)\nFirst off: I‚Äôm re-creating the new ‚ÄúR Package‚Äù from scratch, so that it has the right name, the right files‚Ä¶\nIt will be a simple version:\n\nNo parallel computing stuff\nSimple demos included only\nAnd the Reinforcement Learning will be included, but slightly more hidden than the rest."
  },
  {
    "objectID": "posts/2025-09-15_RLCS_To_Nationals/index.html#i-am-so-happy-about-this",
    "href": "posts/2025-09-15_RLCS_To_Nationals/index.html#i-am-so-happy-about-this",
    "title": "RLCS goes to the National R Congress!",
    "section": "",
    "text": "I received the news that my proposal to present the RLCS work to the yearly Spanish R congress was accepted!\nAnd so, I am re-shifting my focus onto the RLCS project a bit for the upcoming weeks :)\nFirst off: I‚Äôm re-creating the new ‚ÄúR Package‚Äù from scratch, so that it has the right name, the right files‚Ä¶\nIt will be a simple version:\n\nNo parallel computing stuff\nSimple demos included only\nAnd the Reinforcement Learning will be included, but slightly more hidden than the rest."
  },
  {
    "objectID": "posts/2025-09-15_RLCS_To_Nationals/index.html#a-bit-of-cleaning-up-and-testing",
    "href": "posts/2025-09-15_RLCS_To_Nationals/index.html#a-bit-of-cleaning-up-and-testing",
    "title": "RLCS goes to the National R Congress!",
    "section": "A bit of cleaning up and testing‚Ä¶",
    "text": "A bit of cleaning up and testing‚Ä¶\nIn order to make it available on my GitHub (it‚Äôs unfortunate, but I doubt I would get it published on CRAN in the upcoming month and a half, although, who knows‚Ä¶), anyhow, I need to clean it up. Too many prints and too many functions exposed.\nThen I need to test it (once uploaded to my GitHub) from a different environment (say, a Windows).\nThen I need to make sure I document the key parts."
  },
  {
    "objectID": "posts/2025-09-15_RLCS_To_Nationals/index.html#the-presentation-needs-a-review-too",
    "href": "posts/2025-09-15_RLCS_To_Nationals/index.html#the-presentation-needs-a-review-too",
    "title": "RLCS goes to the National R Congress!",
    "section": "The presentation needs a review too",
    "text": "The presentation needs a review too\nI have had the presentation ‚Äúslides‚Äù ready and shown in the past, but the goal now is to allow the audience to ‚Äúfollow along‚Äù while I do the demos.\nI ‚Äúonly‚Äù have about a month left to work on all that, and I won‚Äôt have as many hours as I would like, but I am confident it will all turn out OK."
  },
  {
    "objectID": "posts/2025-09-15_RLCS_To_Nationals/index.html#conclusion",
    "href": "posts/2025-09-15_RLCS_To_Nationals/index.html#conclusion",
    "title": "RLCS goes to the National R Congress!",
    "section": "Conclusion",
    "text": "Conclusion\nLots of work, but a bit more focused now that I have a new deadline.\nAnd although this is a very small audience for sure, being a simple independent enthusiast, I am still quite proud of myself here."
  }
]